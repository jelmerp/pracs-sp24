[
  {
    "objectID": "week3/w3_overview.html#links",
    "href": "week3/w3_overview.html#links",
    "title": "Week 3: Git & GitHub",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – Getting started with Git\nThursday – Remotes on GitHub\nBonus (optional self-study) content: Branching & merging, undoing changes, and viewing the past\n\n\n\nExercises & assignments\n\nMake sure you have done the assignment from week 2 by Tuesday: Create a GitHub account\nExercises\n\n\n\n\n\n\n\n\nFinal project\n\n\n\nYou should start thinking about your final project for this course — recall that this is the only part of the course that is graded.\n\nTake a look at the page with general info on the final project.\nYour proposal (plan) is due on Monday, April 1st (week 5).\nI will also point this out on Tue, and we can talk about this in class if needed on Thursday."
  },
  {
    "objectID": "week3/w3_overview.html#content-overview",
    "href": "week3/w3_overview.html#content-overview",
    "title": "Week 3: Git & GitHub",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will learn about the why and how of using Git version control for your projects, and sharing your code on GitHub.\nBe aware that Git is a challenging topic. Therefore, if you can, complete the main reading before Tuesday’s lecture, and also read the Buffalo chapter at some point this week.\nA good way to get used to Git is to make dummy repositories where you’re just editing one or a few simple text files with dummy lines of text. That way, you can get used to the basic workflow, and freely experiment also with commands to undo things and move back in time. We’ll do this in our Zoom meetings and I recommend you do it outside of there, too.\nSome of the things you will learn this week:\n\nUnderstand why you should use a formal Version Control System (VCS) for research projects.\nLearn the basics of the most widely used VCS: Git.\nLearn how to put your local repositories online at GitHub, and how to keep local and online (“remote”) repositories in sync.\nLearn about single-user and multi-user workflows with Git and GitHub.\n\nOptional self-study content:\n\nLearn how to use Git branches to safely make experimental changes.\nLearn how to undo things and “travel back in time” for your project using Git."
  },
  {
    "objectID": "week3/w3_overview.html#readings",
    "href": "week3/w3_overview.html#readings",
    "title": "Week 3: Git & GitHub",
    "section": "3 Readings",
    "text": "3 Readings\nThis week’s main reading is the CSB chapter on Git, chapter 2. We will also roughly work our way through this chapter in the Zoom sessions.\nThe optional reading is the Buffalo chapter on Git, chapter 5. Like the CSB chapter, this starts with the very basics of Git; but it goes a bit further.\nThere are also some useful further resources mentioned below.\n\nRequired readings\n\nCSB Chapter 2: “Version Control” up until section 2.5 (the rest of the chapter is optional)\n\n\n\nOptional readings\n\nBuffalo Chapter 5: “Git for Scientists”\n\n\n\nFurther resources\n\nGitHub has a nice little overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\nFor some more background on why to use version control, and another perspective on some Git basics, I recommend the article “Excuse me, do you have a moment to talk about version control?” by Jenny Bryan.\nEspecially if you work with R a lot, I would recommend checking out Happy Git and GitHub for the useR, also by Jenny Bryan. This is a very accessible introduction to Git.\nGit-it is a small application to learn and practice Git and GitHub basics.\nIf you want to try some online exercises with Git with helpful visuals of what Git commands do, try https://learngitbranching.js.org/. (But be aware that this does fairly quickly move to fairly advanced topics, including several that we will not touch on in the course.)\nA good list of even more Git resources…"
  },
  {
    "objectID": "week3/w3_git3.html#branching-merging",
    "href": "week3/w3_git3.html#branching-merging",
    "title": "Git: Undoing",
    "section": "1 Branching & merging",
    "text": "1 Branching & merging\n\n1.1 A repo with a couple of commits\n\nFirst, we create a dummy repo with a few commits by running a script:\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\n\n# Have a look at the script:\nless ../data/create_repository.sh  # press `q` to exit less\n\n# Run the script, which will create our repo:\n../data/create_repository.sh\n\n# Move to the repo dir:\ncd branching_example\nLet’s see what has been done in this repo:\ngit log --oneline # --oneline: 1-line summary per commit\nWe will later modify the file code.txt — let’s see what it contains now:\ncat code.txt\n\n\n\n\n1.2 Branches in Git\n\nWe now want to improve the code, but these changes are experimental,\nand we want to retain our previous version that we know works.\nThis is where branching comes in. With a new branch, we can make changes that don’t affect the master branch, and we can also keep working on the master branch:\n\n\n\n\nFigure modified after Allesino & Wilmes (2019)\n\n\n\nCreating a new branch\n\nFirst, we create a new branch as follows:\ngit branch fastercode   # We name the branch \"fastercode\"\nLet’s see what our log looks like now, and list the branches:\ngit log --oneline\ngit branch  # Without args, git branch will list the branches\nIt turns out we created a new branch but we are still on the master branch. We can switch branches with git checkout:\ngit checkout fastercode\n\ngit branch   # Should now show we are on `fastercode`\ngit status   # Should also tell us we are on `fastercode`\n\n\n\n\nMaking experimental changes on the new branch\n\nWe edit the code, stage and commit the changes:\necho \"Yeah, faster code\" &gt;&gt; code.txt\ncat code.txt\ngit add code.txt\ngit commit -m \"Managed to make code faster\"\nLet’s check the log again:\ngit log --oneline # Last commit: on branch \"fastercode\"\n\n\n\n\nMoving back to the master branch\n\nWe need to switch gears and add references to the paper draft.\nSince this has nothing to do with our attempt at faster code,\nwe should make these changes back on the master branch:\ngit checkout master # move back to master\nWhat does code.txt, which we edited on fastercode, now look like?\ncat code.txt  # Our working dir contents has changed!!\nNow, let’s add the reference, stage and commit:\necho \"Marra et al. 2014\" &gt; references.txt\ngit add references.txt\ngit commit -m \"Fixed the references\"\nNow that we’ve made changes to each of the two branches, let’s see the log in “graph” format with --graph, also listing all branches with --all:\ngit log --oneline --graph --all\n\n\n\n\nFinishing up on the experimental branch\n\nEarlier, we finished speeding up the code at the fastercode branch, but we still need to document our changes. So, we go back:\ngit checkout fastercode\nDo we still have the references.txt file from the master branch?\nls\nThen, we add the “documentation” to the code, stage and commit:\necho \"# My documentation\" &gt;&gt; code.txt\ngit add code.txt\ngit commit -m \"Added comments to the code\"\nAgain we check the log graph:\ngit log --oneline --all --graph\n\n\n\n\nMerging the branches\n\nWe are happy with the changes to the code, and want to make the fastercode version the default version of the code. This means we should merge the fastercode branch back into master. To do so, we first have to move back to master:\ngit checkout master\nNow we are ready to merge, and we use the git merge command.\nWe also provide a commit message, because a merge is always accompanied by a commit:\ngit merge fastercode -m \"Much faster version of code\"\nOnce again, we check the log graph:\ngit log --oneline --all --graph\n\n\n\n\nCleaning up\n\nWe no longer need the fastercode branch, so we can delete it:\ngit branch -d fastercode\nAnd again we check the log graph:\ngit log --oneline --all --graph\n\n\n\n\n\n1.3 Branching and merging – Workflow summary\n\n\n\nFigure from after Allesino & Wilmes (2019)\n\n\n\n\n\n\n\n\nTip\n\n\n\nVisualization with http://git-school.github.io/visualizing-git.\n\n\n\nOverview of commands used in the branching workflow\n# Create a new branch:\ngit branch mybranch\n\n# Move to new branch:\ngit checkout mybranch\n\n# Add and commit changes:\ngit add --all\ngit commit -m \"my message\"\n\n# Done with branch - move back to main trunk and merge\ngit checkout master\ngit merge mybranch -m \"Message for merge\"\n\n# And [optionally] delete the branch:\ngit -d mybranch\n\n\n Exercise (Intermezzo 2.2)\n\n(a) Move to the directory CSB/git/sandbox.\n\n\n\nSolution\n\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\n\n\n(b) Create a directory thesis and turn it into a Git repository.\n\n\n\nSolution\n\nmkdir thesis\ncd thesis\ngit init\n\n\n(c) Create the file introduction.txt with the line “Best introduction ever.”\n\n\n\nSolution\n\necho \"The best introduction ever\" &gt; introduction.txt\n\n\n(d) Stage introduction.txt and commit with the message “Started introduction.”\n\n\n\nSolution\n\ngit add introduction.txt\ngit commit -m \"Started introduction\"\n\n\n\n(e) Create the branch newintro and change into it.\n\n\n\nSolution\n\ngit branch newintro\ngit checkout newintro\n\n\n(f) Overwrite the contents of introduction.txt, create a new file methods.txt, stage, and commit.\n\n\n\nSolution\n\necho \"A much better introduction\" &gt; introduction.txt\ntouch methods.txt\ngit add --all\ngit commit -m \"A new introduction and methods file\"\n\n\n(g) Move back to master. What does your working directory look like now?\n\n\n\nSolution\n\ngit checkout master\nls     # Changes made on the other branch are not visible here!\ncat introduction.txt\n\n\n(h) Merge in the newintro branch, and confirm that the changes you made there are now in your working dir.\n\n\n\nSolution\n\ngit merge newintro -m \"New introduction\"\nls\ncat introduction.txt\n\n\n(i) Bonus: Delete the branch newintro.\n\n\n\nSolution\n\ngit branch -d newintro"
  },
  {
    "objectID": "week3/w3_git3.html#merge-conflicts",
    "href": "week3/w3_git3.html#merge-conflicts",
    "title": "Git: Undoing",
    "section": "2 Merge conflicts",
    "text": "2 Merge conflicts\nA merge conflict can occur when all three of the following conditions are met:\n\nYou try to merge two branches (including when pulling from remote: recall that a pull includes a merge)\nOne or more files have been changed (via commits) on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\n\nWhen this occurs, Git has no way of knowing which changes to keep,\nand it will report a merge conflict as follows:\n\n\n\n\n\n\nResolving a merge conflict\nWhen you get a merge conflict, follow these steps:\n\nUse git status to find the conflicting file(s).\n\n\n\n\n\n\n\nOpen and edit those files manually to a version that fixes the conflict.\n\n\n\n\n\n\n\nNote\n\n\n\nGit has changed this file to add the conflicting lines from both versions of the file, and to add marks indicating these conflicting lines:\nOn the Origin of Species # Line preceding conflicting line\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD # GIT MARK 1: Next line = current branch\nSecond line of the book - from main # Conflict line: current branch\n======= # GIT MARK 2: Dividing line\nSecond line of the book - from conflict-branch # Conflict line: incoming branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; conflict-branch # GIT MARK 3: Prev line = incoming branch\nYou have to manually change the contents in your text editor to keep the conflicting content that you want, and to remove the indicator marks that Git made.\n\n\n\n\n\n\n\n\nNote\n\n\n\nVS Code has some nice functionality to make this easier:\ncode &lt;conflicting-file&gt;  # Open the file in VS Code\n\n\n\n\n\nIf you click on “Accept Current Change” or “Accept Incoming Change”, etc., it will keep the desired lines and remove the Git indicator marks. Then, save and exit.\n\n\n\nUse git add to tell Git you’ve resolved the conflict in a particular file.\ngit add origin.txt\n\n\n\n\n\n\n\nOnce all conflicts are resolved, use git status to check that all changes are staged. Then, use git commit to finish the merge commit.\nEven if you provided a commit message for the merge at the outset, Git will now launch your text editor if you don’t again do so.\ngit commit"
  },
  {
    "objectID": "week3/w3_git3.html#forking-and-creating-a-pull-request",
    "href": "week3/w3_git3.html#forking-and-creating-a-pull-request",
    "title": "Git: Undoing",
    "section": "3 Forking and creating a Pull Request",
    "text": "3 Forking and creating a Pull Request\n\nForking a GitHub repository\nYou can follow along by e.g. forking my originspecies repo.\n\nGo to a GitHub repository, and click the “Fork” button in the top-right:\n\n\n\n\n\n\n\nYou’ll be asked which account or organization to fork to (you should pick your personal account, which is likely the only one you have).\nNow, you have your own version of the repository,\nand it is labeled explicitly as a fork:\n\n\n\n\n\n\n\n\nForking workflow\nYou can’t directly modify the original repository, but you can:\n\nFirst, modify your fork (with local edits and pushing).\nThen, submit a so-called Pull Request to the owner of the original repo to pull in your changes.\nAlso, you can also easily keep your fork up-to-date with changes to the original repository.\n\n\n\n\nFigure from Happy Git and GitHub for the useR\n\n\n\n\nEditing the forked repository\n\nClone your forked GitHub repository to your computer. Find the URL for the GitHub repository by clicking the green Code button.\nMake sure you get the SSH URL, and then click the clipboard button next to the URL to copy it.\n\n\n\n\n\n\ngit clone git@github.com:jelmerp/originspecies.git\n\nNow, we can make changes to the repository in the familiar way:\necho \"# Chapter 1. Variation under domestication\" &gt; origin.txt\n\ngit add origin.txt\ngit commit -m \"Suggested title for first chapter.\"\n\ngit push origin\n\n\n\nCreating a Pull Request\n\nIf we then go back to GitHub, we see that our forked repo is “x commit(s) ahead” of the original repo:\n\n\n\n\n\n\n\nClick Pull Request, and check whether the right repositories and branches are being compared: You can also see the changes that were made in this window.\n\n\n\n\n\n\n\nIf it looks good, click the green Create Pull Request button:\n\n\n\n\n\n\n\nGive your pull request a title, and write a brief description of your changes:\n\n\n\n\n\n\n\n\nKeeping your fork up-to-date\n\nAs we saw, you can’t directly push to original repo but instead have to submit a Pull Request (yes, this terminology is confusing!).\nBut, you can create an ongoing connection to the original repo, which you can use to periodically pull to keep your fork up-to-date.\nThis works similarly to connecting your own GitHub repo:\n\ngit remote add upstream git@github.com:jelmerp/originspecies.git\n\n# List the remotes:\ngit remote -v\norigin   git@github.com:pallass-boszanger/originspecies.git  (fetch)\norigin   git@github.com:pallass-boszanger/originspecies.git  (push)\nupstream   git@github.com:jelmerp/originspecies.git  (fetch)\nupstream   git@github.com:jelmerp/originspecies.git  (push)\n\n# Pull from the upstream repository:\ngit pull upstream main\n\n\n\n\n\n\nNote\n\n\n\n“upstream” is an arbitrary name often used by convention,\nas opposed to “origin” for your own version of the online repo."
  },
  {
    "objectID": "week3/w3_git3.html#miscellaneous",
    "href": "week3/w3_git3.html#miscellaneous",
    "title": "Git: Undoing",
    "section": "4 Miscellaneous",
    "text": "4 Miscellaneous\n\n4.1 Amending commits\nLet’s say we forgot to add a file to a commit, or we notice a silly typo in something we just committed.\nCreating a separate commit for this seems “wasteful” or even confusing,\nand including these changes along with others in a next commit is also likely to be inappropriate. In such cases, we can amend the commit.\n\nFirst, we stage the forgotten or fixed file:\ngit add &lt;file&gt;\nThen, we amend the commit (also using the --no-edit flag because we do not want change the commit message):\ngit commit --amend --no-edit\n\n\n\n\n\n\n\nWarning\n\n\n\nBecause amending commits “changes history”, some recommend avoiding this altogether. For sure, do not amend commits that have been published in (pushed to) the online counterpart of the repo.\n\n\n\n\n4.2 git stash\nGit stash can be useful when you need to pull from remote, but have changes in your working dir that:\n\nAre not appropriate for a separate commit\nAre not worth starting a new branch for, because you want to get these changes back on the current branch immediately.\n\n# Stash changes to tracked files:\ngit stash    # Note: add option -u to include untracked files\n\n# Pull from the remote repository:\ngit pull\n\n# Apply stashed changes:\ngit stash apply\n\n\n4.3 More Git tidbits\n\nGit will not pay attention to empty directories.\nYou can create a new branch and move to it in one go using:\ngit checkout -b &lt;new-branch-name&gt;\nTo show commits in which a specific file was changed, you can simply use:\ngit log &lt;filename&gt;\n“Aliases” (command shortcuts) can be useful with Git, and can be added to the ~/.gitconfig file or set with git config:\n[alias]\n  hist = log --graph --pretty=format:'%h %ad | %s%d [%an]' --date=short\n  last = log -1 HEAD  # Just show the last commit\ngit config --global alias.last \"log -1 HEAD\"\n\n\n\n4.4 Add a collaborator in GitHub\n\nYou can add a collaborator to a repository by going to the repository’s settings:"
  },
  {
    "objectID": "week3/w3_git3.html#undoing-i-changes-that-have-not-been-committed",
    "href": "week3/w3_git3.html#undoing-i-changes-that-have-not-been-committed",
    "title": "Git: Undoing",
    "section": "5 Undoing I: changes that have not been committed",
    "text": "5 Undoing I: changes that have not been committed\n\n\n\n\n\n\n\n\n\n\n\n5.1 Recovering a version from the repo\nThis applies to changes that have not been staged.\n\nLet’s say we accidentally overwrite instead of append to a file:\necho \"todo: ask sequencing center about adapters\" &gt; README.md\nAlways start by checking the status:\ngit status\nWe want to “discard changes in working directory” by recovering the version of the file in the index (and HEAD), and Git told us how to do this:\ngit checkout -- README.md\n\n\n\n\n\n\n\nNote\n\n\n\nFor git checkout, the CSB book example omits the dashes --. These indicate that the checkout command should operate on a file, but since the filename is provided too, this is not strictly necessary.\n\n\nIf you accidentally deleted a file, you can similarly retrieve it with git checkout:\ngit checkout -- deleted-file.txt\nFor recent Git versions (not yet on OSC), Git will instead of git checkout recommend the following when you do git status:\ngit restore README.md\n\n\n\n\n\n\n\n5.2 Unstaging a file\n\ngit reset can unstage a file, which is most often needed when you added a file that was not supposed to be part of the next commit:\necho \"The following TruSeq adapters were used:\" &gt;&gt; README.md\necho \"wc -l *fastq\" &gt; count_reads.sh\ngit add --all\nOops, those two file changes should be part of separate commits.\nAgain, we check the status first – and learn we should use git reset:\ngit reset HEAD README.md\n\n\n\n\n\n\n\nWarning\n\n\n\ngit reset will only unstage and not revert the file back to its state at the last commit (cf. CSB — mistake in the book!).\n(git reset --hard does revert things back to the state of a desired commit, but only works on commits and not individual files.)\n\n\n\nIf you staged a file and realize you made a mistake, or staged prematurely, you can continue editing the file and re-add it.\nFor recent Git versions (not yet on OSC), Git will instead of git reset recommend the following when you do git status:\ngit restore --staged README.md\n\n\n\n\n\n\n\n\n5.3 Undoing staged changes\nWhat if we had staged our mistaken changes, and we need to recover the file from the last commit?\nFor instance, we overwrote the README.md and staged the misshapen file:\necho \"Todo: ask sequencing center about adapters\" &gt; README.md\ngit add README.md\nTo recover the version stored in the last commit, and disregard any staged and unstaged changes to the file:\ngit checkout HEAD -- README.md\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with the git checkout command,\nbecause it irrevocably discards the non-committed changes.\nMore broadly: your data is only safe with Git once it has been committed.\n\n\n\n\n\n\n\n\n\n5.4 Summary\nFor a particular file README.md, I want to:\n\nUnstage the file, but don’t discard (replace) its changes:\ngit reset HEAD -- README.md\nGo back to the last commit and discard unstaged changes:\ngit checkout -- README.md # Technically: grabs file from *Index*\nGo back to the last commit and discard staged changes\n(and any unstaged changes if those are also present):\ngit checkout HEAD -- README.md\nBonus: undo any and all staged and unstaged changes (unsafe!):\ngit reset --hard HEAD"
  },
  {
    "objectID": "week3/w3_git3.html#undoing-ii-changes-that-have-been-committed",
    "href": "week3/w3_git3.html#undoing-ii-changes-that-have-been-committed",
    "title": "Git: Undoing",
    "section": "6 Undoing II: changes that have been committed",
    "text": "6 Undoing II: changes that have been committed\n\n6.1 Viewing past versions of the repository\nSay, we want to see what our project looked like at some point in the past.\n\nFirst, we print an overview of past commits and their messages:\ngit log --oneline --all --graph\nWe find a commit we want to go back to, and look around in the past:\ngit checkout &lt;sha-id&gt; # Replace &lt;sha-id&gt; by an actual hash\nless myfile.txt       # Etc. ...\nTo go back to where we were originally:\ngit checkout master\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to move your repo back to this earlier state,\nthere are several strategies — see the next section.\nFor a single file, a quick way can be: copy it out of your repo, move back to the “present”, and put it back in your repo in the present.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote the confusing re-use of git checkout!\nWe have now seen git checkout being used to: - Move between branches - Revert files back to previous states - Move to previous commits to explore (figure below)\n\n\n\n\n\nFigure from swcarpentry.github.io\n\n\n\n\n6.2 Undoing entire commits\n\nTo undo commits, i.e. move the state of your repository back to how it was before the commit you want to undo, there are two main commands:\n\ngit revert: Undo the changes made by commits by reverting them in a new commit.\ngit reset: Delete commits as if they were never made.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUndoing with git revert is much safer than with git reset, because git revert does not erase any history.\nFor this reason, some argue you should not use git reset on commits altogether. At any rate, you should never use git reset for commits that have already been pushed online.\n\n\n\n\n6.3 Undoing commits with git revert\n\nCreate a new commit that will revert all changes made in the\nspecified commit:\ngit revert HEAD     # Undo changes by most recent commit\n\ngit revert HEAD^    # Undo changes by second-to-last commit\ngit revert e1c5739  # Undo changes by any arbitrary commit\n\n\n\n6.4 Undoing commits with git reset\ngit reset is quite complicated as it has three modes (--hard, --mixed (default), and --soft) and can act either on individual files and on entire commits.\nWe’ve already used git reset on individual files (to unstage).\nTo undo a commit, and:\n\nStage all changes made by that commit:\ngit reset --soft HEAD^ #Undo LAST=reset to 2nd-to-last\nPut all changes made by that commit in the working dir:\ngit reset [--mixed] HEAD^ # --mixed is default\nCompletely discard all changes made by that commit:\ngit reset --hard HEAD^ \n\n\n\n6.5 Viewing & reverting to earlier versions of files\n\nGet a specific version of a file from a past commit:\ngit checkout HEAD^^ -- README.md   # From second-to-last commit\ngit checkout e1c5739 -- README.md  # From arbitrary commit\nYour now have the old version in the working dir & staged:\ncat README.md\ngit status\nYou can go on to commit this version from the past, or go back current version as we will do below:\ngit checkout HEAD -- README.md\n\nless README.md  # Back to the current version\ngit status      # Up-to-date\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with git checkout as any uncommitted changes to the same file would be overwritten by the past version you are getting!\n\n\nAn alternative method to view and revert to older versions of specific files is to use git show.\n\nWe can view a file from any commit as follows:\ngit show HEAD:README.md    # Version in the last commit\n\ngit show ad4ca74:README.md # An arbitrary version\nThanks to our shell skills, it’s also easy to revert a file to a previous version in this way:\ngit show ad4ca74:README.md &gt; README.md"
  },
  {
    "objectID": "week3/w3_git1.html#an-introduction-to-version-control",
    "href": "week3/w3_git1.html#an-introduction-to-version-control",
    "title": "Getting started with Git",
    "section": "1 An introduction to version control",
    "text": "1 An introduction to version control\n\n1.1 Why use a Version Control System (VCS)?\nHere are some “versioning”-related challenges for your research project files that you may run into when not using a formal Version Control System (VCS):\n\nWhat to save periodic copies of?\n\nDo you only save versions of individual files?\nSpace-efficient, but doesn’t allow you to go back to the state of other project files at the same point in time.\nDo you save a copy of the full project periodically?\nBetter than the above option, but can become prohibitive in terms of disk storage.\n\nHow to know what changes were made between saved versions? Try to summarize this in the file name?\nHow to manage simultaneous variants of files, such as when making experimental changes?\nHow to collaborate, especially when working simultaneously?\nHow to restore an accidentally deleted, modified, or overwritten file? This can especially be an issue at OSC where there is no recycle bin or undo button.\n\nA formal VCS can help you with these challenges. With a VCS:\n\nYou can easily see your history of changes.\nYou have a time machine: you can go back to past states of your project (and not just of individual files!).\nYou can do simultaneous collaborative work — you can always track down who made which changes.\nYou can make experimental changes without affecting current functionality.\nSharing your code and other aspects of your project is easy.\n\n\nOr, as the CSB book put it:\n\nVersion control is a way to keep your scientific projects tidily organized, collaborate on science, and have the whole history of each project at your fingertips.\n— CSB Chapter 2\n\n\n\n\n1.2 How Git roughly works\nGit is the most widely used Version Control System1. With Git, you save snapshots of your entire project with every minor piece of progress. Git manages this cleverly without having to create full copies of the project for every snapshot:\n\n\n\nThe boxes with dashed lines depict files that have not changed: these will not be saved repeatedly.Figure from https://git-scm.com\n\n\n\nAs illustrated above, files that haven’t changed between snapshots are not saved again and again. But Git doesn’t even save full copies of files that have changed: it tracks changes on a line-by-line bases, and saves changed lines!\nNote that one Git database (repository) manages files inside a single directory structure, so it is important that your project(s) are properly organized or at least kept in separate dirs, as discussed last week.\n\n\nKey Git term 1: Repository\nA “repository” (or “repo”) is the version-control database for a project:\n\nIt is saved in a hidden folder .git in the root dir of your project.\nYou can start a repository in any dir on your computer.\nTypically, you should have one Git repository for one research project.\nYou can also download any public online Git repository. In fact, we already did this in week 1, when we used git clone to download the CSB book’s repository.\n\n\n\n\n\n\n\nWhereas Git is software for version control, GitHub is a website to host Git repositories online2.\n\n\n\n\n\n\n\n\nKey Git term 2: Commit\nA “commit” is a saved snapshot of the project:\n\nIt is always possible to go the exact state of the entire project or individual files for any commit.\n\nWhenever you commit, you also include a message describing the changes.\n\n\n\n\n\n1.3 What do I put under version control?\nThe primary files to put under version control are:\n\nScripts.3\nProject documentation files.\nManuscripts, if you write them in a plain text format.\nMetadata.\n\nWhat about data and results?\n\nRaw data may or may not be committed — for omics data, this is generally not feasible due to large file sizes.\nResults should generally not be committed.\n\n\n\nSource versus derived files\nThe general idea is that you should version-control the source, not derived files. For instance:\n\nVersion-control your Markdown file, not the HTML it produces.\nVersion-control your script, not the results it produces.\n\n\n\n\n\n\n\nDerived files\n\n\n\nRecall last week’s point that results and other derived files are (or should be) dispensable, because they can be regenerated using the raw data and the scripts.\n\n\n\n\n\nFile limitations\nThere are some limitations to the types and sizes of files that can be committed:\n\nFile type: binary (non-text) files, such a Word or Excel files, or compiled software, can be included but can’t be tracked in quite the same way as plain-text files.4\nFile size: GitHub will not allow files over 100 MB.\nRepository size: it’s best to keep individual repositories under about 1 GB.\n\nAs such, omics data is usually too large to be version-controlled. You should use dedicated repositories for this like the NCBI’s Sequence Read Archive (SRA).\n\n\n\n\n1.4 User Interfaces for Git\n\n\n\nBy xkcd\n\n\nYou can work with Git in several different ways — using:\n\nThe native command-line interface (CLI).\nThird-party GUIs such as Git Kraken.\nIDEs/editors with Git integration like RStudio and VS Code.\n\nIn this course, we will mainly focus on the CLI because it’s the most universal and powerful interface. But it’s absolutely fine to (partially) switch to GUI usage later, which will not be hard if you’ve learned the basics with the CLI.\nGit takes some getting used to, regardless of the interface. Many people have one or more “false starts” with it. I hope that being “forced” to use it in a course5 will take you past that!"
  },
  {
    "objectID": "week3/w3_git1.html#the-basic-git-workflow",
    "href": "week3/w3_git1.html#the-basic-git-workflow",
    "title": "Getting started with Git",
    "section": "2 The basic Git workflow",
    "text": "2 The basic Git workflow\nGit commands always start with git followed by a second command/subcommand or “verb”: git add, git commit, etc. Only three commands tend to make up the vast majority of your Git work:\n\ngit add does two things:\n\n\nStart tracking files (needed because files in your directory structure are not automatically tracked).\nMark changed/new files as ready to be committed, which is called “staging” files.\n\ngit commit\nCommit all currently staged files (changes): create a new snapshot of the project.\ngit status\nGet the status of your repo: which files have changed, which new files are present, tips on next steps, etc.\n\n\n\n\nAdding and committing changes with Git commands.The Git database, which is in a hidden folder .git, is depicted with a gray background.\n\n\n\n\n\n\nAnother way of visualizing the adding and committing of changes in Git.Note that git add has a dual function: it starts tracking files and stages them."
  },
  {
    "objectID": "week3/w3_git1.html#getting-set-up",
    "href": "week3/w3_git1.html#getting-set-up",
    "title": "Getting started with Git",
    "section": "3 Getting set up",
    "text": "3 Getting set up\nWe will start with loading the correct Git version at OSC and then do some one-time personal Git configuration:\n\nLaunch VS Code at https://ondemand.osc.edu as before, at the dir /fs/ess/PAS2700/users/$USER, and open a terminal in VS Code.\nLoad the most recent version of the OSC Git module:\nmodule load git/2.39.0\nUse git config to make your (actual, not user) name known to Git:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known to Git (make sure that you use the same email address you used to sign up for GitHub):\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\nOccasionally, when you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line, Git will open up a text editor for you. Even though we’ll be mostly be working in VS Code during this course, in this case, it is better to select a text editor that can be run within the terminal, like nano (or vim, if you are familiar with it). To specify Nano as your default text editor for Git:\ngit config --global core.editor \"nano -w\"\nCheck whether you successfully changed the settings:\ngit config --global --list\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\nActivate colored output if needed. Make sure you see colour.ui=true in the list (like above), so Git output in the terminal will use colors. If you don’t see this line, set it using:\ngit config --global color.ui true"
  },
  {
    "objectID": "week3/w3_git1.html#your-first-repository",
    "href": "week3/w3_git1.html#your-first-repository",
    "title": "Getting started with Git",
    "section": "4 Your first repository",
    "text": "4 Your first repository\nHere, we will create a repository for a mock book project: the writing of Charles Darwin’s “On the Origin of Species”.\n\n\n4.1 Start a new Git repository\nCreate a dir for a mock project that we will version-control with Git:\n# Before starting, you should be in /fs/PAS2700/users/$USER, cd there first if needed\nmkdir -p week03/originspecies\ncd week03/originspecies\nThe command to initialize a new Git repository is git init — we’ll use that to start a Git repo for the originspecies directory:\ngit init\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/week03/originspecies/.git/\nNext, we can check the status of our new repository with git status:\ngit status\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\n\n\n\n4.2 Your first Git commit\nWe will start writing the book by creating a new file with touch, and echo-ing some text into it:\ntouch origin.txt\necho \"An Abstract of an Essay on ...\" &gt; origin.txt\nNow, let’s check the status of the repository again:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        origin.txt\nWhat’s happening here is that Git has detected the new file — but as mentioned above, Git does not automatically start tracking files. It tells us the file is “Untracked” and gives us a hint on how we can add it to the repository.\nSo, we start tracking the file and stage it all at once with git add:\n# (Note that tab-completion on file names will work here, too)\ngit add origin.txt\nCheck the status of the repo again:\ngit status\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   origin.txt\nNow, our file has been added to the staging area (also called the Index) and is listed as a “change to be committed” (we also get a hint on how we can “unstage” the file: i.e., reverting what we just did with git add and leaving the file untracked once again). This means that if we now run git commit, the file would be included in that commit.\nSo, with our file tracked & staged, let’s make our first commit — note that we have to use the option -m followed by a string which is the “commit message”: this is supposed to be a short description of the changes we are including in the current commit.\n# We use the commit message (option '-m') \"Started the book\" to describe our commit\ngit commit -m \"Started the book\"\n[main (root-commit) 3df4361] Started the book\n 1 file changed, 1 insertion(+)\n create mode 100644 origin.txt\n\nNow that we’ve made a commit, let’s check the status of the repo again:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\n\nTry to get used to using git status a lot — as a sanity check before and after other git actions.\n\n\n\n\n\n\nWe will also look at the commit history of the repo with git log:\ngit log\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21 (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\nNote the “hexadecimal code” (which uses numbers and the letters a-f) on the first line — this is a unique identifier for each commit, called the SHA-1 checksum. You can reference and access each past commit with these checksums.\n\n\n\n4.3 Your second commit\nWe will start by modifying the book file — we’ll actually overwrite the earlier content:\necho \"On the Origin of Species\" &gt; origin.txt\nCheck the status of the repo:\ngit status\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   origin.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nWe see that Git has noticed the changes, because the file is being tracked: origin.txt is listed as a “modified” file. But changes to tracked files aren’t automatically staged, so we need to use git add to stage the file as a first step to committing these changes:\ngit add origin.txt\nNow, we’re ready to commit:\ngit commit -m \"Changed the title as suggested by Murray\"\n[main f106353] Changed the title as suggested by Murray\n 1 file changed, 1 insertion(+), 1 deletion(-)\nGit is giving us a brief summary of the changes that were made: we changed 1 file (origin.txt), and since we replaced the first and only line of text in that file, it is interpreting that as 1 insertion (the new line) and 1 deletion (the removed/replace line).\nLet’s check the history of the repo again — we’ll see that there are now 2 commits:\ngit log\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\n\n\n\n\n\n\n\nStaging files efficiently\n\n\n\nWhen you have multiple files that you would like to stage, you don’t need to add them one-by-one:\n# NOTE: Don't run any of this - these are hypothetical examples\n\n# Stage all files in the project:\ngit add --all\ngit add *\n\n# Stage all files in a specific dir (here: 'scripts') in the project:\ngit add scripts/*\n\n# Stage all shell scripts *anywhere* in the project:\ngit add *sh   \nFinally, you can use the -a option for git commit as a shortcut to stage and commit all changes with a single command (but note that this will not add untracked files):\n# Stage & commit all tracked files:\ngit commit -am \"My commit message\"\n\n\n\n\n\n4.4 What to include in individual commits\nIn the last example in the box above, you saw the -a option to git commit, which allows you to all at once stage & commit all changes that you made after the last commit. That seems much more convenient than separately adding individual files. But thinking about the purposes of version control broadly, what could be a disadvantage of committing all changes simultaneously?\nIt’s good practice to not simply and only commit, say, at the end of each day, but to try and create commits for units of progress worth saving and as such create separate commits for distinct changes.\nFor example, let’s say that you use git status to check which files you’ve changed since your last commit, and you find that you have:\n\nUpdated a README file to include more information about your samples.\nWorked on a script to run quality control of sequence files.\n\nThese are completely unrelated changes, and it would not be recommended to include both in a single commit.\n\n\n Exercise (CSB Intermezzo 2.1)\n\nCreate a new file todo.txt containing the line: “June 18, 1858: read essay from Wallace”.\n\n\n\nClick to see the solution\n\necho \"June 18, 1858: read essay from Wallace\" &gt; todo.txt\n\n\nUse a Git command to stage the file.\n\n\n\nClick to see the solution\n\ngit add todo.txt\n\n\nCreate a Git commit with the commit message “Added to-do list”.\n\n\n\nClick to see the solution\n\ngit commit -m \"Added to-do list\""
  },
  {
    "objectID": "week3/w3_git1.html#showing-changes-and-referring-to-the-past",
    "href": "week3/w3_git1.html#showing-changes-and-referring-to-the-past",
    "title": "Getting started with Git",
    "section": "5 Showing changes and referring to the past",
    "text": "5 Showing changes and referring to the past\n\n5.1 File states (and the three “trees”)\nTracked files can be in one of three states:\n\nUnchanged since the last commit: committed.\nModified and staged since the last commit: staged.\nModified but not staged since the last commit: modified.\n\n\n\n\n\n\n\n\nThe three trees of Git\n\n\n\nThese three states correspond to the three “trees” of Git:\n\nHEAD: State of the project in the most recent commit6.\nIndex (Stage): State of the project ready to be committed.\nWorking directory: State of the project as currently on your computer.\n\n\n\n\n\nThe three “trees” of Git: HEAD, the index, and the working dir.The hexadecimals in the Commits rectangles are abbreviated checksums for each commit.\n\n\n\nOr consider this table for a hypothetical example in which HEAD, the Index, and the working dir all differ with regards to the the version of file 1, and there also is an untracked file in the working dir:\n\n\n\n\n\n\n\n\nFile state\nVersion\nWhich tree\n\n\n\n\nCommitted\nfile 1 version X\nHEAD\n\n\nStaged\nfile 1 version Y\nIndex (stage)\n\n\nModified\nfile 1 version Z\nWorking dir\n\n\nUntracked\nfile 2 version X\nWorking dir\n\n\n\n\n\n\n\n\n\n\n\n\nWays to refer to past commits (Click to expand)\n\n\n\n\n\nTo refer to specific past commits, you can:\n\nUsing the hexadecimal checksum (either the full ID or the 7-character abbreviation)\nUse HEAD notation: HEAD is the most recent commit, and there are two ways of indicating ancestors of HEAD:\n\n\n\n\n\nTo refer to past commits, you can use checksums (e.g. dab0dc4 for the second-to-last commit)or HEAD notation (HEAD^^ or HEAD~2 for the second-to-last commit).\n\n\n\n\n\n\n\n\n5.2 Showing changes\nYou can use the git diff command to show specific changes that you have made. By default, git diff will show all changes between the working dir and:\n\nIf something has been staged: the Index (stage).\nIf nothing has been staged: the last commit.\n\nRight now, there are no differences to report in our originspecies repository, because our working dir, the stage/Index, and the last commit are all the same:\n# Git diff will not have output if there are no changes to report\ngit diff\nWe change the to-do list (note: for this to work, you should have done the exercise above!), and check again:\necho \"June 20, 1858: Send first draft to Huxley\" &gt;&gt; todo.txt\n\ngit diff\ndiff --git a/todo.txt b/todo.txt\nindex e3b5e55..9aca508 100644\n--- a/todo.txt\n+++ b/todo.txt\n@@ -1 +1,2 @@\n June 18, 1858: read essay from Wallace\n+June 20, 1858: Send first draft to Huxley\nWe won’t go into the details of the “diff format” that git diff uses, but at the bottom of the output above, you can see some specific changes: the line “Send first draft to Huxley” was added (hence the + sign) in our latest version of the file.\nIf you have changed multiple files, but just want to see differences for one of them, you can specify the filename — in our case here, that will give the same output as the plain git diff command above, since we only changed one file:\ngit diff todo.txt\n# Output not shown, same as above\n\n\n\n\n\n\nVS Code can nicely show differences between files too:\n\n\n\n\nClick on the Git symbol in the narrow side bar to open the Source Control side bar.\nIn the source control sidebar, click on the M next to the file todo.txt.\n\n\n\n\n\n\n\n\n\nAdvanced git diff (Click to expand)\n\n\n\n\n\n\nTo show changes between the Index (stage) and the last commit, use the --staged option to git diff.\nYou can also compare your repo or individual files between any two arbitrary commits:\n# Last commit vs second-to-last commit - full repo:\ngit diff HEAD HEAD^\n\n# Last commit vs a specified commit - specific file: \ngit diff HEAD d715c54 todo.txt \n\n\n\n\n\n Exercise\nStage and commit the changes to todo.txt, then check what you have done.\n\n\nClick to see the solution\n\n\nStage the file:\ngit add todo.txt\nCommit:\ngit commit -m \"Update the TODO list\"\n[main 8ec8103] Update the TODO list\n1 file changed, 1 insertion(+)\nCheck the log:\ngit log\ncommit 8ec8103e8d01b342f9470908b87f0649be53edd5\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 12:30:35 2024 -0400\n\n    Update the TODO list\n\ncommit 9715ab5325429526a90ea49e9d40a923c93ccb72\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:37:32 2024 -0400\n\n    Added a gitignore file\n\ncommit 603d1792619bf628d66cd91a45cd7114e3d6b95b\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:21:36 2024 -0400\n\n    Added to-do list\n\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book"
  },
  {
    "objectID": "week3/w3_git1.html#ignoring-files-and-directories",
    "href": "week3/w3_git1.html#ignoring-files-and-directories",
    "title": "Getting started with Git",
    "section": "6 Ignoring files and directories",
    "text": "6 Ignoring files and directories\nAs discussed above, it’s best not to track some files, such as very bulky data files, temporary files, and results.\nWe’ve seen that Git will notice and report any “untracked” files in your project whenever you run git status. This can get annoying and can make it harder to spot changes and untracked files that you do want to add, and you might also accidentally start tracking files such as with git add --all.\nTo deal with this, you can tell Git not to pay attention to certain files by adding file names and wildcard selections to a .gitignore file. This way, these files won’t be listed as untracked files.\nTo see this in action, let’s start by adding some content that we don’t want to commit to our repository: a dir data, and a file ending in a ~ (a temporary file type that e.g. text editors can produce):\nmkdir data\ntouch data/drawings_1855-\\{01..12\\} todo.txt~\nWhen we check the status of the repo, we can see that Git has noticed these files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        todo.txt~\nIf we don’t do anything about this, Git will keep reporting these untracked files whenever we run git status. To prevent this, we will we create a .gitignore file:\n\nThis file should be in the project’s root dir and should be called .gitignore.\nOnce such a file exists, Git will automatically check and process its contents.\n.gitignore is a plain text file that contains dir and file names/patterns, all of which will be ignored by Git.\n\n\n\n\n\n\n\nHidden files\n\n\n\nThe leading . in the file name means that this is a so-called “hidden file”. Hidden files don’t show up in file browsers by default, and don’t show up in ls file listings unless you use the -a (all) option.\n\n\nWe will create our .gitignore file and add the following to it to instruct Git to ignore everything in the data/ dir, and any file that ends in a ~:\necho \"data/\" &gt; .gitignore\necho \"*~\" &gt;&gt; .gitignore\ncat .gitignore\ndata/\n*~\nWhen we check the status again, Git will have automatically processed the contents of the .gitignore file, and the files we want to ignore should no longer be listed as untracked files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\nHowever, we do now have an untracked .gitignore file, and we should track and commit this file:\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\n[main 9715ab5] Added a gitignore file\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\n\n\n\n\n\n\n\nGood project file organization helps with version control\n\n\n\nGood project file organization, as discussed last week, can make your life with Git a lot easier. This is especially true when it comes to files that you want to ignore.\nSince you’ll generally want to ignore data and results files, if you keep all of those in their own top-level directories, it will be easy and not error-prone to tell Git to ignore them.\nBut if, for example, you were mixing scripts and either results or data within dirs, it would be much harder to keep this straight for Git."
  },
  {
    "objectID": "week3/w3_git1.html#moving-and-removing-tracked-files",
    "href": "week3/w3_git1.html#moving-and-removing-tracked-files",
    "title": "Getting started with Git",
    "section": "7 Moving and removing tracked files",
    "text": "7 Moving and removing tracked files\nWhen wanting remove, move, or rename files that are tracked by Git, it is good practice to preface regular rm and mv commands with git: so, git rm &lt;file&gt; and git mv &lt;source&gt; &lt;dest&gt;.\nWhen removing or moving/renaming a tracked file with git rm / git mv, changes will be made to your working dir just like with a regular rm/mv, and the operation will also be staged. For example:\n# (Don't run this, hypothetical examples)\ngit rm file-to-remove.txt\ngit mv myoldname.txt mynewname.txt\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n        deleted:    file-to-remove.txt\n\n\n\n\n\n\nWhat if I forget to use git rm/git mv? (Click to expand)\n\n\n\n\n\nIt is inevitable that you will occasionally forget about this, in which case Git will eventually figure out what happened. For example:\n\nFor a renamed file, Git will first be confused and register a removed file and an added file:\n# (Don't run this, hypothetical examples)\nmv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    myoldname.txt\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        mynewname.txt\nBut after you stage both changes (the new file and the deleted file), Git realizes it was renamed.\ngit add myoldname.txt\ngit add mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n\nSo, there is no need to stress if you forget this, but when you remember, use git mv and git rm.\n\n\n\n\n\n Exercise\nTODO"
  },
  {
    "objectID": "week3/w3_git1.html#footnotes",
    "href": "week3/w3_git1.html#footnotes",
    "title": "Getting started with Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOthers include SVN and Mercurial.↩︎\nGitHub is the most widely used but not the only such website – others include GitLab, Bitbucket, and SourceForge.↩︎\nAnd if you’re writing software, all its source code.↩︎\nGit will just save an entirely new version whenever there’s been a change rather than tracking changes in individual lines.↩︎\nE.g., you’ll have to use Git for you final project.↩︎\nOn the current “branch” – see the optional self-study page or CSB chapter 2.6 to learn about branches.↩︎"
  },
  {
    "objectID": "week4/w4_overview.html",
    "href": "week4/w4_overview.html",
    "title": "Week 4",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/w4_exercises.html",
    "href": "week4/w4_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week5/w5_overview.html",
    "href": "week5/w5_overview.html",
    "title": "Week 5",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_github-account.html",
    "href": "week2/w2_github-account.html",
    "title": "Assignment: Create a GitHub account",
    "section": "",
    "text": "What?\nCreate a personal GitHub account.\n\n\nWhy?\nGitHub is a website that hosts Git repositories, i.e. version-controlled projects. In Week 3 of this course, you will be learning how to use Git together with GitHub. In addition, you will submit your final project assignments through GitHub.\n\n\nHow?\nIf you already have a GitHub account, log in and start at step 6.\n\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFollow the prompts to create your account — some notes:\n\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\n\nCheck your email and click the link to verify your email address.\nBack on GitHub website, now logged in: in the far top-right of the page, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Public profile” tab, enter your name.\nStill in the “Profile tab”, upload a Profile picture. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_overview.html#links",
    "href": "week2/w2_overview.html#links",
    "title": "Week 2: Project organization and Markdown",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Project organization\nTue: Markdown & VS Code\nThu: Managing files in the shell\n\n\n\nExercises & assignments\n\nAssignment: Create a GitHub account\nWeek 2 exercises"
  },
  {
    "objectID": "week2/w2_overview.html#content-overview",
    "href": "week2/w2_overview.html#content-overview",
    "title": "Week 2: Project organization and Markdown",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we’ll talk about some best practices for project organization, managing your project’s files in the Unix shell, and documenting your project with Markdown files. We’ll also spend a bit of time getting to know VS Code, the text editor that you will spend a lot of time in during this course.\nSome of the things you will learn this week:\n\nA number of best practices for project organization, documentation, and management.\nHow to apply these best practices this in the shell.\nHow to use Markdown for documentation (and beyond).\nGet to know our text editor for the course, VS Code, a bit better."
  },
  {
    "objectID": "week2/w2_overview.html#readings",
    "href": "week2/w2_overview.html#readings",
    "title": "Week 2: Project organization and Markdown",
    "section": "3 Readings",
    "text": "3 Readings\nThis week’s reading is Chapter 2 of our secondary book, “Bioinformatics Data Skills” (“Buffalo” for short – the author is Vince Buffalo), which deals with project organization and (file) management.\nThe chapter also contains several tips and tricks for using the shell for project organization, which we will work through in the Zoom sessions. For more material along these lines, optional reading is Buffalo Chapter 3.\n\nRequired readings\n\nBuffalo Chapter 2: “Setting up and Managing a Bioinformatics Project”\n\n\n\nOptional readings\n\nBuffalo Chapter 3: “Remedial Unix Shell”\n\n\n\nFurther resources\n\nWilson et al. 2017, PLOS Computational Biology: “Good enough practices in scientific computing”\nKieran Healy: “The Plain Person’s Guide to Plain Text Social Science”"
  },
  {
    "objectID": "week2/w2_shellfiles.html#overview-setting-up",
    "href": "week2/w2_shellfiles.html#overview-setting-up",
    "title": "Managing files in the shell",
    "section": "1 Overview & setting up",
    "text": "1 Overview & setting up\nIn this session, we will learn some more Unix shell skills, focusing on commands to manage files with an eye on organizing your research projects.\nSpecifically, we will learn about:\n\nWildcard expansion to select and operate on multiple files at once\nBrace expansion to help create regular series of files and dirs\nCommand substitution to save the output of commands\nFor loops to repeat operations across, e.g., file\nRenaming multiple files using for loops\n\nAnd for those that are interested, there is some optional at-home reading about changing file permissions (e.g. to make your raw data read-only) and creating symbolic links (to access files across different projects).\n\n\n1.1 VS Code setup\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2700\nThe starting directory: /fs/ess/PAS2700/users/&lt;user&gt; (replace &lt;user&gt; with your OSC username)\nNumber of hours: 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nType pwd to check where you are.\nIf you are not in /fs/ess/PAS2700/users/&lt;user&gt; click      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS2700/users/&lt;user&gt; and press OK.\n\n\n\n\n\n\n\n1.2 Create a dummy project – following Buffalo\nGo into the dir for this week that you created earlier:\n# You should be in /fs/ess/PAS2700/users/$USER/\ncd week02\nFirst, we’ll create a set of directories representing a dummy research project:\nmkdir zmays-snps\ncd zmays-snps\n\n# The -p option for mkdir will allow for 'recursive' (nested) dir creation\nmkdir -p data/fastq scripts results/figs\nThe touch command will create one or more empty files. We will use it to create some empty files that are supposed to represent sequence files with forward (“R1”) and reverse (“R2”) DNA sequence reads for 3 samples:\ncd data/fastq\n\ntouch sample1_R1.fastq.gz sample1_R2.fastq.gz\ntouch sample2_R1.fastq.gz sample2_R2.fastq.gz\ntouch sample3_R1.fastq.gz sample3_R2.fastq.gz\nFor a nice recursive overview of your directory structure, use the tree command (with option -C to show colors):\n# \"../..\" tells tree to start two levels up\n# (Output colors are not shown on this webpage)\ntree -C ../..\n../..\n├── data\n│   └── fastq\n│       ├── sample1_R1.fastq.gz\n│       ├── sample1_R2.fastq.gz\n│       ├── sample2_R1.fastq.gz\n│       ├── sample2_R2.fastq.gz\n│       ├── sample3_R1.fastq.gz\n│       └── sample3_R2.fastq.gz\n├── results\n│   └── figs\n└── scripts\n\n5 directories, 6 files"
  },
  {
    "objectID": "week2/w2_shellfiles.html#wildcard-expansion",
    "href": "week2/w2_shellfiles.html#wildcard-expansion",
    "title": "Managing files in the shell",
    "section": "2 Wildcard expansion",
    "text": "2 Wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\n\nThe * wildcard\nIn globbing, the * wildcard matches any number of any character, including nothing.\nWith the following files in our directory:\nls\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz\nsample2_R2.fastq.gz  sample3_R1.fastq.gz  sample3_R2.fastq.gz\nWe can match both “sample1” files as follows:\nls sample1*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nls sample1*fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\n\nTo match only files with forward reads (contain “_R1”):\nls *_R1*\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\nls *R1.fastq.gz\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\n\nWhen globbing, the pattern has to match the entire file name, so this doesn’t match anything:\n# There are no files that _end in_ R1: we'd need another asterisk at the end\nls *R1\nls: cannot access *R1: No such file or directory\n\nIn summary:\n\n\n\n\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files)1\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nsample1*\nStart with “sample1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n\n Exercise: File matching 1\n\nList only the FASTQ files for sample 3.\n\n\n\nClick for the solution\n\nls sample3*\nsample3_R1.fastq.gz  sample3_R2.fastq.gz\n\n\nWhich files would ls samp*le* match?\n\n\n\nClick for the solution\n\nAll of them, since all file names start with sample, and because * also matches “zero characters”, there is no requirement for there to be a character between the p and the l.\n\n\n\n\nOther shell wildcards\nThere are two more shell wildcards, and here is a complete overview of shell wildcards:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\nUsing the ? wildcard to match both R1 and R2:\nls sample1_R?.fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nTo match files for sample1 and sample2 using only a character class with []:\n\nMethod 1 — List all possible characters (1 and 2 in this case):\nls sample[12]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 2 – Use a range like [0-9], [A-Z], [a-z]:\nls sample[1-2]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 3 – Exclude the unwanted sample ID:\nls sample[^3]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\n\n\n\n\n\n\n\n[] works on single character ranges only: 0-9 works but 10-13 does not.\n\n\n\n\n\n\nThe examples so far may seem trivial, but you can use these techniques to easily operate on selections among 100s or 1000s of files.\n\n\n\nExpansion is done by the shell itself\nThe expansion –to all matching file names– is done by the shell, not by ls or another command you might be using wildcards with. Therefore, ls will “see”/“receive” the list of files after the expansion has already happened.\nFor example: we can copy (cp command), move (mv) or delete (rm) files with shell expansion, and we can also first check which files those command will “see” by first using echo (or ls) with the exact same globbing pattern:\n# Check which files are selected \necho sample[12]*\nsample1_R1.fastq.gz sample1_R2.fastq.gz sample2_R1.fastq.gz sample2_R2.fastq.gz\n# Remove the files with rm\n# (The -v option will make rm report what it's removing)\nrm -v sample[12]*\nremoved ‘sample1_R1.fastq.gz’\nremoved ‘sample1_R2.fastq.gz’\nremoved ‘sample2_R1.fastq.gz’\nremoved ‘sample2_R2.fastq.gz’\n\n\n\n\n\n\nWildcards vs. regular expressions\n\n\n\nDon’t confuse wildcards with regular expressions! You may have used regular expressions before, for example with R or a text editor. They are similar to but not the same as shell wildcards. We’ll talk about regular expressions in Week 4."
  },
  {
    "objectID": "week2/w2_shellfiles.html#brace-expansion",
    "href": "week2/w2_shellfiles.html#brace-expansion",
    "title": "Managing files in the shell",
    "section": "3 Brace expansion",
    "text": "3 Brace expansion\nWhereas wildcard expansion looks for corresponding files and expands to whichever files are present, brace expansion with {}, is another type of shell expansion that expands to whatever you tell it to.\n# First move up to zmays-snps\ncd ../..\nUse .. within {} to indicate ranges of numbers or letters:\n# Here we'll create 31 _dirs_ for different dates\nmkdir -p data/obs/2024-03-{01..31}\n\nls data/obs\n2024-03-01  2024-03-04  2024-03-07  2024-03-10  2024-03-13  2024-03-16  2024-03-19  2024-03-22  2024-03-25  2024-03-28  2024-03-31\n2024-03-02  2024-03-05  2024-03-08  2024-03-11  2024-03-14  2024-03-17  2024-03-20  2024-03-23  2024-03-26  2024-03-29\n2024-03-03  2024-03-06  2024-03-09  2024-03-12  2024-03-15  2024-03-18  2024-03-21  2024-03-24  2024-03-27  2024-03-30\n# Here we'll create 6 empty _files_\ntouch results/figs/fig-1{A..F}.png\n\nls results/figs\nfig-1A.png  fig-1B.png  fig-1C.png  fig-1D.png  fig-1E.png  fig-1F.png\n\nFinally, you can also use a comma-separated list, and multiple brace expansions — with the latter, you will get all combinations among values in the expansions:\nmkdir -p data/obs2/treatment-{Kr,Df,Tr}_temp-{lo,med,hi}\n\nls data/obs2\ntreatment-Df_temp-hi   treatment-Kr_temp-hi   treatment-Tr_temp-hi\ntreatment-Df_temp-lo   treatment-Kr_temp-lo   treatment-Tr_temp-lo\ntreatment-Df_temp-med  treatment-Kr_temp-med  treatment-Tr_temp-med\n\n\n Exercise: File matching 2\n\nMove back into data/fastq/ and remove all (remaining) files in there in one go.\n\n\n\nClick for the solution\n\ncd data/fastq/   # Assuming you were still in /fs/ess/PAS2700/users/$USER/week02/zmays-snps\n\n# (You don't have to use the -v flag)\nrm -v *fastq.gz\nremoved ‘sample3_R1.fastq.gz’\nremoved ‘sample3_R2.fastq.gz’\n\n\nUsing brace expansion and the touch command, create empty R1 and R2 FASTQ files for 100 samples with IDs from 001 to 100: sample&lt;ID&gt;_R1.fastq and sample&lt;ID&gt;_R2.fastq.\n\n\n\nClick for the solution\n\ntouch sample{001..100}_R{1,2}.fastq\n\nls\nsample001_R1.fastq  sample026_R1.fastq  sample051_R1.fastq  sample076_R1.fastq\nsample001_R2.fastq  sample026_R2.fastq  sample051_R2.fastq  sample076_R2.fastq\nsample002_R1.fastq  sample027_R1.fastq  sample052_R1.fastq  sample077_R1.fastq\nsample002_R2.fastq  sample027_R2.fastq  sample052_R2.fastq  sample077_R2.fastq\nsample003_R1.fastq  sample028_R1.fastq  sample053_R1.fastq  sample078_R1.fastq\nsample003_R2.fastq  sample028_R2.fastq  sample053_R2.fastq  sample078_R2.fastq\nsample004_R1.fastq  sample029_R1.fastq  sample054_R1.fastq  sample079_R1.fastq\nsample004_R2.fastq  sample029_R2.fastq  sample054_R2.fastq  sample079_R2.fastq\nsample005_R1.fastq  sample030_R1.fastq  sample055_R1.fastq  sample080_R1.fastq\nsample005_R2.fastq  sample030_R2.fastq  sample055_R2.fastq  sample080_R2.fastq\nsample006_R1.fastq  sample031_R1.fastq  sample056_R1.fastq  sample081_R1.fastq\nsample006_R2.fastq  sample031_R2.fastq  sample056_R2.fastq  sample081_R2.fastq\nsample007_R1.fastq  sample032_R1.fastq  sample057_R1.fastq  sample082_R1.fastq\nsample007_R2.fastq  sample032_R2.fastq  sample057_R2.fastq  sample082_R2.fastq\nsample008_R1.fastq  sample033_R1.fastq  sample058_R1.fastq  sample083_R1.fastq\nsample008_R2.fastq  sample033_R2.fastq  sample058_R2.fastq  sample083_R2.fastq\nsample009_R1.fastq  sample034_R1.fastq  sample059_R1.fastq  sample084_R1.fastq\nsample009_R2.fastq  sample034_R2.fastq  sample059_R2.fastq  sample084_R2.fastq\nsample010_R1.fastq  sample035_R1.fastq  sample060_R1.fastq  sample085_R1.fastq\nsample010_R2.fastq  sample035_R2.fastq  sample060_R2.fastq  sample085_R2.fastq\nsample011_R1.fastq  sample036_R1.fastq  sample061_R1.fastq  sample086_R1.fastq\nsample011_R2.fastq  sample036_R2.fastq  sample061_R2.fastq  sample086_R2.fastq\nsample012_R1.fastq  sample037_R1.fastq  sample062_R1.fastq  sample087_R1.fastq\nsample012_R2.fastq  sample037_R2.fastq  sample062_R2.fastq  sample087_R2.fastq\nsample013_R1.fastq  sample038_R1.fastq  sample063_R1.fastq  sample088_R1.fastq\nsample013_R2.fastq  sample038_R2.fastq  sample063_R2.fastq  sample088_R2.fastq\nsample014_R1.fastq  sample039_R1.fastq  sample064_R1.fastq  sample089_R1.fastq\nsample014_R2.fastq  sample039_R2.fastq  sample064_R2.fastq  sample089_R2.fastq\nsample015_R1.fastq  sample040_R1.fastq  sample065_R1.fastq  sample090_R1.fastq\nsample015_R2.fastq  sample040_R2.fastq  sample065_R2.fastq  sample090_R2.fastq\nsample016_R1.fastq  sample041_R1.fastq  sample066_R1.fastq  sample091_R1.fastq\nsample016_R2.fastq  sample041_R2.fastq  sample066_R2.fastq  sample091_R2.fastq\nsample017_R1.fastq  sample042_R1.fastq  sample067_R1.fastq  sample092_R1.fastq\nsample017_R2.fastq  sample042_R2.fastq  sample067_R2.fastq  sample092_R2.fastq\nsample018_R1.fastq  sample043_R1.fastq  sample068_R1.fastq  sample093_R1.fastq\nsample018_R2.fastq  sample043_R2.fastq  sample068_R2.fastq  sample093_R2.fastq\nsample019_R1.fastq  sample044_R1.fastq  sample069_R1.fastq  sample094_R1.fastq\nsample019_R2.fastq  sample044_R2.fastq  sample069_R2.fastq  sample094_R2.fastq\nsample020_R1.fastq  sample045_R1.fastq  sample070_R1.fastq  sample095_R1.fastq\nsample020_R2.fastq  sample045_R2.fastq  sample070_R2.fastq  sample095_R2.fastq\nsample021_R1.fastq  sample046_R1.fastq  sample071_R1.fastq  sample096_R1.fastq\nsample021_R2.fastq  sample046_R2.fastq  sample071_R2.fastq  sample096_R2.fastq\nsample022_R1.fastq  sample047_R1.fastq  sample072_R1.fastq  sample097_R1.fastq\nsample022_R2.fastq  sample047_R2.fastq  sample072_R2.fastq  sample097_R2.fastq\nsample023_R1.fastq  sample048_R1.fastq  sample073_R1.fastq  sample098_R1.fastq\nsample023_R2.fastq  sample048_R2.fastq  sample073_R2.fastq  sample098_R2.fastq\nsample024_R1.fastq  sample049_R1.fastq  sample074_R1.fastq  sample099_R1.fastq\nsample024_R2.fastq  sample049_R2.fastq  sample074_R2.fastq  sample099_R2.fastq\nsample025_R1.fastq  sample050_R1.fastq  sample075_R1.fastq  sample100_R1.fastq\nsample025_R2.fastq  sample050_R2.fastq  sample075_R2.fastq  sample100_R2.fastq\n\n\n\nBonus: Count the number of “R1” files by first using ls with a globbing pattern that only selects R1 files, and then piping the ls output into wc -l.\n\n\n\nClick for the solution\n\n# wc -l will count the number of lines, i.e. the number of files\n# (Note that this works properly even though raw ls output may\n# put multiple files on 1 line.)\nls *R1*fastq | wc -l\n100\n\n\nBonus: Copy all files except the two for “sample100” into a new directory called selection — use a wildcard to do the move with a single command. (You will first need to create the new dir separately.)\n\n\n\nClick for the solution\n\nFirst create the selection dir:\nmkdir selection\nMethod 1 — Exclude sample numbers starting with a 1:\ncp sample[^1]* selection/\nMethod 2 — Other way around; include sample numbers starting with a 0:\ncp sample0* selection/"
  },
  {
    "objectID": "week2/w2_shellfiles.html#variables-and-command-substitution",
    "href": "week2/w2_shellfiles.html#variables-and-command-substitution",
    "title": "Managing files in the shell",
    "section": "4 Variables and command substitution",
    "text": "4 Variables and command substitution\n\n4.1 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs. Using variables makes it easier to change such settings. We also need to understand variables to work with loops and scripts.\n\nAssigning and referencing variables\nTo assign a value to a variable in the shell, use the syntax variable_name=value:\n# Assign the value \"beach\" to a variable with the name \"location\":\nlocation=beach\n\n# Assign the value \"200\" to a variable with the name \"n_lines\":\nn_lines=200\n\n\n\n\n\n\nRecall that there can’t be spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value):\n\nYou need to put a dollar sign $ in front of its name.\nIt is good practice to double-quote (\"...\") variable names2.\n\nAs before with the environment variable $HOME, we’ll use the echo command to see what values our variables contain:\necho \"$location\"\nbeach\necho \"$n_lines\"\n200\nConveniently, we can use variables in lots of contexts, as if we had directly typed their values:\ninput_file=results/figs/fig-1A.png\n\nls -lh \"$input_file\"\n-rw-rw----+ 1 jelmer PAS0471 0 Mar  7 13:17 results/figs/fig-1A.png\n\n\n\n\n\n\nRules and tips for naming variables (Click to expand)\n\n\n\n\n\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\n\n\n\n\n\n\n\n4.2 Command substitution\nCommand substitution allows you to store and pass the output of a command to another command. Let’s see an example. As you know, the date command will print the current date and time:\ndate\nThu Mar  7 14:52:22 EST 2024\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored instead:\ntoday=date\necho \"$today\"\ndate\nThat’s why we need command substitution, which we can use by wrapping the command inside $():\ntoday=$(date)\necho \"$today\"\nThu Mar  7 14:53:11 EST 2024\n\nOne practical example of using command substitution is when you want to automatically include the current date in a file name. First, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\ndate +%F\n2024-03-07\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\ntouch README_\"$(date +%F)\".txt\n\nls\nREADME_2024-03-07.txt\n\n\n Bonus exercise: Command substitution\nSay we wanted to store and report the number of lines in a FASTQ file, which tells us how many sequence “reads” are in it (because FASTQ files contain 4 lines per read).\nHere is how we can get the number of lines of a compressed FASTQ file:\n\nUse zcat (instead of regular cat) to print the contents despite the file compression\nAs we’ve seen before, wc -l gets you the number of lines, but note here that if you pipe input into wc -l, it won’t include the file name in the output:\n\nzcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l\n2000000\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print the following:\nThe file has 2000000 lines\n\n\nClick for the solution\n\nn_lines=$(zcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l)\n\necho \"The file has $n_lines lines\"\nThe file has 2000000 lines\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n4.3 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\nWhat was actually run under the hood is the following:\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/sample001_R1.fastq...\nRunning an analysis for file data/fastq/sample001_R2.fastq...\nRunning an analysis for file data/fastq/sample002_R1.fastq...\nRunning an analysis for file data/fastq/sample002_R2.fastq...\nRunning an analysis for file data/fastq/sample003_R1.fastq...\nRunning an analysis for file data/fastq/sample003_R2.fastq...\n# [...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "week2/w2_shellfiles.html#renaming-files-with-loops",
    "href": "week2/w2_shellfiles.html#renaming-files-with-loops",
    "title": "Managing files in the shell",
    "section": "5 Renaming files with loops",
    "text": "5 Renaming files with loops\nThere are many different ways to rename many files in a programmatic way in the shell – admittedly none as easy as one might have hoped.\nHere, we’ll use the basename command and a for loop. for loops are a verbose method for tasks like renaming, but are relatively intuitive and good to get practice with.\n\nbasename\nFirst, we’ll have to learn about the basename command, which removes any dir name that may be present in a file name (path), and optionally, removes a suffix too:\n# Just remove the directories:\nbasename data/fastq/sample001_R1.fastq\nsample001_R1.fastq\n# Also remove a suffix by specifying it after the file name:\nbasename data/fastq/sample001_R1.fastq .fastq\nsample001_R1\n\n\n\n\n\n\nDon’t have these FASTQ files? (Click to expand)\n\n\n\n\n\nWe made these in one of the exercises above, but if you don’t have them:\n# You should be in /fs/ess/PAS2700/users/$USER/week02\ntouch data/fastq/sample{001..100}_R{1,2}.fastq\n\n\n\n\n\n\nRenaming a single file\nLet’s say that we wanted to rename these files so that they have the suffix .fq instead of .fastq. Here’s how we could do that for one file in a way that we can use in a loop:\nThe original file name will be contained in a variable:\noldname=sample001_R1.fastq\nWe can also save the new name in a variable\nnewname=$(basename \"$oldname\" .fastq).fq\nBefore actually renaming, note this trick with echo to just print the command instead of executing it:\necho mv -v \"$oldname\" \"$newname\"\nmv -v sample001_R1.fastq sample001_R1.fq\nLooks good? Then we remove echo and rename the file (we’re using the -v to make mv report what it’s doing):\nmv -v \"$oldname\" \"$newname\"\nsample001_R1.fastq -&gt; sample001_R1.fq\n\n\n\nLooping over all files\nHere’s how we can loop over these files, saving each file name (one at a time) in the variable $oldname:\nfor oldname in *.fastq; do\n    # ...\ndone\nNext, we assign a new name for each file:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\ndone\nWe build and check the renaming command:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    echo mv -v \"$oldname\" \"$newname\"\ndone\nmv -v sample001_R1_001.fastq sample001_R1_001.fq\nmv -v sample001_R2_001.fastq sample001_R2_001.fq\nmv -v sample002_R1_001.fastq sample002_R1_001.fq\nmv -v sample002_R2_001.fastq sample002_R2_001.fq\n# [...output truncated...]\nWe do the renaming by removing echo:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    mv -v \"$oldname\" \"$newname\"\ndone\n‘sample001_R1_001.fastq’ -&gt; ‘sample001_R1_001.fq’\n‘sample001_R2_001.fastq’ -&gt; ‘sample001_R2_001.fq’\n‘sample002_R1_001.fastq’ -&gt; ‘sample002_R1_001.fq’\n‘sample002_R2_001.fastq’ -&gt; ‘sample002_R2_001.fq’\n‘sample003_R1_001.fastq’ -&gt; ‘sample003_R1_001.fq’\n‘sample003_R2_001.fastq’ -&gt; ‘sample003_R2_001.fq’\n# [...output truncated...]"
  },
  {
    "objectID": "week2/w2_shellfiles.html#bonus-content",
    "href": "week2/w2_shellfiles.html#bonus-content",
    "title": "Managing files in the shell",
    "section": "6 Bonus content",
    "text": "6 Bonus content\n\n6.1 Viewing and modifying file permissions\nFile “permissions” are the types of things (e.g. reading, writing) that different groups of users (creator, group, anyone else) are permitted to do with files and dirs.\nThere are a couple of reasons you may occasionally need to view and modify file permissions:\n\nYou may want to make your data read-only\nYou may need to share files with other users at OSC\n\n\n\nViewing file permissions\nTo show file permissions, use ls with the -l (long format) option that we’ve seen before. The command below also uses the -a option to show all files, including hidden ones (and -h to show file sizes in human-readable format):\n\n\n\n\n\nHere is an overview of the file permission notation in ls -l output:\n\n\n\n\n\nIn the two lines above:\n\nrwxrwxr-x means:\nread + write + execute permissions for both the owner (first rwx) and the group (second rwx), and read + execute but not write permissions for others (r-x at the end).\nrw-rw-r-- means:\nread + write but not execute permissions for both the owner (first rw-) and the group (second rw-), and only read permissions for others (r-- at the end).\n\nLet’s create a file to play around with file permissions:\n# Create a test file\ntouch testfile.txt\n\n# Check the default permissions\nls -l testfile.txt\n-rw-rw----+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\nChanging file permissions\nThis can be done in two different ways with the chmod command. Here, we’ll focus on the method with = (set permission to), + (add permission), and - (remove permission).\nFor example, to add read (r) permissions for all (a):\n# chmod &lt;who&gt;+&lt;permission-to-add&gt;:\nchmod a+r testfile.txt\n\nls -l testfile.txt\n-rw-rw-r--+ 1 jelmer PAS0471 0 Mar  7 13:40 testfile.txt\nTo set read + write + execute (rwx) permissions for all (a):\n# chmod &lt;who&gt;=&lt;permission-to-set&gt;`:\nchmod a=rwx testfile.txt\n\nls -l testfile.txt\n-rwxrwxrwx+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\nTo remove write (w) permissions for others (o):\n# chmod &lt;who&gt;-&lt;permission-to-remove&gt;:\nchmod o-w testfile.txt\n\nls -l testfile.txt\n-rwxrwxr-x+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\n\n\n\nAlternative: changing file permissions with numbers (Click to expand)\n\n\n\n\n\nYou can also use a series of 3 numbers (for user, group, and others) to set permissions, where each number can take on the following values:\n\n\n\nNr\nPermission\nNr\nPermission\n\n\n\n\n1\nx\n5\nr + x\n\n\n2\nw\n6\nr + w\n\n\n4\nr\n7\nr + w + x\n\n\n\nFor example, to set read + write + execute permissions for all:\nchmod 777 testfile.txt\nTo set read + write + execute permissions for yourself, and only read permission for the group and others:\nchmod 744 file.txt\n\n\n\n\n\n\nMaking your data read-only\nSo, if you want to make your raw data (here: the files in the data/fastq dir) read-only, you can use:\n\nSet only read permissions for everyone:\nchmod a=r data/fastq/*\nTake away write permissions for yourself (no-one else should have it by default):\nchmod u-w data/fastq/*\n\n\n\n\n\n\n\n\nRead/execute permissions for directories\n\n\n\nOne tricky and confusing aspect of file permissions is that to list a directory’s content, you need execute permissions for the dir! This is something to take into account when you want to grant others access to your project e.g. at OSC.\nTo set execute permissions for everyone but only for dirs throughout a dir hierarchy, use an X (uppercase x):\nchmod -R a+X my-dir\n\n\n\nAfter running one or both of the above commands, let’s check the permissions:\nls -l data/fastq\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R2.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R2.fastq\n# [...output truncated...]\nWhat happens when we try to remove write-protected files?\nrm data/fastq/*fastq\nrm: remove write-protected regular empty file ‘data/fastq/sample001_R1.fastq’?\nYou’ll be prompted for every file! If you answer y (yes), you can still remove them. (But note that people other than the file’s owners cannot overried file permissions; only if they are system administrators.)\n\n\n\n\n6.2 Using files across projects: Using symbolic links\n\nSingle files\nA symbolic (or soft) links only links to the path of the original file, whereas a hard link directly links to the contents of the original file. Note that modifying a file via either a hard or soft link will modify the original file.\nCreate a symlink to a file using ln -s &lt;source-file&gt; [&lt;link-name&gt;]:\n# Only provide source =&gt; create link of the same name in the wd:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz\n  \n# The link can also be given an arbitrary name/path:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz shared-fastq.fastq.gz\n\n\n\n\n\n\nUse an absolute path to refer to the source file when creating links\n\n\n\nAt least at OSC, you have to use an absolute path for the source file(s), or the link will not work. The $PWD environment variable, which contains your current working directory can come in handy to do so:\n# (Fictional example, don't run this)\nln -s $PWD/shared-scripts/align.sh project1/scripts/\n\n\n\n\n\nMultiple files\nLink to multiple files in a directory at once:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/* project1/scripts/ \nLink to a directory:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/ project1/scripts/\nln -s $PWD/shared_scripts/ project1/scripts/ln-shared-scripts\n\n\n\n\n\n\nBe careful not to remove source files!\n\n\n\nBe careful when linking to directories: you are creating a point of entry to the original dir. Therefore, even if you enter via the symlink, you are interacting with the original files.\nThis means that a command like the following would remove the original directory!\nrm -r symlink-to-dir\nInstead, use rm symlink-to-dir (the link itself is a file, not a dir, so you don’t need -r!) or unlink symlink-to-dir to only remove the link.\n\n\n\n\n Exercise: Creating symbolic links\n\nCreate a symbolic link in your $HOME dir that points to your personal dir in the project dir (/fs/ess/PAS2700/users/$USER).\nIf you don’t provide a name for the link, it will be your username (why?), which is not particularly informative about its destination. Therefore, give it a name that makes sense to you, like PLNTPTH6193-SP24 or pracs-sp24.\n\n\n\nClick for the solution\n\nln -s /fs/ess/PAS1855/users/$USER ~/PLNTPTH6193-SP24\n\n\nWhat would happen if you do rm -rf ~/PLNTPTH8300-SP21? Don’t try this.\n\n\n\nClick for the solution\n\nThe content of the original dir will be removed."
  },
  {
    "objectID": "week2/w2_shellfiles.html#footnotes",
    "href": "week2/w2_shellfiles.html#footnotes",
    "title": "Managing files in the shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept so-called hidden files.↩︎\nWe’ll talk more about quoting later.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 6193 Spring 2024, second session\n\n\n\n\nWeek\nTue\nThu\nHomework\n\n\n\n\n\n(Pre-course work)\n\n\n\n\n1\n2/27: Course Intro & OSC Intro\n2/29: Shell basics\n   \n\n\n2\n3/05: Project organization & Markdown\n3/07: Managing files in the shell\n   \n\n\n3\n3/19: Getting started with Git\n3/21: Git remotes on GitHub\n   \n\n\n4\n3/26: Shell data tools\n3/28: CLI software & shell scripting\n   \n\n\n5\n4/02: Data & software management\n4/04: OSC Slurm batch jobs\n      \n\n\n6\n4/09: Nextflow I\n4/11: Nextflow II\n      \n\n\n7\n4/16: Recap\n4/18: Student presentations\n      \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/proposal.html",
    "href": "finalproj/proposal.html",
    "title": "General information about your final project",
    "section": "",
    "text": "Write a concise, informal summary of what you plan to do for your final project (due Monday, Apr 1). [10 points]\nSome pointers:\n\nCreate a directory for your project and start a Git repository. This will be a repository for the project as a whole, not just for this proposal. [1]\nWrite the proposal in Markdown and include it in your repository. When you’re done, push your repository to GitHub and tag me (@jelmerp) in an “Issue” on GitHub (please make sure to tag me in the text body for the issue, it won’t be parsed as a tag if you do this in the title of the issue). [1]\nIn the proposal, start with a general description of what your project will be about, without going into detail about coding languages and approaches. Describe what data you will work with and what kind of output your project will produce. (This can be pretty minimal if your project is “code-first” with a trivial data set.) [2]\nNext, summarize how you envision the more technical aspects: in which language(s) do you intend to code, what do you think you will need separate scripts for, and how will you structure results? Will you mostly be coding your data processing/analyses from scratch, or are you primarily running external programs? [2]\nCheck the list of graded aspects on the page with general information about the project. If you are not sure you will be able to fulfill one of these, or intend to skip something (e.g. OSC SLURM jobs), mention that here.\nBriefly mention which aspects of your project you are uncertain about, for instance because you intend to include some topics that we have yet to cover in the course, or because something will depend on how other things go. [2]\nFinally, briefly describe why you chose to pick this project. Because it will be useful for your research? Because it gives you practice with coding topics you like or that you want to / need to get better with? All of the above and/or something else? [2]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/info.html#footnotes",
    "href": "finalproj/info.html#footnotes",
    "title": "General information about your final project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Note that “subsetting” the data may be useful or necessary if you have a large genomic dataset, and I can help you with that↩︎\n A locally run project could be okay if your research does not and will not require the use of supercomputers like those at OSC – check in with me if this is the case and you would therefore prefer not to use OSC.↩︎\n This is a challenging aspect, and your grade won’t plummet if you don’t succeed, but it is important to try!↩︎"
  },
  {
    "objectID": "week7/w7_overview.html",
    "href": "week7/w7_overview.html",
    "title": "Week 7",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week0/overview.html#assignments",
    "href": "week0/overview.html#assignments",
    "title": "Before the course starts",
    "section": "1 Assignments",
    "text": "1 Assignments\nPlease complete these assignments in the week of Feb 19th:\n\nFill out the pre-course survey\nCreate or check your OSC account"
  },
  {
    "objectID": "week0/overview.html#readings",
    "href": "week0/overview.html#readings",
    "title": "Before the course starts",
    "section": "2 Readings",
    "text": "2 Readings\n\nSyllabus\nPlease read the updated syllabus, which you can find here and on the CarmenCanvas site for the course.\n\n\nAdditional information\n\nCourse websites\nThe main place to access information about this course is this github.io website. I will only use the CarmenCanvas site for this course for announcements and as a place to share some files with you.\n\n\nNo R in the course\nI’ve had to remove the modules on R while transitioning this course from 3-credit full-semester to 2-credit half-semester. If this is disappointing to you and you want to learn R at OSU, I can recommend the weekly Code Club meeting that I co-organize. And if you’re also interested in microbial metabarcoding data, or would like to learn how you can use R in a bioinformatics/omics data context, contact Plant Pathology professor Soledad Benitez-Ponce to sign up for a metabarcoding workshop during Spring break from March 13th-15th.\n\n\nGlossary\nIf some of the terms in the description of the course material are unfamiliar to you, take a look at the glossary page of this website. Of course, you will learn a lot more about these terms and concepts during the course!"
  },
  {
    "objectID": "week1/w1_exercises.html",
    "href": "week1/w1_exercises.html",
    "title": "Week 1 Exercises",
    "section": "",
    "text": "The following are some of the exercises from Chapter 1 of the CSB book."
  },
  {
    "objectID": "week1/w1_exercises.html#getting-set-up",
    "href": "week1/w1_exercises.html#getting-set-up",
    "title": "Week 1 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files in your personal dir within /fs/ess/PAS2700 from Thursday’s class. (If not, cd to /fs/ess/PAS2700/users/$USER, and run git clone https://github.com/CSB-book/CSB.git — this downloads the CSB directory referred to in the first step of the first exercise.)"
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.1",
    "href": "week1/w1_exercises.html#intermezzo-1.1",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.1",
    "text": "Intermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within CSB/python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.2",
    "href": "week1/w1_exercises.html#intermezzo-1.2",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.2",
    "text": "Intermezzo 1.2\nTo familiarize yourself with using basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 23) demonstrates the use of the touch command to create a new, empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.3",
    "href": "week1/w1_exercises.html#intermezzo-1.3",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.3",
    "text": "Intermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv (in CSB/unix/data/) in alphabetical order, which is the first species? And which is the last?\n\n\nShow hints\n\nYou can either first select the 5th column using cut and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter (if you use the latter approach, check the book for how to do that with sort).\nTo view just the first or the last line so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nCheck the first line of the file to see which column contains the family, and then select the relevant column with cut.\nUse the “tail trick” we saw in class to exclude the first line.\nRemember to sort before using uniq."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "href": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.1: Next-Generation Sequencing Data",
    "text": "Exercise 1.10.1: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n4. How many “contigs” (FASTA entries, in this case) are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pass the output of grep to wc -l.\n\n5. Modify my_file.fasta to replace the original “two-spaces” delimiter with a comma (i.e. don’t just print the output to screen, but end up with a modified file). You’ll probably want to take a look at the “output file hints” below to see how you can end up with modified file contents.\n\n\nShow output file hints\n\n Due to the “streaming” nature of Unix commands, we can’t write output to a file that also serves as input (see here). So the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n\n\nShow other hints\n\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically — we didn’t see those sort options in class, so check the book for details."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "href": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.2: Hormone Levels in Baboons",
    "text": "Exercise 1.10.2: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. The data file is in CSB/unix/data/Gesquiere2011_data.csv.\nEvery individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can first use cut to extract just the maleID column from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week1/w1_exercises.html#solutions",
    "href": "week1/w1_exercises.html#solutions",
    "title": "Week 1 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nIntermezzo 1.1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\n\nWith cd -:\n# The '-' shortcut for cd will move you back to the previously visited dir\n# (Note: you can't keep going back with this: using it a second time will toggle you \"forward\" again.)\ncd -\nUsing a relative path:\ncd ../data\n\n\n\n\n\nIntermezzo 1.2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n9515 Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\nPapers and reviews  toremove.txt\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nIntermezzo 1.3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\nAbditomys latidens\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\nZyzomys woodwardi\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t \";\" -k 5 Pacifici2013_data.csv | head -n 1\n42641;Rodentia;Muridae;Abditomys;Abditomys latidens;268.09;PanTHERIA;no information;no information;no information;no information;no information;no information;639.6318318208;Mean_family_same_body_mass\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n152\n\n\n\n\nExercise 1.10.1: Next-Generation Sequencing Data\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 ../data/Marra2014_data.fasta\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n560K    ../data/Marra2014_data.fasta\n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,. Importantly, we also write the output to a new file (see the Hints for details):\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nIf we want to change the original file, we can now overwrite it as follows:\nmv my_file.tmp my_file.fasta\nLet’s take a look to check whether out delimiter replacement worked:\ngrep \"&gt;\" my_file.fasta | head\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n&gt;contig00003,length=541,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00004,length=291,numreads=3,gene=isogroup00001,status=it_thresh\n&gt;contig00005,length=580,numreads=12,gene=isogroup00001,status=it_thresh\n&gt;contig00006,length=3288,numreads=35,gene=isogroup00001,status=it_thresh\n&gt;contig00008,length=1119,numreads=10,gene=isogroup00001,status=it_thresh\n&gt;contig00010,length=202,numreads=4,gene=isogroup00001,status=it_thresh\n&gt;contig00011,length=5563,numreads=61,gene=isogroup00001,status=it_thresh\n&gt;contig00012,length=824,numreads=10,gene=isogroup00001,status=it_thresh\n\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\ngrep '&gt;' my_file.fasta | head -n 2\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, add cut to extract the 4th column:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\ngene=isogroup00001\ngene=isogroup00001\nFinally, add sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n43\n\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nFirst, we need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n&gt;contig00001,numreads=2\n&gt;contig00002,numreads=8\n&gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t '=' to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n&gt;contig00089,numreads=1\n&gt;contig00176,numreads=1\n&gt;contig00210,numreads=1\n&gt;contig00001,numreads=2\n&gt;contig00003,numreads=2\nAdding the sort option -r, we can sort in reverse order, which tells us that contig00302 has the highest coverage, with 3330 reads:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n&gt;contig00302,numreads=3330\n\n\n\n\n\nExercise 1.10.2: Hormone Levels in Baboons\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s move back into the data dir:\ncd ../data\nNext, let’s take a look at the structure of the file:\nhead -n 3 Gesquiere2011_data.csv\nmaleID        GC      T\n1     66.9    64.57\n1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 Gesquiere2011_data.csv | head -n 3\nmaleID\n1\n1\n\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the option -w to match whole “words” – this will make it match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 3\n61\n# For maleID 27\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 27\n5"
  },
  {
    "objectID": "week1/w1_shell.html#goals-for-this-session",
    "href": "week1/w1_shell.html#goals-for-this-session",
    "title": "Unix Shell Basics",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nIn this session, we’ll cover much of the material from CSB Chapter 1, to learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files.\n\n\n\n\n\n\n\n\nSee the “Topic Overview” page on the Unix shell for an overview of shell commands we’ll cover during this course."
  },
  {
    "objectID": "week1/w1_shell.html#introduction-ch.-1.1-1.2",
    "href": "week1/w1_shell.html#introduction-ch.-1.1-1.2",
    "title": "Unix Shell Basics",
    "section": "2 Introduction (Ch. 1.1-1.2)",
    "text": "2 Introduction (Ch. 1.1-1.2)\n\n2.1 Some terminology\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “*nix”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface2 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably.\n\n\n\n2.2 Why use the Unix shell?\n\nVersus programs with graphical user interfaces:\n\nUsing software\nBest or only option to use many programs, especially in bioinformatics.\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nThe shell keeps a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data\nRemote computing – especially HPCs\nIt is often only possible to work in a terminal when doing remote computing.\n\nVersus scripting languages like Python or R:\n\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nThe Unix shell has a direct interface to other programs."
  },
  {
    "objectID": "week1/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "href": "week1/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "title": "Unix Shell Basics",
    "section": "3 Getting started with Unix (Ch. 1.3)",
    "text": "3 Getting started with Unix (Ch. 1.3)\n\nUnix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nFor example, the path to our OSC project’s dir is /fs/ess/PAS2700. This means: the dir PAS2700 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/&lt;a-project&gt;/&lt;username&gt;.\n\n\n\n\n\n\nGeneric example, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in OSC dir structure"
  },
  {
    "objectID": "week1/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "href": "week1/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "title": "Unix Shell Basics",
    "section": "4 Getting started with the shell (Ch. 1.4)",
    "text": "4 Getting started with the shell (Ch. 1.4)\n\n4.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\n\n\n\n4.2 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nClearing the screen\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\n\n\n\n\n\n4.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n4.4 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this is supposed to be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSo, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options3.\n\n\n\n\n\n4.5 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet4. (You can also combine this new option with other options, if you want.)\n\n\n\nSolution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nSolution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\nBonus: Try to figure out / guess what the cal [options] [[[day] month] year] means. Can you print a calendar for April 2017?\n\n\n\nSolution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\n\n\n\n\n4.6 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2700\n# Double-check that we moved:\npwd\n/fs/ess/PAS2700\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2700\n-bash: cd: /fs/ess/PAs2700: No such file or directory\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names (hence the error above).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n4.7 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2700).\nPress Enter. What does the resulting error mean?\nbash: /fs/ess/PAS2700/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you perhaps expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2700/\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.8 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell5.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.9 Create your own dir & get the CSB data\nOur base OSC directory for the course is the /fs/ess/PAS2700 dir we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.10 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2700)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. CSB/unix/sandbox)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2700/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2700/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week1/w1_shell.html#basic-unix-commands-ch.-1.5",
    "href": "week1/w1_shell.html#basic-unix-commands-ch.-1.5",
    "title": "Unix Shell Basics",
    "section": "5 Basic Unix commands (Ch. 1.5)",
    "text": "5 Basic Unix commands (Ch. 1.5)\n\n5.1 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nFor which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n5.2 cp to copy files\nThe cp command copies files and/or dirs from one location to another. It has two required arguments: what you want to copy (source), and where you want to copy it to (destination). Its basic syntax is cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n5.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n5.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm Buzzard2015_about.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n5.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example6:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "week1/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "href": "week1/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "title": "Unix Shell Basics",
    "section": "6 Advanced Unix commands (Ch. 1.6)",
    "text": "6 Advanced Unix commands (Ch. 1.6)\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n6.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n6.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument with a file name like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n6.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as Unix “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files (Click to expand)\n\n\n\n\n\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\n\nThe cut command will select/“cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\n6.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n6.5 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern.7\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options — more in Week 4:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n6.6 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab8. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\n\nDelete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nRemove consecutive duplicates a’s:\necho \"aabbccddee\" | tr -s a\nabbccddee\n\n\n\n\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension) in the sandbox dir (that’s not where you are located yourself).\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "week1/w1_shell.html#wrap-up-the-unix-philosophy",
    "href": "week1/w1_shell.html#wrap-up-the-unix-philosophy",
    "title": "Unix Shell Basics",
    "section": "7 Wrap-up & the Unix philosophy",
    "text": "7 Wrap-up & the Unix philosophy\n\n7.1 Covered in the chapter but not in today’s lecture\n\nThe less pager to view files\nThe find command to find files\nShowing and changing file permissions\nBasic shell scripting\nfor loops\n$PATH and bash profile settings\n\nWe’ll cover all of these in class over the next few weeks.\n\n\n\n7.2 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n7.3 The Unix shell in the weeks ahead\n\nIn all course weeks, we will be working in the Unix shell, though our focus in several cases will be on a specific tool, such as Git in week 3.\nIn week 4, we’ll fully focus on the shell and shell scripting."
  },
  {
    "objectID": "week1/w1_shell.html#footnotes",
    "href": "week1/w1_shell.html#footnotes",
    "title": "Unix Shell Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\n Though some commands are flexible and accept either order.↩︎\nThere really is only one proper additional options: two others reflect the defaults, and then there’s the version option.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expressions, this is not strictly necessary, it’s good habit to always quote.↩︎\nWe’ll learn more about regular expressions in Week 4.↩︎"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-jelmer",
    "href": "week1/w1_course-intro.html#introductions-jelmer",
    "title": "Course Intro",
    "section": "Introductions: Jelmer",
    "text": "Introductions: Jelmer\n\nBioinformatician at MCIC in Wooster since June 2020\n\nThe majority of my time is spent providing research assistance,\nworking with grad students and postdocs on mostly genomic & transcriptomic data\nI also teach: some courses, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\n\n\n\nIn my free time, I enjoy bird watching – locally & all across the world"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-you",
    "href": "week1/w1_course-intro.html#introductions-you",
    "title": "Course Intro",
    "section": "Introductions: You",
    "text": "Introductions: You\n\nName\nLab and Department\nResearch interests and/or current research topics\nSomething about you that is not work-related"
  },
  {
    "objectID": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "href": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "title": "Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills to:\n\nDo your research more reproducibly and efficiently\nPrepare yourself for working with large “omics” datasets"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility",
    "href": "week1/w1_course-intro.html#course-background-reproducibility",
    "title": "Course Intro",
    "section": "Course background: Reproducibility",
    "text": "Course background: Reproducibility\n\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\nOur focus is on #2."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "href": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "title": "Course Intro",
    "section": "Course background: Reproducibility (cont.)",
    "text": "Course background: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n—Karl Broman\n\n\n\nAlso important:\n\nProject organization and documentation (week 2)\nSharing data and code (for code: Git & GitHub, week 3)\nHow you code (e.g. week 4 - shell scripts, and 6 - Nextflow)\n\n\n\n\n\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "href": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "title": "Course Intro",
    "section": "Course background: Efficiency and automation",
    "text": "Course background: Efficiency and automation\n\nUsing code enables you to work more efficiently and automatically —\nparticularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo a project after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-omics-data",
    "href": "week1/w1_course-intro.html#course-background-omics-data",
    "title": "Course Intro",
    "section": "Course background: Omics data",
    "text": "Course background: Omics data\n\nOmics data is increasingly important in biology, and most notably includes:\n\nGenomics + Microbiomics + Metagenomics\nTranscriptomics\nProteomics\nMetabolomics\n\n\n\n\n\nWhile we’ll be using some example omics datasets, this course will not teach you how to analyze omics data in full — our focus is on fundamental computational skills.\n\n\n\n\n\n\n\nI should say that this course is less relevant for proteomics and metabolomics, especially in its slimmed-down half-semester form with no R."
  },
  {
    "objectID": "week1/w1_course-intro.html#the-unix-shell-shell-scripts-wk-1-4",
    "href": "week1/w1_course-intro.html#the-unix-shell-shell-scripts-wk-1-4",
    "title": "Course Intro",
    "section": "The Unix shell & shell scripts (Wk 1 & 4)",
    "text": "The Unix shell & shell scripts (Wk 1 & 4)\nBeing able to work in the Unix shell is a fundamental skill in computational biology.\n\n\nYou’ll spend a lot of time with the Unix shell, starting this week\nWe’ll also write shell scripts, and will use VS Code for this and other purposes.\n\n\n\n\n\n\n\n\n\n\nBash (shell language)\n\n\n\n\n\n\n\n\nVS Code"
  },
  {
    "objectID": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "href": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "title": "Course Intro",
    "section": "Project organization and Markdown (Wk 2)",
    "text": "Project organization and Markdown (Wk 2)\nGood project organization & documentation is a necessary starting point for reproducible research.\n\n\nYou’ll learn best practices for project organization, file naming, etc.\nTo document and report what you are doing, you’ll use Markdown files.\n\n\n\n\n\n\n\n\nMarkdown"
  },
  {
    "objectID": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "href": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "title": "Course Intro",
    "section": "Version control with Git and GitHub (Wk 3)",
    "text": "Version control with Git and GitHub (Wk 3)\nUsing version control, you can more effectively keep track of project progress, collaborate, share code, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories).\nYou’ll also use Git+GitHub to hand in your final project assignments."
  },
  {
    "objectID": "week1/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "href": "week1/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "title": "Course Intro",
    "section": "High-performance computing with OSC (Wk 5)",
    "text": "High-performance computing with OSC (Wk 5)\nThanks to supercomputer resources, you can work with very large datasets at speed — running up to 100s of analyses in parallel, and using much larger amounts of memory and storage space than a personal computer has.\n\n\n\nWe will use OSC throughout the course, and you’ll get a brief intro to it today.\nIn week 5, we’ll learn to submit shell scripts as OSC “batch jobs” with Slurm, and use Conda to manage software."
  },
  {
    "objectID": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "href": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "title": "Course Intro",
    "section": "Automated workflow management (Wk 6)",
    "text": "Automated workflow management (Wk 6)\nUsing a workflow written with a workflow manager, you can run and rerun entire analysis pipeline with a single command, and easily change and rerun parts of it, too.\n\n\nWe’ll use the workflow language Nextflow."
  },
  {
    "objectID": "week1/w1_course-intro.html#zoom",
    "href": "week1/w1_course-intro.html#zoom",
    "title": "Course Intro",
    "section": "Zoom",
    "text": "Zoom\n\nBe muted by default, but feel free to unmute yourself to ask questions any time.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week1/w1_course-intro.html#websites-books",
    "href": "week1/w1_course-intro.html#websites-books",
    "title": "Course Intro",
    "section": "Websites & Books",
    "text": "Websites & Books\n\nI will only use the CarmenCanvas website for Announcements.\n\n\n\nThe GitHub website is the main website for the course, containing all course material:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nExercises\nFinal project assignment information\n\n\n\n\n\nBooks:\n\nComputing Skills for Biologists (“CSB”; Allesina & Wilmes 2019)\nBioinformatics Data Skills (“Buffalo”; Buffalo 2015)"
  },
  {
    "objectID": "week1/w1_course-intro.html#final-project-graded",
    "href": "week1/w1_course-intro.html#final-project-graded",
    "title": "Course Intro",
    "section": "Final project (graded)",
    "text": "Final project (graded)\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 4 – 10 points)\nII: Draft (due week 6 – 10 points)\nIII: Oral presentations on Zoom (week 7 – 10 points)\nIV: Final submission (due April 29 – 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIf you have your own data set & analysis ideas, that is ideal. If not, I can provide you with this.\nMore information about the final project will follow in week 2 or 3."
  },
  {
    "objectID": "week1/w1_course-intro.html#ungraded-homework",
    "href": "week1/w1_course-intro.html#ungraded-homework",
    "title": "Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings — somewhat up to you when to do these\nWeekly exercises — I recommend doing these on Fridays\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nWeekly materials & homework\n\n\nI will try add the materials for each week on the preceding Friday — at the least the week’s overview and readings.\nNone of this homework had to be handed in."
  },
  {
    "objectID": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "href": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "title": "Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nIf there is interest, we can have a weekly Monday meeting in which we go through the exercises for the preceding week.\n\nIf you’re interested, indicate your availability here:\nhttps://www.when2meet.com/?23841132-KV8fY"
  },
  {
    "objectID": "week1/w1_course-intro.html#rest-of-this-week",
    "href": "week1/w1_course-intro.html#rest-of-this-week",
    "title": "Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nBrief introduction to the Ohio Supercomputer Center (OSC)\n\n\n\nUnix shell basics\n\n\n\nHomework:\n\nReadings: mostly CSB Chapter 1\nExercises"
  },
  {
    "objectID": "week6/w6_overview.html",
    "href": "week6/w6_overview.html",
    "title": "Week 6",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/shell.html#basic-commands",
    "href": "ref/shell.html#basic-commands",
    "title": "Topic overview: Unix shell",
    "section": "1 Basic commands",
    "text": "1 Basic commands\n\n\n\nCommand\nDescription\nExamples / options\n\n\n\n\npwd\nPrint current working directory (dir).\npwd\n\n\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\n\n\ncd\nChange working dir. As with all commands, you can use an absolute path (starting from the root dir /) or a relative path (starting from the current working dir).\ncd /fs/ess/PAS1855 (With absolute path)  cd ../.. (Two levels up)  cd - (To previous dir)\n\n\ncp\nCopy files or, with -r, dirs and their contents (i.e., recursively).  If target is a dir, file will keep same name; otherwise, a new name can be provided.\ncp *.fq data/ (All .fq files into dir data)  cp my.fq data/new.fq (With new name)  cp -r data/ ~ (Copy dir and contents to home dir) \n\n\nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name)  mv my.fq my.fastq (Simple rename)  mv file1 file2 mydir/ (Last arg is destination)\n\n\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files)  rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\n\n\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir  mkdir -p new1/new2/new3\n\n\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\n\n\ncat\nPrint file contents to standard out (screen).\ncat my.txt  cat *.fa &gt; concat.fq (Concatenate files)\n\n\nhead\nPrint the first 10 lines of a file or specify number with -n &lt;n&gt; or shorthand -&lt;n&gt;.\nhead -n 40 my.fq (print 40 lines)  head -40 my.fq (equivalent)\n\n\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (“trick” to skip first line)  tail -f slurm.out (“follow” file)\n\n\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\n\n\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file:  column -s \",\" -t my.csv\n\n\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\n\n\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x).  ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable)  chmod a=r data/raw/* (Make data read-only)      -R recursive  ddddddddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#data-tools",
    "href": "ref/shell.html#data-tools",
    "title": "Topic overview: Unix shell",
    "section": "2 Data tools",
    "text": "2 Data tools\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples and options\n\n\n\n\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\n\n\ncut\nSelect one or more columns from a file.\nSelect columns 1-4:  cut -f 1-4 my.csv      -d \",\" comma as delimiter\n\n\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically,  column 2 reverse numerically:  sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\n\n\nuniq\nRemove consecutive duplicate lines (often from single-column selection): i.e., removes all duplicates if input is sorted.\nUnique values for column 2:  cut -f2 my.tsv | sort | uniq\n\n\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3:  cut -f3 my.tsv | sort | uniq -c\n\n\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV:  cat my.csv | tr \"\\t\" \",\"  Uppercase to lowercase: tr A-Z a-z &lt; in.txt &gt; out.txt      -d delete      -s squeeze\n\n\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines before match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC:  grep \"A[AG]C\" my.fa  Omit comment lines:  grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only"
  },
  {
    "objectID": "ref/shell.html#miscellaneous",
    "href": "ref/shell.html#miscellaneous",
    "title": "Topic overview: Unix shell",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nexample\n\n\n\n\n/\nRoot directory.\ncd /\n\n\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir)  Use ./ to execute script if not in $PATH:  ./myscript.sh\n\n\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n\n\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n\n\n$USER\nUser name.\nmkdir $USER\n\n\n&gt;\nRedirect standard out to a file.\necho \"My 1st line\" &gt; myfile.txt\n\n\n&gt;&gt;\nAppend standard out to a file.\necho \"My 2nd line\" &gt;&gt; myfile.txt\n\n\n2&gt;\nRedirect standard error to a file.\nSend standard out and standard error for a script to separate files:  myscript.sh &gt;log.txt 2&gt; err.txt\n\n\n&&gt;\nRedirect standard out and standard error to a file.\nmyscript.sh &&gt; log.txt\n\n\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nThe output of the sort command will be piped into head to show the first lines:  sort myfile.txt | head\n\n\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 =&gt; 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31)  touch fig1{A..F} (fig1A, fig1B, …, fig1F)  mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n\n\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files:  echo \"I see $(ls *fastq | wc -l) files\"  Substitute with date in YYYY-MM-DD format:  mkdir results_$(date +%F)  nlines=$(wc -l &lt; $infile)\n\n\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name.  ddddddddddddddddddddddddddddddddddddd\nAdd dir to path:  PATH=$PATH:/new/dir  (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#shell-wildcards",
    "href": "ref/shell.html#shell-wildcards",
    "title": "Topic overview: Unix shell",
    "section": "4 Shell wildcards",
    "text": "4 Shell wildcards\n\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n\n*\nAny number of any character, including nothing.\nls data/*fastq.gz (Matches any file ending in “fastq.gz”)  ls *R1* (Matches any file containing “R1” somewhere in the name.)\n\n\n?\nAny single character.\nls sample1_?.fastq.gz (Matches sample1_A.fastq.gz but not sample1_AA.fastq.gz)\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets.  ddddddddddddddddddddddddddddddddddddd\nls fig1[A-C] (Matches fig1A, fig1B, fig1C)  ls fig[0-3] (Matches fig0, fig1, fig2, fig3)  ls fig[^4]* (Does not match files with a “4” after “fig”)  ddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#regular-expressions",
    "href": "ref/shell.html#regular-expressions",
    "title": "Topic overview: Unix shell",
    "section": "5 Regular expressions",
    "text": "5 Regular expressions\n\n\n\n\n\n\n“ERE” = GNU Extended regular expressions\n\n\n\nWhere it says “yes” in the ERE column, the symbol in questions needs to have ERE turned on in order to work1: use a -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE.\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nERE2\nMatches\nExample\n\n\n\n\n.\n\nAny single character\nMatch Olfr with none or any characters after it:  grep -o \"Olfr.*\"\n\n\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n\n\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits:  grep -E [0-9]+\n\n\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit:  grep -E [0-9]?\n\n\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs:  grep -E \"G{50,100}\"\n\n\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines:  grep -v \"^$\"  Exclude lines beginning with a “#”:  grep -v \"^#\"\n\n\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\n\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\n\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed.\nMatch gene_id followed by a space and a “word”:  grep -E -o 'gene_id \"\\w+\"'  Change any word character to X:  sed s/\\w/X/\n\n\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either intron or exon:  grep -E \"intron|exon\"\n\n\n()\nyes\nGrouping\nFind “AAG” repeated 10 times:  grep (AAG){10}\n\n\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc.  ddddddddddddddddddddddddddddddddddddd\nInvert order of two words:  sed -E 's/(\\w+) (\\w+)/\\2 \\1/'  ddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#more-details-for-a-few-commands",
    "href": "ref/shell.html#more-details-for-a-few-commands",
    "title": "Topic overview: Unix shell",
    "section": "6 More details for a few commands",
    "text": "6 More details for a few commands\n\n6.1 less\n\n\n\n\n\n\n\nKey\nFunction\n\n\n\n\nq\nExit less\n\n\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\n\n\nd / u\nGo down / up half a page.\n\n\ng / G\nGo to the first / last line (home / end also work).\n\n\n/&lt;pattern&gt; or ?&lt;pattern&gt;\nSearch for &lt;pattern&gt; forwards / backwards: type your search after / or ?.\n\n\nn / N\nWhen searching, go to next / previous search match.\ndddddddddddddddddddddddddddddddddddddddddddddddddddd\n\n\n\n\n\n\n6.2 sed\n\nsed flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n-E\nUse extended regular expressions\n\n\n-e\nWhen using multiple expressions, precede each with -e\n\n\n-i\nEdit a file in place\n\n\n-n\nDon’t print lines unless specified with p modifier\n\n\n\n\n\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (&gt;1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#&gt; chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#&gt; words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#&gt; sampleA.fastq\n\n\n\n\n6.3 awk\n\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\n\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 &lt; 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\n\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 &gt; 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\n\n\nawk comparison and logical operators\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\na == b\na is equal to b\n\n\na != b\na is not equal to b\n\n\na &lt; b\na is less than b\n\n\na &gt; b\na is greater than b\n\n\na &lt;= b\na is less than or equal to b\n\n\na &gt;= b\na is greater than or equal to b\n\n\na ~ /b/\na matches regular expression pattern b\n\n\na !~ /b/\na does not match regular expression pattern b\n\n\na && b\nlogical and: a and b\n\n\na || b\nlogical or: a or b [note typo in Buffalo]\n\n\n!a\nnot a (logical negation)\n\n\n\n\n\nawk special variables and keywords\n\n\n\n\n\n\n\nkeyword/variable\nmeaning\n\n\n\n\nBEGIN\nUsed as a pattern that matches the start of the file\n\n\nEND\nUsed as a pattern that matches the end of the file\n\n\nNR\nNumber of Records (running count; in END: total nr. of lines)\n\n\nNF\nNumber of Fields (for each record)\n\n\n$0\nContains entire record (usually a line)\n\n\n$1 - $n\nContains one column each\n\n\nFS\nInput Field Separator (default: any whitespace)\n\n\nOFS\nOutput Field Separator (default: single space)\n\n\nRS\nInput Record Separator (default: newline)\n\n\nORS\nOutput Record Separator (default: newline)\n\n\n\n\n\nSome awk functions\n\n\n\n\n\n\n\nFunction\nMeaning\n\n\n\n\nlength(&lt;string&gt;)\nReturn number of characters\n\n\ntolower(&lt;string&gt;)\nConvert to lowercase\n\n\ntoupper(&lt;string&gt;)\nConvert to uppercase\n\n\nsubstr(&lt;string&gt;, &lt;start&gt;, &lt;end&gt;)\nReturn substring\n\n\nsub(&lt;from&gt;, &lt;to&gt;, &lt;string&gt;)\nSubstitute (replace) regex\n\n\ngsub(&lt;from&gt;, &lt;to&gt; &lt;string&gt;)\n&gt;1 substitution per line\n\n\nprint\nPrint, e.g. column: print $1\n\n\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\n\n\nnext\nDon’t process later fields: to next iteration"
  },
  {
    "objectID": "ref/shell.html#keyboard-shortcuts",
    "href": "ref/shell.html#keyboard-shortcuts",
    "title": "Topic overview: Unix shell",
    "section": "7 Keyboard shortcuts",
    "text": "7 Keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl+Shift+C\nCopy selected text\n\n\nCtrl+Shift+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line3\n\n\nCtrl+W\nCut word before before cursor4\n\n\nCtrl+Y\nPaste (“yank”)\n\n\nAlt+.\nLast argument of previous command (very useful!)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nKill (stop) currently active command\n\n\nCtrl+D\nExit (a program or the shell depending on the context)\n\n\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background."
  },
  {
    "objectID": "ref/shell.html#footnotes",
    "href": "ref/shell.html#footnotes",
    "title": "Topic overview: Unix shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen using the default regular expressions in grep and sed, Basic Regular Expressions (BRE), the symbol would need to be preceded by a backslash to work.↩︎\nGNU Extended Regular Expressions↩︎\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.↩︎"
  },
  {
    "objectID": "ref/further-resources.html",
    "href": "ref/further-resources.html",
    "title": "Further Resources",
    "section": "",
    "text": "An extended version of this introduction\nOSC’s online asynchronous courses\nOSC’s new User Resource Guide 1"
  },
  {
    "objectID": "ref/further-resources.html#footnotes",
    "href": "ref/further-resources.html#footnotes",
    "title": "Further Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC.↩︎"
  },
  {
    "objectID": "ref/glossary.html",
    "href": "ref/glossary.html",
    "title": "Glossary of common terms in this course",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\nBuffalo\nCourse book: Bioinformatics Data Skills (Buffalo 2015).\n\n\nCLI\nCommand-line Interface — a software interface in which one types commands (cf. “GUI”).\n\n\nCSB\nCourse book: Computing Skills for Biologists (Allesina & Wilmes 2019).\n\n\ncluster\nAnother word for supercomputer, a set of interconnected computers for high-performance computing\n\n\ndir\nShort for “directory”, which how a folder is often referred to in the Unix shell and Linux.\n\n\nGit\nSoftware for “version control”, a system to track changes in code and other text files, and collaborate on those.\n\n\nGitHub\nA website that hosts Git projects, which are known as repositories.\n\n\nGUI\nGraphical User Interface – a visual software interface with which one interacts by clicking (cf. “CLI”).\n\n\nHPC\nHigh-Performance Computing, for instance as can be done using a supercomputer.\n\n\nLinux\nThe operating system that OSC runs on. Free Linux distributions include Ubuntu.\n\n\nMarkdown\nA simple text markup language (think LaTeX or HTML but much simpler).\n\n\nOSC\nThe Ohio Supercomputer Center.\n\n\nshell\nThe Unix shell is a command-line interface to the operating system that runs within a terminal. There are several shell flavors, and in this course, we will be working with the bash shell.\n\n\nNextflow\nSoftware for automated analysis workflow (pipeline) management.\n\n\nSlurm\nSoftware that schedules “compute jobs” (access to the main parts of the system) at OSC.\n\n\nUnix\nA family of operating systems that includes Mac and Linux, but not Windows.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week6/w6_exercises.html",
    "href": "week6/w6_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week1/w1_osc.html#goals-for-this-session",
    "href": "week1/w1_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC).\nThis is only meant as a brief introductory overview to give some context about the working environment that we will start using this week. During the course, you’ll learn a lot more about most topics touched on in this page — week 5 in particular focuses on OSC."
  },
  {
    "objectID": "week1/w1_osc.html#high-performance-computing",
    "href": "week1/w1_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2700.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course’s OSC Project PAS2700: /fs/ess/PAS2700. (We’ll talk more about these different file systems in week 5.)\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use in week 5, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs2.\n\n\n\n\n\n\n\n\nAt-home reading: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "week1/w1_osc.html#osc-ondemand",
    "href": "week1/w1_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2700, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2700 project’s “scratch” directory (/fs/scratch/PAS2700)\nThe PAS2700 project’s “project” directory (/fs/ess/PAS2700)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2700.\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files3 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server.\n\n\n\n4.3 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell in the next session."
  },
  {
    "objectID": "week1/w1_osc.html#footnotes",
    "href": "week1/w1_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "week1/w1_overview.html#links",
    "href": "week1/w1_overview.html#links",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Course intro (slides)\nTue: OSC intro\nThu: Shell basics\n\n\n\nExercises\n\nWeek 1 exercises\n\n\n\n\n\n\n\n\nPre-course assignments (if you didn’t do this already)\n\n\n\n\nPre-course survey\nGet access to the Ohio Supercomputer Center (OSC)"
  },
  {
    "objectID": "week1/w1_overview.html#content-overview",
    "href": "week1/w1_overview.html#content-overview",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an overview of the course and a brief intro to the Ohio Supercomputer (OSC) during the Tuesday meeting, and will be taught the basics of working in a Unix shell environment during the Thursday meeting.\nMore specifically, some of the things you will learn this week:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nOSC Intro (Tuesday class)\n\nWhat is a supercomputer and why is it useful?\nOverview of the resources the Ohio Supercomputer Center (OSC) provides.\nHow to use OSC OnDemand and access a Unix Shell in your browser.\n\n\n\nUnix shell basics (Thursday class, Readings, and Exercises)\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing Unix commands, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week1/w1_overview.html#readings",
    "href": "week1/w1_overview.html#readings",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of the book Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book as well as this course.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell.\nI would recommend reading or at least skimming this before Thursday’s class, when we will go through much of the chapter’s content. That said, I will not assume you’ve read it during class, and you may prefer to focus your reading after the lectures. (We’ll discuss reading expectations and recommendations on Tuesday, too.)\nYou can access the books directly online through the links below, or download PDFs from the CarmenCanvas website. Consider buying a paper copy of one or both if you can afford it.\n\nRequired readings\n\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "week1/removed.html",
    "href": "week1/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week1/removed.html#bonus-material",
    "href": "week1/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week0/osc-setup.html",
    "href": "week0/osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore the course starts, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2700).\n\n\nBackground\nMuch of the coding during this course will be done through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC Project, and membership of this specific project will allow you to access our shared files and reserve “compute nodes”.\n\n\nWhat you should do\nAfter completing the pre-course survey, you will receive an invitation email from OSC referencing the course project number PAS2700:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/finalproject_progress.html",
    "href": "finalproj/finalproject_progress.html",
    "title": "Final project: Progress report",
    "section": "",
    "text": "0.1 Please report on your progress for your final project (due Tuesday, Apr 13). [10 points]\n\nI would like to see the following in your final project repository:\n\nOne or more (Bash/Python/R) scripts with a significant amount code. [4]\nThe scripts do not yet need to be complete or functioning, but make sure I can understand what you are trying to do. Also, it should be clear what the general purpose of the scripts is.\nYou can earn as many points with clarity of purpose and documentation (use comments!) as with the code itself.\nAn overview of all the scripts you envision writing, and their functions. [3]\nThis is essentially a more worked-out version of the technical description you wrote in your proposal (some remaining uncertainties are fine!).\nHaving learned about Snakemake, you should include how you want to implement being able to (re)run the entire pipeline/workflow: with Snakemake, or a Bash or Python script.\nA to-do list. [1.5]\nThis could be a separate list or it could be (partially) integrated with the overview of scripts mentioned above.\n\nBoth the technical description and the to-do list would be suitable for the main README.md of the project (and it would be good to continue to update these later on). But you could also create separate documents for each, whatever works better for you.\n\n\n0.2 Some general pointers:\n\nI will also start looking at your Git commits and associated commit messages. [1.5]\nI will not grade this harshly, and will just skim you commit history, but will subtract points if:\n\nYour commits do not form logical units at all – especially if they include work on multiple unrelated items. (“Oversplitting” of commits, i.e. having some really small commits that are not as much of a problem, that just tends to happen unless you amend commits.)\nYour commit messages are consistently uninformative (“Update file X” is generally not enough – especially when repeated).\n\nYou can also flag specific things that you would like feedback/advice on, perhaps best in the GitHub Issue mentioned below.\nOnce again, tag me (@jelmerp) in an “Issue” on GitHub and make sure you point me to the places you want me to look, particularly if you have a lot going on (in such cases, I won’t grade files that you don’t point me to).\nSome of you already had most of these pieces in place in your repository when I looked at your proposal. If that is the case, just point me where to look – you don’t necessarily need to restructure things for this checkpoint.\n\nRemember that I have added some topic overviews to the GitHub site that should be helpful as you work on you final project – see the top navigation bar.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/finalproject_submission.html",
    "href": "finalproj/finalproject_submission.html",
    "title": "Final project: submission",
    "section": "",
    "text": "The submission of your final project is due on April 30th. [20 points]\n\n0.1 How to submit\nAs usual, open a new GitHub issue for your repository and tag @jelmerp in the text body of the Issue.\n\n\n0.2 What to submit\nYour repository should now contain:\n\nA finished set of scripts.\nFinal documentation in one or more README files that clearly describes:\n\nWhat the project does as a whole.\nWhat each script does.\nWhere to access the data at OSC, assuming that the data is not in your repository.\nHow the project’s scripts can be rerun using a single script or Snakefile.\n\nA single script or Snakefile that aims to rerun the full workflow.\nA file (e.g. submission_notes.md) or a section in your main README file that provides some additional information for the instructor to grade your project appropriately. Some hypothetical examples of things you may want to include:\n\nAdditional instructions the instructor will need to try and rerun your project.\nYou want to alert the instructor to some files files in the repository that should be ignored.\nYou want to explain why you don’t have a functioning script or Snakefile, or why you didn’t run any SLURM jobs (which can be acceptable in some cases).\n\n\n\n\n0.3 Graded aspects\nBelow is a long list of graded aspects and what to aim for if you want a perfect score. I’m providing a lot of detail here, so there are no surprises. The TLDR is that you should aim to have a reproducible, well-organized and well-documented workflow – workflow size/complexity on the other hand, is fairly unimportant. (See also the General Info page for the final project for some more general background.)\n\n\n\n\n\n\n\n\nCategory\nMax.  score\nMax. score if your project (examples given):\n\n\n\n\nProject organization\n2\n\nHas a clear and appropriate directory structure.\nHas informative and appropriate directory and file names.\nDoes not mix data, scripts, and results in individual directories.\n\n\n\nProject background and documentation\n2\n\nHas a clear description of its background and goals.\nHas a clear description of how different scripts are being used to achieve these goals.\nWhere appropriate, indicates what is still a work-in-progress (and optionally future directions).\n\n\n\nScript documentation\n2\n\nUses extensive (yet succinct) comments to document what is being done within scripts.\n\n\n\nGood practices in scripts\n4\n\nUses no absolute paths in scripts.\nUses scripts that take arguments where appropriate and minimizes “hard-coding” of potentially variable things like input/output dirs, file names, and some software settings. Any hard-coded variables/constants that are present are clearly set at the top of scripts.\nHas individual scripts that are not overly long and don’t do multiple unrelated things.\nHas no or only clearly annotated lines that are “commented out” in scripts.\nUses Bash scripts with proper set settings, and similar good practices as taught in the course.\n\n\n\nCoding quality and complexity\n3\n\nHas code that demonstrates an understanding of topics covered in the course.\nHas code that is appropriately broken up in small parts within scripts, e.g. with functions in Python.\nUses tools and commands that are (by and large) appropriate to accomplish its goals. (I will not dig in to fine details and parameter settings.)\n\n\n\nWorkflow  reproducibility\n3\n\nHas a script or Snakefile that includes all steps in the workflow and that can be run by anybody with access to your repository and the raw data files.\nHas information for the instructor (or any other reader of the project!) about where at OSC to find the raw data files and other details needed to try to rerun the analyses.\nBonus: good software management, e.g. Conda environments (preferred) or OSC modules and no manual installs unless necessary; YAML files describing environments.\n\n\n\nSLURM jobs at OSC\n2\n\nHas one or more scripts that are run as OSC jobs at SLURM.\nUses appropriate SLURM directives – either in the scripts or in a Snakemake profile YAML file.\n\n\n\nVersion control\nddddddddddddddddd\n2\ndddddd\n\nHas Git commit messages that are informative.\nHas reasonably appropriate commits, e.g. individual Git commits don’t consist of multiple completely unrelated edits.\nHas a single .gitignore file that ignores files like large raw data files, and in most cases, results files.\nBonus: has a Git tag for the submitted version.\n\ndddddddddddddddddddddddddddddddddddddddddddddd dddddddd dddddddd\n\n\n\n\n\n0.4 Questions and advice\nDon’t hesitate to contact Jelmer (or, where fitting, Zach) for questions about topics like:\n\nSpecific expectations for the final project that are unclear to you.\nWhether you are on the right track in making some adjustments that I asked for after your progress report.\nAdvice on how to code or organize aspects of your project.\n\nWe’re happy to answer questions by e-mail or in a Zoom meeting!\n\n\n0.5 Late submissions\nLate submission may be accommodated depending on circumstances, but you will need to contact Jelmer before 3 pm on April 30th and we can take it from there.\nFor late submissions with no advance notice, 4 points will be subtracted for each day the submission is late. (Things like forgetting to open an Issue won’t lead to subtracted points.)\n\n\n0.6 Good luck!!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/finalproject_present.html",
    "href": "finalproj/finalproject_present.html",
    "title": "Final project: presentations",
    "section": "",
    "text": "Every student is expected to give a 10-minute presentation about their final project during the Zoom sessions on April 20th and 22nd. [10 points]\n\nA few pointers:\n\nAim for the presentation itself to take about 10 minutes – the acceptable range is roughly 8-11 minutes.\nAfterwards, be prepared to answer a few questions both from your peers and the TA or instructor. You’re also expected to ask one or more questions (in total) to other students after their presentations.\nPrepare at least several slides. Your entire presentation can be given using slides. But if you want, you can also switch to showing your actual repository / scripts during part of the presentation (just be mindful of font sizes if you do the latter).\nStart with some general background about the data / research project, and an overview of the goals of the project.\nYou can see for yourself if you would like to run through some code line-by-line, or give a more high-level overview of the code you’ve written.\nAt the end, briefly mention what you still have to do – and if this is work that you will continue after this course, you can also discuss this in a broader sense. You can even explicitly ask for some advice, if you want.\n\nWhat you will be graded on:\n\nTechnical content [3 points]\nContextualization [2 points]\nDelivery [2 points]\nClarity [2 points]\nQuestions for other students [1 point]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 6193), a 2-credit Independent Studies course at Ohio State University during the Spring semester of 2024.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which usually cannot be analyzed on a desktop computer, where most software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, installing software, and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell basics and tools\nShell scripting\nComputing at OSC with Slurm batch jobs and Conda software management\nVersion control with Git and GitHub\nProject documentation with Markdown and project organization\nReproducible workflows with Nextflow\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#vs-code",
    "href": "week2/w2_vscode-markdown.html#vs-code",
    "title": "VS Code and Markdown",
    "section": "1 VS Code",
    "text": "1 VS Code\n\n1.1 Why VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, and it’s also called “Code Server” at OSC.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code.\nSome advantages of VS Code:\n\nWorks with all operating systems, is free, and open source.\nHas an integrated terminal.\nVery popular nowadays – lots of development going on including by users (extensions).\nAvailable at OSC OnDemand (and also allows you to SSH-tunnel-in with your local installation).\n\n\n\n\n1.2 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2700.\nThe “Number of hours” we want to make a reservation for: 2\nThe “Working Directory” for the program: your personal folder in /fs/ess/PAS2700/users (e.g. /fs/ess/PAS2700/users/jelmer)\nThe “Codeserver Version”: 4.8 (most recent)\n\nClick Launch.\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Welcome/Get Started page — you don’t have to go through steps that may be suggested there.\n\n\n\n\n1.3 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide/Primary) Side Bar options:\n\nExplorer: File browser & outline for the active file.\nSearch: To search recursively across all files in the active folder.\nSource Control: To work with Git (next week).\nDebugger\nExtensions: To install extensions (up soon).\n\n\n\n\n\n\n\n\nToggle (hide/show) the side bars\n\n\n\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar and the Primary Side Bar.\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\n\n\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nCreate a directory for this week, e.g.:\n# You should be in your personal dir in /fs/ess/PAS2700\npwd\n/fs/ess/PAS2700/users/jelmer\nmkdir week02\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Welcome document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n Let’s create and save a new file:\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside the dir you just created, as a Markdown file, e.g. markdown-intro.md. (Markdown files have the extension .md.)\n\n\n\n\n\n1.4 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/ess/PAS2700/users/$USER.\n\n\n\n\n\n\nIf you need to switch folders, click      &gt;   File   &gt;   Open Folder.\n\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in that it will:\n\nRe-open any files you had open in the editor pane\nRe-open a terminal if you had one active\n\nThis is quite convenient, especially when you start working on multiple projects and frequently switch between those.\n\n\n\n\nSome tips and tricks\n\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\nSpecific useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you’ll have to replace Ctrl with ⌘):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl/⌘+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl/⌘+Shift+K will delete a line\nAlt/Option+⬆/⬇ will move lines up or down.\n\n\n\n\n\n\n\n Exercise: Install two extensions\nClick the gear icon  and then Extensions, and search for and then install:\n\nshellcheck (by simonwong) — this will check our shell scripts later on!\nRainbow CSV (by mechatroner) — make CSV/TSV files easier to view with column-based colors"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#an-introduction-to-markdown",
    "href": "week2/w2_vscode-markdown.html#an-introduction-to-markdown",
    "title": "VS Code and Markdown",
    "section": "2 An introduction to Markdown",
    "text": "2 An introduction to Markdown\nMarkdown is a very lightweight text markup language that is:\n\nEasy to write — a dozen or so syntax constructs is nearly all you use.\nEasy to read — also in its raw (non-rendered) form.\n\nFor example, surrounding one or more characters by single or double asterisks (*) will make those characters italic or bold, respectively:\n\nWhen you write *italic example* this will be rendered as: italic example.\nWhen you write **bold example**this will be rendered as: bold example.\n\nSource Markdown files are plain text files (they can be “rendered” to HTML or PDF). I recommend that you use Markdown files (.md) instead of plain text (.txt) files to document your research projects as outlined in the previous session.\n\n\n\n\n\n\nMarkdown documentation\n\n\n\nLearn more about Markdown and its syntax in this excellent documentation: https://www.markdownguide.org.\n\n\n\n\nMarkdown in VS Code\nBelow, we’ll be trying some Markdown syntax in the markdown-intro.md file we created earlier.\nWhen you save a file in VS Code with an .md extension, as you have done:\n\nSome formatting will be automatically applied in the editor.\nYou can open a live rendered preview by pressing the icon to “Open Preview to the Side” (top-right corner):\n\n\n\n\n\n\nThat will look something like this in VS Code:\n\n\n\n\n\n\n\n\n2.1 Most common syntax\nHere is an overview of the most commonly used Markdown syntax:\n\n\n\nSyntax\nResult\n\n\n\n\n*italic*\nitalic (alternative: single _)\n\n\n**bold**\nbold (alternative: double _)\n\n\n[link text](website.com)\nlink text\n\n\n&lt;https://website.com&gt;\nClickable link: https://website.com\n\n\n# My Title\nHeader level 1 (largest)\n\n\n## My Section\nHeader level 2\n\n\n### My Subsection\nHeader level 3 – and so forth\n\n\n- List item\nUnordered (bulleted) list\n\n\n1. List item\nOrdered (numbered) list\n\n\n`inline code`\ninline code\n\n\n```\nStart/end of generic code block (on its own line)\n\n\n```bash\nStart of bash code block (end with ```)\n\n\n---\nHorizontal rule (line)\n\n\n&gt; Text\nBlockquote (like quoted text in emails)\n\n\n![](path/to/figure.png)\n[The figure will be inserted]\n\n\n\n\nLet’s try some of these things — type:\n# Introduction to Markdown\n\n## Part 1: Documentation\n\n- The Markdown _documentation_ can be found [here](https://www.markdownguide.org/)\n- To be clear, **the URL is &lt;https://www.markdownguide.org/&gt;**.\n\n## Part 2: The basics\n\n1. When you create a _numbered_ list...\n1. ...you don't need the numbers to increment.\n1. Markdown will take care of that for you.\n\n--------\n\n### Part 2b: Take it from the experts\n\n&gt; Markdown will take your science to the next level\n&gt; -- Wilson et al. 1843\n\n--------\n\n## Part 3: My favorite shell commands\n\nThe `date` command is terribly useful.\n\nHere is my shell code in a code block:\n\n```bash\n# Print the current date and time\ndate\n\n# List the files with file sizes\nls -lh\n```\n\n**The end.**\n\nThat should be previewed/rendered as:\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for a side-by-side screenshot in VS Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 Tables\nTables are not all that convenient to create in Markdown, but you can do it as follows.\n\n\n\n\nThis:\n| city             | inhabitants |\n|——————|——————|\n| Columbus   | 906 K       |\n| Cleveland   | 368 K       |\n| Cincinnati   | 308 K       |\n\nWill be rendered as:\n\n\n\ncity            \ninhabitants\n\n\n\n\nColumbus  \n906 K      \n\n\nCleveland  \n368 K      \n\n\nCincinnati  \n308 K      \n\n\n\n\n\n\n\n\n2.3 Whitespace\n\nIt’s recommended (in some cases necessary) to leave a blank line between different sections: lists, headers, etc.:\n## Section 2: List of ...\n\n- Item 1\n- Item 2\n\nFor example, ....\n\n\n\nA blank line between regular text will start a new paragraph, with some whitespace between the two:\n\n\n\n\n\nThis:\n\nParagraph 1.\n  \nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1.\nParagraph 2.\n\n\n\n\nWhereas a single newline will be completely ignored!:\n\n\n\n\n\nThis:\n\nParagraph 1.\nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1. Paragraph 2.\n\n\n\n\n\n\n\nThis:\n\nWriting  \none  \nword  \nper  \nline.\n\nWill be rendered as:\n\nWriting one word per line.\n\n\n\n\nMultiple consecutive spaces and blank line will be “collapsed” into a single space/blank line:\n\n\n\n\n\nThis:\n\nEmpty             space\n\nWill be rendered as:\n\nEmpty space\n\n\n\n\n\n\n\nThis:\n\nMany\n\n\n\n\nblank lines\n\nWill be rendered as:\n\nMany\nblank lines\n\n\n\n\nA single linebreak can be forced using two or more spaces (i.e., press the spacebar twice) or a backslash \\ after the last character on a line:\n\n\n\n\n\nThis:\n\nMy first sentence.\\\nMy second sentence.\n\nWill be rendered as:\n\nMy first sentence.\nMy second sentence.\n\n\n\n\nIf you want more vertical whitespace than what is provided between paragraphs, you’ll have to resort to HTML1: each &lt;br&gt; item forces a visible linebreak.\n\n\n\n\n\nThis:\n\nOne &lt;br&gt; word &lt;br&gt; per line\nand &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\nseveral blank lines.\n\nWill be rendered as:\n\nOne  word  per line and      several blank lines.\n\n\n\n\n\n\n\n\n\nSidenote: HTML and CSS in Markdown\n\n\n\n\nIf you need “inline colored text”, you can also use HTML:\ninline &lt;span style=\"color:red\"&gt;colored&lt;/span&gt; text.\nFor systematic styling of existing or custom elements, you need to use CSS. For example, including the following anywhere in a Markdown document will turn all level 1 headers (#) red:\n&lt;style&gt;\nh1 {color: red}\n&lt;/style&gt;\n\n\n\n\n\n\n2.4 Markdown extensions – Markdown for everything?!\nSeveral Markdown extensions allow Markdown documents to contain code that runs, and whose output can be included in rendered documents:\n\nR Markdown (.Rmd) and the follow-up Quarto\nJupyter Notebooks\n\nThere are many possibilities with Markdown! For instance, consider that:\n\nThis website and last week’s slides are written using Quarto.\nR Markdown/Quarto also has support for citations, journal-specific formatting, etc., so you can even write manuscripts with it.\n\n\n\n\n\n\n\n\nPandoc to render Markdown files (Click to expand)\n\n\n\n\n\nI very rarely render “plain” Markdown files because:\n\nMarkdown source is so well readable\nGitHub will render Markdown files for you\n\nThat said, if you do need to render a Markdown file to, for example, HTML or PDF, use Pandoc:\npandoc README.md &gt; README.html\npandoc -o README.pdf README.md\nFor installation (all OS’s): see https://pandoc.org/installing.html.\n\n\n\n\n\n\n\n\n\nSome additional Markdown syntax (Click to expand)\n\n\n\n\n\nThe below is “extended syntax” that is not supported by all interpreters:\n\n\n\nSyntax\nResult\n\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\nFootnote ref[^1]\nFootnote ref1\n\n\n[^1]: Text\nThe actual footnote"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#footnotes",
    "href": "week2/w2_vscode-markdown.html#footnotes",
    "title": "VS Code and Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any HTML markup in Markdown!↩︎"
  },
  {
    "objectID": "week2/w2_project-org.html#overview-of-this-week",
    "href": "week2/w2_project-org.html#overview-of-this-week",
    "title": "Project organization",
    "section": "1 Overview of this week",
    "text": "1 Overview of this week\n\nThis page:\n\nLearn some best practices for project organization, documentation, and management.\n\nAlso today:\n\nGet to know our text editor, VS Code.\nLearn how to use Markdown for documentation (and beyond).\n\nThursday\n\nLearn how to manage files in the Unix shell."
  },
  {
    "objectID": "week2/w2_project-org.html#project-organization-best-practices-recommendations",
    "href": "week2/w2_project-org.html#project-organization-best-practices-recommendations",
    "title": "Project organization",
    "section": "2 Project organization: best practices & recommendations",
    "text": "2 Project organization: best practices & recommendations\nGood project organization and documentation facilitates:\n\nCollaborating with others (and yourself in the future…)\nReproducibility\nAutomation\nVersion control\nPreventing your files slowly devolving into a state of incomprehensible chaos\n\nIn short, it is a necessary foundation to use this course’s tools and to reach some of its goals.\n\n\n2.1 Some underlying principles\n\nUse one dir (dir hierarchy) for one project\nUsing one directory hierarchy for one project means:\n\nDon’t mix files/subdirs for multiple distinct projects inside one dir.\nDon’t keep files for one project in multiple places.\n\nWhen you have a single directory hierarchy for each project, it is:\n\nEasier to find files, share your project, not throw away stuff in error, etc.\nPossible to use relative paths within a project’s scripts, which makes it more portable (more on that in a bit).\n\n\n\n\n\nTwo project dir hierarchies, nicely separated and self-contained.\n\n\n\n\n\nSeparate different kinds of files using a consistent dir structure\nWithin your project’s directory hierarchy:\n\nSeparate code from data.\nSeparate raw data from processed data & results.\n\nAlso:\n\nTreat raw data as read-only.\nTreat generated output as somewhat disposable and as possible to regenerate.\n\nAnd, as we’ll talk about below:\n\nUse consistent dir and file naming that follow certain best practices.\nSlow down and document what you’re doing.\n\n\n\n\n\n2.2 Absolute versus relative paths\nRecall that:\n\nAbsolute paths start from the computer’s root dir and do not depend on your working dir.\nRelative paths start from a specific working dir (and won’t work if you’re elsewhere).\n\n\n\nDon’t absolute paths sound better? What could be a disadvantage of them?\n\nAbsolute paths: - Don’t generally work across computers - Break when your move a project\nWhereas relative paths, as long as you consistently use the root of the project as the working dir, keep working when moving the project within and between computers.\n\n\n\n\n\nTwo project dir hierarchies, and the absolute and relative path to a FASTQ file.\n\n\n\n\n\n\nNow everything was moved into Dropbox.The absolute path has changed, but the relative path remains the same.\n\n\n\n\n\n2.3 But how to define and separate projects?\nFrom Wilson et al. 2017 - Good Enough Practices in Scientific Computing:\n\nAs a rule of thumb, divide work into projects based on the overlap in data and code files:\n\nIf 2 research efforts share no data or code, they will probably be easiest to manage independently.\nIf they share more than half of their data and code, they are probably best managed together.\nIf you are building tools that are used in several projects, the common code should probably be in a project of its own.\n\n\n\nProjects with shared data or code\nTo access files outside of the project (e.g., shared across projects), it is easiest to create links to these files:\n\n\n\nThe data is located in project1 but used in both projects.project2 contains a link to the data.\n\n\nBut shared data or scripts are generally better stored in separate dirs, and then linked to by each project using them:\n\n\n\nNow, the data is in it’s own top-level dir, with links to it in both projects.\n\n\nThese strategies do decrease the portability of your project, and moving the shared files even within your own computer will cause links to break.\nA more portable method is to keep shared (multi-project) files online — this is especially feasible for scripts under version control:\n\n\n\nA set of scripts shared by two projects is stored in an online repository like at GitHub.\n\n\n\nFor data, this is also possible but often not practical due to file sizes. It’s easier after data has been deposited in a public repository.\n\n\n\n\n\n2.4 Example project dir structure\nHere is one good way of organizing a project with top-levels dirs:\n\n\n\n\n\n\n\n\n\n\n\nOther reasonable options\n\n\n\nThese recommendations only go so far, and several things do depend on personal preferences and project specifics:\n\ndata as single top-level dir, or separate metadata, refdata, raw_data dirs?\n\nNaming of some dirs, like:\n\nresults vs analysis (Buffalo)\nsrc (source) vs scripts\n\nSometimes the order of subdirs can be done in multiple different ways. For example, where to put QC figures — results/plots/qc or results/qc/plots/?\n\n\n\nAnother important good practice is to use subdirectories liberally and hierarchically. For example, in omics data analysis, it often makes sense to create subdirs within results for each piece of software that you are using:"
  },
  {
    "objectID": "week2/w2_project-org.html#file-naming",
    "href": "week2/w2_project-org.html#file-naming",
    "title": "Project organization",
    "section": "3 File naming",
    "text": "3 File naming\nThree principles for good file names (from Jenny Bryan):\n\nMachine-readable\nHuman-readable\nPlaying well with default ordering\n\n\nMachine-readable\nConsistent and informative naming helps you to programmatically find and process files.\n\nIn file names, provide metadata like Sample ID, date, and treatment:\n\nsample032_2016-05-03_low.txt\n\nsamples_soil_treatmentA_2019-01.txt\n\nWith such file names, you can easily select samples from e.g. a certain month or treatment (more on Thursday):\nls *2016-05*\n\nls *treatmentA*\nSpaces in file names lead to inconvenience at best and disaster at worst (see example below).\nMore generally, only use the following in file names:\n\nAlphanumeric characters A-Za-z0-9\nUnderscores _\nHyphens (dashes) -\nPeriods (dots) .\n\n\n\n\n\n Spaces in file names — what could go wrong?\n\nSay, you have a dir with some raw data in the dir raw:\nls\nraw\nNow you create a dir for sequences, with a space in the file name:\nmkdir \"raw sequences\"\nYou don’t want this dir after all, and carelessly try to remove it\nrm -r raw sequences\n\n\n\n\nWhat will go wrong in the example above? (Click for the answer)\n\nThe rm command will not remove the dir with the space in the file name, but it will remove the “earlier” raw dir.\n\n\n\n\nHuman-readable\n\n“Name all files to reflect their content or function. For example, use names such as bird_count_table.csv, manuscript.md, or sightings_analysis.py.”\n— Wilson et al. 2017\n\n\n\n\nCombining machine- and human-readable\n\nOne good way (opinionated recommendations):\n\nUse underscores (_) to delimit units you may later want to separate on: sampleID, batch, treatment, date.\nWithin such units, use dashes (-) to delimit words: grass-samples.\nLimit the use of periods (.) to indicate file extensions.\nGenerally avoid capitals.\n\nFor example:\nmmus001_treatmentA_filtered-mq30-only_sorted_dedupped.bam\nmmus002_treatmentA_filtered-mq30-only_sorted_dedupped.bam\n.\n.\nmmus086_treatmentG_filtered-mq30-only_sorted_dedupped.bam\n\n\n\n\nPlaying well with default ordering\n\nUse leading zeros for lexicographic sorting: sample005.\nDates should always be written as YYYY-MM-DD: 2020-10-11.\nGroup similar files together by starting with same phrase, and number scripts by execution order:\nDE-01_normalize.R\nDE-02_test.R\nDE-03_process-significant.R"
  },
  {
    "objectID": "week2/w2_project-org.html#slow-down-and-document",
    "href": "week2/w2_project-org.html#slow-down-and-document",
    "title": "Project organization",
    "section": "4 Slow down and document",
    "text": "4 Slow down and document\n\nUse README files to document\nUse README files to document the following:\n\nYour methods\nWhere/when/how each data and metadata file originated\nVersions of software, databases, reference genomes\n…Everything needed to rerun whole project\n\n\n\n\n\n\n\nSee this week’s Buffalo chapter (Ch. 2) for further details.\n\n\n\n\n\n\n\n\n\nFor documentation, use plain text files\nPlain text files offer several benefits over proprietary & binary formats (like .docx and .xlsx)1:\n\nCan be accessed on any computer, including over remote connections\nAre future-proof\nAllow to be version-controlled\n\nMarkdown files are plain-text and strike a nice balance between ease of writing and reading, and added functionality — we’ll talk about those next."
  },
  {
    "objectID": "week2/w2_project-org.html#footnotes",
    "href": "week2/w2_project-org.html#footnotes",
    "title": "Project organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese considerations apply not just to files for documentation, but also to data files, etc!↩︎"
  },
  {
    "objectID": "week2/w2_exercises.html#exercise-1-course-notes-in-markdown",
    "href": "week2/w2_exercises.html#exercise-1-course-notes-in-markdown",
    "title": "Week 2 exercises",
    "section": "Exercise 1: Course notes in Markdown",
    "text": "Exercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\n\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, and hyperlinks.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline."
  },
  {
    "objectID": "week2/w2_exercises.html#exercise-2-organize-project-files",
    "href": "week2/w2_exercises.html#exercise-2-organize-project-files",
    "title": "Week 2 exercises",
    "section": "Exercise 2: Organize project files",
    "text": "Exercise 2: Organize project files\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\n\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. Do this within your personal dir in the course’s project dir (/fs/ess/PAS2700/users/$USER/w02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\n\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nCreate mock “alignment” files1\n\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results)\nInside the alignment dir, create files with names like sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam for all combinations of:\n\n30 samples (01-30),\n5 treatments (A-E)\n2 dates (08-14-2020 and 09-16-2020 – yes, use this date format for now)\n\n\nThese 300 files can be created with a single touch command 2.\n\n\n\nHints\n\nUse brace expansion three times in the command: to expand (1) sample IDs, (2) treatments, and (3) dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with BAM files, and use a for loop to rename them, changing the extension from .sam to .bam.\n\n\n\nHints\n\n\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\n\n\n\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\n\nThe sample ID/number should be 01-19;\nThe treatment should be A, B, or C.\n\nCreate a README.md in the dir that explains what you did.\n\n\n\nHints\n\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\n\nCreate a README\nInclude a README.md that described what you did; again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\n\n\n\nHints\n\n\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\n\n\n\nBonus: Change file permissions\nMake sure no-one has write permissions for the raw data files, not even yourself. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\n\n\n\nHints\n\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee this Bonus section of the Managing files in the shell page for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents)."
  },
  {
    "objectID": "week2/w2_exercises.html#bonus-exercises",
    "href": "week2/w2_exercises.html#bonus-exercises",
    "title": "Week 2 exercises",
    "section": "Bonus exercises",
    "text": "Bonus exercises\n\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\n\n\n\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3."
  },
  {
    "objectID": "week2/w2_exercises.html#solutions",
    "href": "week2/w2_exercises.html#solutions",
    "title": "Week 2 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 2\n\n\n(1.) Getting set up\n\n# For example:\nmkdir /fs/ess/PAS2700/users/$USER/week02/ex2\n\ncd /fs/ess/PAS2700/users/$USER/week02/ex2\n\n\n\n(2.) Create a disorganized mock project\n\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n\n\n(3.) Organize the mock project\n\nAn example:\n\nCreate directories:\nmkdir -p data/{raw,meta,ref}\nmkdir -p results/{bam,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/raw/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/bam/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n\n\n\n(4.) Create mock alignment files\n\nmkdir -p results/alignment\ncd results/alignment \n\n# Create the files:\ntouch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\nls | wc -l\n3000\n\n\n\n(5.) Rename files in a batch\n\nBelow:\n\n$oldname will contain the old file name in each iteration of the loop\nWe remove the sam suffix using basename \"$oldname\" sam\nWe use command substitution ($() syntax) to catch the output of the basename command, and paste bam at the end\n\nfor oldname in *.sam; do\n   newname=$(basename \"$oldname\" sam)bam\n   mv -v \"$oldname\" \"$newname\"\ndone\nAlso, note that:\n\nWe don’t need a special construction to paste strings together. we simply type bam after what will be the extension-less file name from the basename command.\nI used informative variable names (oldname and newname), not cryptic ones like i and o.\n\n\n\n\n(6.) Copy files with wildcards\n\n\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\n\nThe first digit should be a 0 or a 1 [0-1],\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp sample[0-1][0-9]_[A-C]* subset/\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir 'subset' and copied only files for samples 1-29\nand treatments A-D into this dir\" &gt; subset/README.md\n\n# See if it worked:\ncat subset/README.md\n\n\n\n\n(8.) Bonus: a trickier renaming loop\n\nfor oldname in *.bam; do\n     # Use `cut` to extract month, day, year, and a \"prefix\" that contains\n     # the sample number and the treatment, and save these using command substitution:\n     month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n     day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n     year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n     prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n     \n     # Paste together the new name:\n     newname=\"$prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n     \n     # Execute the move:\n     mv -v \"$oldname\" \"$newname\"\ndone\n(This renaming task can be done more succinctly using regular expressions and the sed command — we’ll learn about both of these topics later in the course.)\n\n\n\n(9.) Change file permissions\n\nTo ensure that no-one has write permission for the raw data, you could, for example, use:\nchmod a=r data/raw/*   # set permissions for \"a\" (all) to \"r\" (read)\n\nchmod a-w data/raw/*   # take away \"w\" (write) permissions for \"a\" (all)"
  },
  {
    "objectID": "week2/w2_exercises.html#footnotes",
    "href": "week2/w2_exercises.html#footnotes",
    "title": "Week 2 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReal alignment files like SAM/BAM are generated by aligning FASTQ sequence reads to a reference genome.↩︎\nIf you already happened to have an alignment dir among your mock project dirs, first delete its contents or rename it.↩︎"
  },
  {
    "objectID": "week5/w5_exercises.html",
    "href": "week5/w5_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/w1_removed.html",
    "href": "week4/w1_removed.html",
    "title": "Removed w1 exercises",
    "section": "",
    "text": "0.1 1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nShow hints\n\n\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n\n\n\n\n\n0.2 1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\nbash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\n\n\nShow hints\n\nTo build the script, you need to combine several commands:\n\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n\n\n\n\n\n0.3 1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\n\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\n\nFor example, running the script as below should produce the following output:\nbash explore.sh ../data/Buzzard2015_data.csv 7\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\n\n\nShow hints\n\n\nYou can select a given column from a csv file using the command cut. Then:\n\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\n\n\n\n\n0.4 Solutions\n\n\n0.5 1.10.2 Hormone Levels in Baboons\n\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\n\ncut -f 1 $1 | grep -c -w $2\n\nA slightly more verbose and readable example:\n\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\n\n\n\n\nNote\n\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\n\nTo run the script, assuming it is named count_baboons.sh:\n\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n\n\n\n0.6 1.10.3 Plant–Pollinator Networks\n\n\nSolution\n\n\nCounting rows:\n\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\n\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\n\nTo avoid printing the file name, we can either use cat or input redirection:\n\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l &lt; ../data/Saavedra2013/n10.txt \nCounting rows:\n\nCounting the number of columns is more work. First, we need only the first line:\n\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n\nNow we can remove all spaces and the line terminator using tr:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\n\nFinally, we can use wc -c to count the number of characters in the string:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\n\nTo run the script, assuming it is named counter.sh:\n\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\n\n\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n\n\n\n\n\n\n\n0.7 1.10.4 Data Explorer\n\n\nSolution\n\n\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\n\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\n\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\n\nHere is an example of what the script could look like:\n\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/usethis.html",
    "href": "week4/usethis.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "More shell keyboard shortcuts\n\n\n\n\nPress ⇧ to get the previous command back on the prompt, and then press Ctrl+U to delete text until the beginning of the line.\nCtrl+U actually cuts the text: “Yank” it back with Ctrl+Y. Press Enter again.\nType cd and then space, and then press Alt+. (or Esc+. on a Mac). That should insert /fs/ess/PAS2700 on the line. In general terms, this keyboard shortcut will insert the last word (argument) from the last command line (and you can cycle back by pressing it multiple times!).\n\n\n\n\n\n Exercise: Add keyboard shortcut to run shell commands from the editor:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week3/w3_git2.html#overview",
    "href": "week3/w3_git2.html#overview",
    "title": "Git: Remotes on GitHub",
    "section": "1 Overview",
    "text": "1 Overview\n\nOverview of this session\n\nRemote repositories on GitHub\nSome Git best practices\n\n\n\n\n\n\n\n\nGetting set up at OSC\n\n\n\n\nLaunch VS Code at https://ondemand.osc.edu as before, at the dir /fs/ess/PAS2700/users/$USER, and open a terminal in VS Code.\nLoad the OSC Git module1: run module load git/2.39.0.\nIn the terminal, cd to your week03/originspecies dir."
  },
  {
    "objectID": "week3/w3_git2.html#remote-repositories",
    "href": "week3/w3_git2.html#remote-repositories",
    "title": "Git: Remotes on GitHub",
    "section": "2 Remote repositories",
    "text": "2 Remote repositories\nSo far, we have been locally version-controlling our originspecies repository. Now, we also want to put this repo online, so we can:\n\nShare our work (e.g. alongside a publication) and/or\nCollaborate with others and/or\nHave an online backup.\n\nWe will use the GitHub website as the place to host our online repositories.\nOnline counterparts of repositories that we also have locally are usually referred to as “remote repositories” or simple “remotes”. To add and manage remotes, we use the git remote command.\n\n2.1 One-time setup: GitHub authentication\nTo be able to link local Git repositories to their online counterparts on GitHub, we need to set up GitHub authentication.\nRegular password access (over HTTP/HTTPS) is now “deprecated” by GitHub, and two better options are to set up SSH access with an SSH key, or HTTPS access with a Public Access Token.\nWe’ll use SSH, as it is easier – though still a bit of drag – and because learning this procedure will also be useful for when you’ll be setting up SSH access to the Ohio Supercomputer Center. (But note that GitHub now labels HTTPS access as the “preferred” method.)\nFor everything on GitHub, there are separate SSH and HTTPS URLs, and GitHub can always show you both types of URLs. When using SSH, we need to use URLs with the following format:\ngit@github.com:&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git\n(And when using HTTPS, you would use URLs like https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git)\n\nUse the ssh-keygen command to generate a public-private SSH key pair like so:\nssh-keygen -t rsa\nYou’ll be asked three questions, and for all three, you can accept the default by just pressing Enter:\n# Enter file in which to save the key (&lt;default path&gt;):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \n\n\n\n\n\n\n\nNow, you have a file called id_rsa.pub in your ~/.ssh folder, which is your public key. To enable authentication, we will put this public key on GitHub — our public key interacts with our private key, which we do not share.\nPrint the public key to screen using cat:\ncat ~/.ssh/id_rsa.pub\nCopy the public key, i.e. the contents of the public key file, to your clipboard. Make sure you get all of it, including the “ssh-rsa” part (but beware that your new prompt may start on the same line as the end the key):\n\n\n\n\n\n\n\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, and select Settings in the drop-down menu.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nGive the key an arbitrary, informative name, e.g. “OSC” to indicate that you are using this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the box.\n\n\n\n\n\n\n\nClick the green Add SSH key button. Done!\n\n\n\n\n2.2 Creating a remote repository\nWhile we can interact with online repos using Git commands, we can’t create a new online repo with the Git CLI. Therefore, we will to go to the GitHub website to create a new online repo:\n\nGo to https://github.com and sign in.\nIn the top-right, click the + next to your avatar and then select “New repository”:\n\n\n\n\n\n\n\nIn the box “Repository name”, we’ll use the same name that we gave to our local directory: originspecies2.\n\n\n\n\n\n\n\nLeave other options as they are, so don’t check any of these boxes, and click “Create repository”:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVS Code Git functionality also allows you to create GitHub repos (Click to expand)\n\n\n\n\n\nVS Code does provide functionality to create GitHub repos directly. If you’re interested, look for the GitLens VS Code extension by Eric Amodio.\n\n\n\n\n\n\n2.3 Link the local and remote repositories\nAfter you clicked the “Create repository”, a page with information similar to this screenshot should appear, which gives us some information about linking the remote and local repositories:\n\n\n\n\n\nWe go back to our Unix shell in VS Code, where we’ll enter the commands that GitHub provided to us under the “…or push an existing repository from the command line” heading shown at the bottom of the screenshot above:\n\nFirst, we tell Git to add a “remote” connection with git remote. We provide three arguments to this command:\n\nadd — because we’re adding a remote.\norigin — the arbitrary nickname we’re giving the connection (usually called “origin” by convention).\n\nThe SSH URL to the GitHub repo (you can click on the HTTPS/SSH button to toggle the URL type).\n\n# git remote add &lt;remote-nickname&gt; &lt;URL&gt;\ngit remote add origin git@github.com:&lt;user&gt;/originspecies.git\nSecond, we push our local repo to remote using git push. Whenever we push a repository3 for the first time, we need to use the -u option to set up an “upstream” counterpart:\n# git push -u &lt;connection&gt; &lt;branch&gt;\ngit push -u origin main\n\n\n\n\n\n\n\n\nPushing will be easier from now on\n\n\n\nNote that when we don’t give git push any arguments, it will push:\n\nTo & from the currently active branch (default: main)\nTo the default remote connection.\n\nTherefore, from now on, we can simply use the following to push:\ngit push\n\n\n\n\n\n2.4 Explore the repository on GitHub\nBack at GitHub, click on &lt;&gt; Code in the lower of the top bars. There, we can see the files that we just uploaded from our local repo.\n\n\n\n\n\nNext, click where it says x commits (should be 5) with a clock icon, and you’ll get an overview of commits, somewhat similar to what we’ve seen when we ran git log:\n\n\n\n\n\nOn the right hand side, there are three buttons for each commit:\n\nClick the hash (hexadecimal ID, 5aff0ae in the screenshot below) to see changes made by that commit.\nClick the &lt; &gt; to see the state of the repo at the time of that commit."
  },
  {
    "objectID": "week3/w3_git2.html#remote-repo-workflows-single-user",
    "href": "week3/w3_git2.html#remote-repo-workflows-single-user",
    "title": "Git: Remotes on GitHub",
    "section": "3 Remote repo workflows: single-user",
    "text": "3 Remote repo workflows: single-user\nIn a single-user workflow, all changes are made in the local repo, and the remote repo is simply periodically updated (pushed to). So, the interaction between local and remote is unidirectional:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we have a remote in a single-user workflow, we commit as usual in our day-to-day work, and in addition, push to remote occasionally — let’s run through an example.\n\nWe start by creating a README.md file for our repo:\necho \"# Origin\" &gt; README.md\necho \"Repo for book draft on my new **theory**\" &gt;&gt; README.md\nWe add and commit the file:\ngit add README.md\ngit commit -m \"Added a README file\"\n[main 63ce484] Added a README file\n1 file changed, 2 insertions(+)\ncreate mode 100644 README.md\nNow, we push to the remote repository:\ngit push\nTODO\n\n\nLet’s go back to GitHub: we should see that the contents of the README.md automatically shows up as a rendered Markdown file!\nTODO ADD SCREENSHOT"
  },
  {
    "objectID": "week3/w3_git2.html#remote-repo-workflows-multi-user",
    "href": "week3/w3_git2.html#remote-repo-workflows-multi-user",
    "title": "Git: Remotes on GitHub",
    "section": "4 Remote repo workflows: multi-user",
    "text": "4 Remote repo workflows: multi-user\nThe added command is git pull EXPAND\n\n\n\n\n\nFirst, a second user downloads (clones) the online repo.\n\n\n\n\n\n\nThen, the second user can push their changes to the shared remote.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo go full circle, you pull in the changes made by the second user.\n\n\n\n\nWith a multi-user workflow, changes made by different users are shared via the online copy of the repo. But note that syncing is not automatic:\n\nChanges to your local repo remain local-only until you push to remote.\nSomeone else’s changes to the remote repo do not make it into your local repo until you pull from remote.\n\nWhen your collaborator has made changes, Git will tell you about “divergence” between your local repository and the remote when you run git status:\n\n\n\n\n\n\n\n4.1 What can you do with someone else’s GitHub repository?\nIn some cases, you may be interested in working in some way with someone else’s repository that you found on GitHub. If you do not have rights to push, you can:\n\nClone the repo and make changes locally (as we have been doing with the CSB repo). When you do this, you can also periodically pull to remain up-to-date with changes in the original repo.\nFork the repository on GitHub and develop it independently. Forking creates a new personal GitHub repo, to which you can push.\nUsing a forked repo, you can also submit a Pull Request with proposed changes to the original repo: for example, if you’ve fixed a bug in someone else’s program.\n\nIf you’re actually collaborating on a project, though, you should ask your collaborator to give you admin rights for the repo, which makes things easier.\n\n\n\n\n\n\nForking, Pull Requests, and Issues (below) are GitHub functionality, and not part of Git.\n\n\n\n\n\n\n\n\n\n4.2 GitHub “Issues”\nEach GitHub repository has an “Issues” tab — issues are mainly used to track bugs and other (potential) problems with a repository. In an issue, you can reference specific commits and people, and use Markdown formatting.\n\n\n\n\n\n\nWhen you hand in your final project submissions, you will create an issue simply to notify me about your repository."
  },
  {
    "objectID": "week3/w3_git2.html#some-git-best-practice-tips",
    "href": "week3/w3_git2.html#some-git-best-practice-tips",
    "title": "Git: Remotes on GitHub",
    "section": "5 Some Git best-practice tips",
    "text": "5 Some Git best-practice tips\n\nWrite informative commit messages.\nImagine looking back at your project in a few months, after finding an error that you introduced a while ago.\n\nNot-so-good commit message: “Updated file”\nGood commit message: “In file x, updated function y to include z”\n\n\n\n\n\nImage source\n\n\n\n\n\n\n\n\nCommit messages for the truly committed\n\n\n\nIt is often argued that commit messages should preferably be in the form of completing the sentence “This commit will…”: When adhering to this, the above commit message would instead say “In file x, update function y to include z.”.\n\n\n\nCommit often, using small commits.\nThis will also help to keep commit messages informative!\nBefore committing, check what you’ve changed.\nUse git diff [--staged] or VS Code functionality.\nAvoid including unrelated changes in commits.\nSeparate commits if your working dir contains work from disparate edits: use git add + git commit separately for two sets of files.\nWhen collaborating: pull often.\nThis will reduce the chances of merge conflicts.\nDon’t commit unnecessary files.\nThese can also lead to conflicts — especially automatically generated, temporary files.\nTags\nIf you have a repo with general scripts, which you continue to develop and use in multiple projects, and you publish a paper in which you use these scripts, it is a good idea to add a “tag” to a commit to mark the version of the scripts used in your analysis:\ngit tag -a v1.2.0 -m \"Clever release title\"\ngit push --follow-tags"
  },
  {
    "objectID": "week3/w3_git2.html#footnotes",
    "href": "week3/w3_git2.html#footnotes",
    "title": "Git: Remotes on GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Git is available at OSC even without loading this module, but that’s a much older version.↩︎\nThough note that these names don’t have to match up.↩︎\nOr technically, for every branch↩︎"
  },
  {
    "objectID": "week3/w3_exercises.html#introduction",
    "href": "week3/w3_exercises.html#introduction",
    "title": "Week 3 exercises",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn these exercises, you will primarily be practicing your Git skills. You will also practice Markdown and a for loop to rename files.\nSome general hints:\n\nDon’t forget to constantly check the status of your repository (repo) with git status: before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand what’s going on as you get up and running with Git.\nIt’s also a good idea to regularly check the log with git log; recall that git log --oneline will provide a quick overview with one line per commit."
  },
  {
    "objectID": "week3/w3_exercises.html#part-i-create-a-new-git-repo",
    "href": "week3/w3_exercises.html#part-i-create-a-new-git-repo",
    "title": "Week 3 exercises",
    "section": "2 Part I – Create a new Git repo",
    "text": "2 Part I – Create a new Git repo\n\nCreate a new directory at OSC.\n\n\n\nA good place for this directory is in /fs/ess/PAS2714/users/$USER/week03/, but you are free to create it elsewhere (I will only be checking the online version of your repo).\nI suggest the name exercises for the dir, but you are free to pick a name that makes sense to you.\n\n\nLoad the OSC git module.\nDon’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory.\nCreate a README file in Markdown format.\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your exercises.\nStage and then commit the README file.\nInclude an appropriate commit message."
  },
  {
    "objectID": "week3/w3_exercises.html#part-ii-add-some-markdown-content",
    "href": "week3/w3_exercises.html#part-ii-add-some-markdown-content",
    "title": "Week 3 exercises",
    "section": "3 Part II – Add some Markdown content",
    "text": "3 Part II – Add some Markdown content\n\nCreate a second Markdown file with some more contents.\nThis file can have any name you want, and you can also choose what you want to write about.\nTry to use of avariety of Markdown syntax, as discussed and practiced in week 2: headers, lists, hyperlinks, and so on. Also, make sure to read the next step before you finish writing.\n(If you’re not feeling inspired, here are some suggestions: lecture and reading notes for this week; a table of Unix and/or Git commands that we’ve covered; things that you so far find challenging or interesting about Git; how computational skills may help you with your research.)\n\n\n\nHints\n\nIn VS Code, open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.\n\n\nCreate at least two commits while you work on the Markdown file. Try to break your progress up into logical units that can be summarized with a descriptive commit message.\n\n\n\nHints\n\n\nBad commits/commit messages:\n“First part of the file” along with “Second part of the file”.\nGood commits and commit messages:\n“Summarized key concepts in the Git workflow” along with “Made a table of common Git commands”.\n\n\n\nUpdate the README.md file.\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file."
  },
  {
    "objectID": "week3/w3_exercises.html#part-iii-add-some-data-files",
    "href": "week3/w3_exercises.html#part-iii-add-some-data-files",
    "title": "Week 3 exercises",
    "section": "4 Part III – Add some “data” files",
    "text": "4 Part III – Add some “data” files\n\nCreate a directory with dummy data files.\nCreate a directory called data and inside this directory, create 100 empty files with a single touch command and at least one brace expansion (e.g. for samples numbered 1-100, or 20 samples for 5 treatments). Give the files the extension .txt.\nStage and commit the data directory and its contents.\nAs always, include a descriptive message with your commit."
  },
  {
    "objectID": "week3/w3_exercises.html#part-iv-create-and-sync-an-online-version-of-the-repo",
    "href": "week3/w3_exercises.html#part-iv-create-and-sync-an-online-version-of-the-repo",
    "title": "Week 3 exercises",
    "section": "5 Part IV – Create and sync an online version of the repo",
    "text": "5 Part IV – Create and sync an online version of the repo\nPhew, we made several commits! Let’s share all of this work with the world.\n\nCreate a Github repository.\nGo to https://github.com, sign in, and create a new repository.\n\nIt’s a good idea to give it the same name as your local repo, but these names don’t have to match.\nYou want to create an empty GitHub repository, because you will upload all the contents from your local repo: therefore, don’t check any of the boxes to create files like a README.\n\nPush your local repo online.\nWith your repo created, follow the instructions that Github now gives you, under the heading “…or push an existing repository from the command line”.\nThese instructions will comprise three commands: git remote add to add the “remote” (online) connection, git branch to rename the default branch from master to main, and git push to actually “push” (upload) your local repo.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\n\n\n\nHints\n\nAssuming that you’re using SSH authentication, which you should have set up in this week’s ungraded assignment, make sure you use the SSH link type: starting with git@github.com rather than HTTPS.\n\n\nCreate an issue to mark that you’re done! Find the “Issues” tab for your repo on Github:\n\n\n\n\n\nIn the Issues tab, click the green button New Issue to open a new issue for you repo.\nGive it a title like “I finished my assignment”, and in the issue text, tag @jelmerp. You can say, for instance, “Hey @jelmerp, can you please take a look at my repo?”."
  }
]