[
  {
    "objectID": "week3/w3_overview.html",
    "href": "week3/w3_overview.html",
    "title": "Week 3",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/w4_overview.html",
    "href": "week4/w4_overview.html",
    "title": "Week 4",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/w4_exercises.html",
    "href": "week4/w4_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week5/w5_overview.html",
    "href": "week5/w5_overview.html",
    "title": "Week 5",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_tue.html",
    "href": "week2/w2_tue.html",
    "title": "Untitled",
    "section": "",
    "text": "Press Ctrl+A to move to the beginning of the line, and add ls to the beginning, then press Enter (anywhere on the line!) to execute the command (the ls command will list files):\nls /fs/ess/PAS2700\nadmin users\nPress ⇧ to get the previous command back on the prompt, and then press Ctrl+U to delete text until the beginning of the line.\nCtrl+U actually cuts the text: “Yank” it back with Ctrl+Y. Press Enter again.\nType cd and then space, and then press Alt+. (or Esc+. on a Mac). That should insert /fs/ess/PAS2700 on the line. In general terms, this keyboard shortcut will insert the last word (argument) from the last command line (and you can cycle back by pressing it multiple times!).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 6193), a 2-credit Independent Studies course at Ohio State University during the Spring semester of 2024.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which usually cannot be analyzed on a desktop computer, where most software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, installing software, and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell basics and tools\nShell scripting\nComputing at OSC with Slurm batch jobs and Conda software management\nVersion control with Git and GitHub\nProject documentation with Markdown and project organization\nReproducible workflows with Nextflow\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week7/w7_overview.html",
    "href": "week7/w7_overview.html",
    "title": "Week 7",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week0/overview.html#assignments",
    "href": "week0/overview.html#assignments",
    "title": "Before the course starts",
    "section": "1 Assignments",
    "text": "1 Assignments\nPlease complete these assignments in the week of Feb 19th:\n\nFill out the pre-course survey\nCreate or check your OSC account"
  },
  {
    "objectID": "week0/overview.html#readings",
    "href": "week0/overview.html#readings",
    "title": "Before the course starts",
    "section": "2 Readings",
    "text": "2 Readings\n\nSyllabus\nPlease read the updated syllabus, which you can find here and on the CarmenCanvas site for the course.\n\n\nAdditional information\n\nCourse websites\nThe main place to access information about this course is this github.io website. I will only use the CarmenCanvas site for this course for announcements and as a place to share some files with you.\n\n\nNo R in the course\nI’ve had to remove the modules on R while transitioning this course from 3-credit full-semester to 2-credit half-semester. If this is disappointing to you and you want to learn R at OSU, I can recommend the weekly Code Club meeting that I co-organize. And if you’re also interested in microbial metabarcoding data, or would like to learn how you can use R in a bioinformatics/omics data context, contact Plant Pathology professor Soledad Benitez-Ponce to sign up for a metabarcoding workshop during Spring break from March 13th-15th.\n\n\nGlossary\nIf some of the terms in the description of the course material are unfamiliar to you, take a look at the glossary page of this website. Of course, you will learn a lot more about these terms and concepts during the course!"
  },
  {
    "objectID": "week1/w1_exercises.html",
    "href": "week1/w1_exercises.html",
    "title": "Week 1 Exercises",
    "section": "",
    "text": "The following are some of the exercises from Chapter 1 of the CSB book."
  },
  {
    "objectID": "week1/w1_exercises.html#getting-set-up",
    "href": "week1/w1_exercises.html#getting-set-up",
    "title": "Week 1 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files.\nIf not, go to /fs/ess/PAS2700/users/$USER, and run git clone https://github.com/CSB-book/CSB.git.\nNow, you should have the CSB directory referred to in the first step of the first exercise."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.1",
    "href": "week1/w1_exercises.html#intermezzo-1.1",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.1",
    "text": "Intermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.2",
    "href": "week1/w1_exercises.html#intermezzo-1.2",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.2",
    "text": "Intermezzo 1.2\nTo familiarize yourself with these basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 23) demonstrates the use of the touch command to create a new, empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.3",
    "href": "week1/w1_exercises.html#intermezzo-1.3",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.3",
    "text": "Intermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? Which is the last?\n\n\nShow hints\n\nYou can either first select the 5th column using cut, and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter.\nTo view just the first or the last line so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nStart by selecting the relevant column.\nUse a tail trick shown in the chapter to exclude the first line.\nRemember to sort before using uniq."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "href": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.1: Next-Generation Sequencing Data",
    "text": "Exercise 1.10.1: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\n\nShow hints\n\nRecall that with the cp command, it is also possible to give the copy a new name at once.\n\n4. How many contigs are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pass the output of grep to wc -l.\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nShow hints\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n With Unix commands, we can’t write output to a file that also serves as input (see here), so the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "href": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.2: Hormone Levels in Baboons",
    "text": "Exercise 1.10.2: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can use cut to extract just the maleID from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week1/w1_exercises.html#solutions",
    "href": "week1/w1_exercises.html#solutions",
    "title": "Week 1 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nIntermezzo 1.1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\ncd -\n\n\n\n\nIntermezzo 1.2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nIntermezzo 1.3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data/\n\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\n\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\n\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t \";\" -k 5 Pacifici2013_data.csv | head -n 1\n\n\n\n\n\n\nBroken pipe\n\n\n\nFollowing the output that you wanted, you may have gotten errors like this:\nsort: write failed: 'standard output': Broken pipe\nsort: write error\nThis may seem disconcerting, but is nothing to worry about, and has to do with the way data streams through a pipe: after head/tail exits because it has done what was asked (print one line), sort or cut may still try to pass on data.\n\n\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n\n\n\n\nExercise 1.10.1: Next-Generation Sequencing Data\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n# Among other output, this should show a file size of 553K\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\n\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\n\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,:\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nmv my_file.tmp my_file.fasta\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\n\ngrep '&gt;' my_file.fasta | head -n 2\n# &gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n# &gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n\nNow, use cut to extract the 4th column:\n\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\n#gene=isogroup00001\n#gene=isogroup00001\n\nFinally, use sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\n\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n# 43\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\nWe need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n# &gt;contig00001,numreads=2\n# &gt;contig00002,numreads=8\n# &gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t ‘=’ to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n# &gt;contig00089,numreads=1\n# &gt;contig00176,numreads=1\n# &gt;contig00210,numreads=1\n# &gt;contig00001,numreads=2\n# &gt;contig00003,numreads=2\nUsing the flag -r we can sort in reverse order:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n# &gt;contig00302,numreads=3330\nFinding that contig 00302 has the highest coverage, with 3330 reads.\n\n\n\n\nExercise 1.10.2: Hormone Levels in Baboons\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s see the structure of the file:\n\nhead -n 3 ../data/Gesquiere2011_data.csv \n# maleID        GC      T\n# 1     66.9    64.57\n# 1     51.09   35.57\n\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\n\ncut -f 1 ../data/Gesquiere2011_data.csv | head -n 3\n# maleID\n# 1\n# 1\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the flag -w as we want to match 3 but not 13 or 23):\n\n# For maleID 3\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 3\n# 61\n\n# For maleID 27\ncut -f 1 ../data/Gesquiere2011_data.csv | grep -c -w 27\n# 5"
  },
  {
    "objectID": "week1/w1_overview_final.html#links",
    "href": "week1/w1_overview_final.html#links",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Course intro\nTue: OSC intro\nThu: Shell basics part I\nThu: Shell basics part II\n\n\n\nExercises\n\nWeek 1 exercises\n\n\n\n\n\n\n\n\nPre-course assignments (if you didn’t do this already)\n\n\n\n\nPre-course survey\nGet access to the Ohio Supercomputer Center (OSC)"
  },
  {
    "objectID": "week1/w1_overview_final.html#content-overview",
    "href": "week1/w1_overview_final.html#content-overview",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an introduction to and overview of the course (Tuesday session) and will learn the basics of working in a Unix shell environment (Thursday session). Some of the things you will learn this week:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nUnix shell basics (Thursday class, Readings, and Exercises)\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nFind and view text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week1/w1_overview_final.html#readings",
    "href": "week1/w1_overview_final.html#readings",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of our primary book, Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book, and behind this course, too.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell. Please read this chapter before Thursday’s class when we will go through the chapter by means of “participatory live-coding” (code-along).\n\nRequired readings\n\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "week1/w1_course-intro.html#todays-objectives",
    "href": "week1/w1_course-intro.html#todays-objectives",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Today’s objectives",
    "text": "Today’s objectives\n\nGet to know each other\nGet an overview of what we will cover in this course\nGet an overview of the course infrastructure\nWe may start with the Unix shell if time allows"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-jelmer",
    "href": "week1/w1_course-intro.html#introductions-jelmer",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Introductions: Jelmer",
    "text": "Introductions: Jelmer\n\nBioinformatician at MCIC in Wooster since June 2020\n\nMost of my time is spent providing research assistance, working with grad students and postdocs on predominantly genomic & transcriptomics data\nAlso teach: some courses, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\n\n\n\nIn my free time, I enjoy bird watching, locally & all across the world"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-you",
    "href": "week1/w1_course-intro.html#introductions-you",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Introductions: You!",
    "text": "Introductions: You!\n\nName\nLab and Department\nResearch interests and/or current topics\nSomething about you that is not work-related"
  },
  {
    "objectID": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "href": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills to:\n\nDo your research more reproducibly and efficiently\nPrepare yourself for working with large “omics” datasets"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility",
    "href": "week1/w1_course-intro.html#course-background-reproducibility",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course background: Reproducibility",
    "text": "Course background: Reproducibility\n\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\nOur focus is on #2."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "href": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course background: Reproducibility (cont.)",
    "text": "Course background: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n\n\n\nAlso important:\n\nProject organization and documentation (week 2)\nSharing data and code (Github for code, week 3)\nHow you code (especially week 10 - Python, and 13 - Snakemake)\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "href": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course background: Efficiency and automation",
    "text": "Course background: Efficiency and automation\n\nUsing code enables you to work efficiently and largely automatically — particularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo everything after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-omics-data",
    "href": "week1/w1_course-intro.html#course-background-omics-data",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course background: Omics data",
    "text": "Course background: Omics data"
  },
  {
    "objectID": "week1/w1_course-intro.html#the-unix-shell-shell-scripts",
    "href": "week1/w1_course-intro.html#the-unix-shell-shell-scripts",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "The Unix shell & shell scripts",
    "text": "The Unix shell & shell scripts\nBeing able to work in the shell is a fundamental skill in computational biology.\n\n\nWeek 1, (2,) and 4: Working in the Unix shell.\nWeek 4: Shell scripting.\nWeek 5: Submitting shell scripts as OSC “batch jobs” with Slurm."
  },
  {
    "objectID": "week1/w1_course-intro.html#working-with-large-data-sets",
    "href": "week1/w1_course-intro.html#working-with-large-data-sets",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Working with large data sets",
    "text": "Working with large data sets"
  },
  {
    "objectID": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "href": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Project organization and Markdown (Wk 2)",
    "text": "Project organization and Markdown (Wk 2)\nGood project organization & documentation is a necessary starting point for reproducible research.\n\n\nBest practices for project organization, file naming, etc., and how to use the Unix shell for this.\nTo document and report what we are doing: Markdown."
  },
  {
    "objectID": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "href": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Version control with Git and GitHub (Wk 3)",
    "text": "Version control with Git and GitHub (Wk 3)\nUsing version control, you can more effectively keep track of project progress, collaborate, experiment, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories)."
  },
  {
    "objectID": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "href": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Automated workflow management (Wk 6)",
    "text": "Automated workflow management (Wk 6)\nTo be able to run an entire analysis pipeline with a single command,\nand easily change and rerun parts of it, we need a workflow manager.\n\n\nWe’ll use Nextflow."
  },
  {
    "objectID": "week1/w1_course-intro.html#zoom-practicalities",
    "href": "week1/w1_course-intro.html#zoom-practicalities",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Zoom practicalities",
    "text": "Zoom practicalities\n\nBe muted by default, but feel free to unmute yourself to ask questions.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week1/w1_course-intro.html#carmencanvas-website",
    "href": "week1/w1_course-intro.html#carmencanvas-website",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "CarmenCanvas website",
    "text": "CarmenCanvas website\nI will only use this for Announcements."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-infrastructure-github-website",
    "href": "week1/w1_course-intro.html#course-infrastructure-github-website",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course infrastructure: GitHub website",
    "text": "Course infrastructure: GitHub website\nThis is the main website for the course, containing all the material:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nExercises\nFinal project assignment information\n\n\n\n\n\n\n\nWeekly materials\n\n\nI will try add the materials for each week on the preceding Friday – at the least the week’s overview and readings."
  },
  {
    "objectID": "week1/w1_course-intro.html#ohio-supercomputer-center-osc",
    "href": "week1/w1_course-intro.html#ohio-supercomputer-center-osc",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Ohio Supercomputer Center (OSC)",
    "text": "Ohio Supercomputer Center (OSC)\n\nAll of our work can be done at OSC, in a browser (!)\n\nAvoids local installs, different behavior for different OS’s, etc."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-infrastructure-github",
    "href": "week1/w1_course-intro.html#course-infrastructure-github",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Course infrastructure: Github",
    "text": "Course infrastructure: Github\n\nYou’ll be submitting your assignments through GitHub.\nThis will double as practice with version control and sharing of scripts."
  },
  {
    "objectID": "week1/w1_course-intro.html#final-project-graded",
    "href": "week1/w1_course-intro.html#final-project-graded",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Final project (graded)",
    "text": "Final project (graded)\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 4, 10 points)\nII: Draft (due week 6, 10 points)\nIII: Presentations on Zoom (week 7, 10 points)\nIV: Final submission (due April 29, 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIf you have your own data set & analysis ideas, that is ideal. If not, I can provide you with this.\nMore information about the final project will follow in week 2 or 3."
  },
  {
    "objectID": "week1/w1_course-intro.html#ungraded-homework",
    "href": "week1/w1_course-intro.html#ungraded-homework",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings\nWeekly exercises\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nNone of these have to be handed in."
  },
  {
    "objectID": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "href": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nIf there is interest, we can have a weekly Monday meeting in which we go through the exercises for the preceding week.\n\nIf you’re interested, indicate your availability here:\nhttps://www.when2meet.com/?23841132-KV8fY"
  },
  {
    "objectID": "week1/w1_course-intro.html#rest-of-this-week",
    "href": "week1/w1_course-intro.html#rest-of-this-week",
    "title": "Week 1 - Lecture 1: Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nUnix shell basics (code-along of CSB Chapter 1)\nHomework – from CSB Chapter 1:\n\nReadings\nExercises"
  },
  {
    "objectID": "week1/w1_shell1.html#goals-for-this-session",
    "href": "week1/w1_shell1.html#goals-for-this-session",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nIn this session, we’ll cover much of the material from CSB Chapter 1, to learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files.\n\n\n\n\n\n\n\n\nTip\n\n\n\nSee the “Topic Overview” page on the Unix shell for an overview of Unix shell commands we’ll cover during this course."
  },
  {
    "objectID": "week1/w1_shell1.html#introduction-ch.-1.1-1.2",
    "href": "week1/w1_shell1.html#introduction-ch.-1.1-1.2",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "2 Introduction (Ch. 1.1-1.2)",
    "text": "2 Introduction (Ch. 1.1-1.2)\n\n2.1 Some terminology\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “*nix”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\nRecall from the previous session that OSC runs on Linux — as do the vast majority of “servers” worldwide.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface2 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably.\n\n\n\n2.2 Why use the Unix shell?\n\nVersus programs with graphical user interfaces:\n\nUsing software\nBest or only option to use many programs, especially in bioinformatics.\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nThe shell keeps a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data\nRemote computing – especially HPCs\nOften not possible to do anything outside of the terminal.\n\nVersus scripting languages like Python or R:\n\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nThe Unix shell has a direct interface to other programs."
  },
  {
    "objectID": "week1/w1_shell1.html#accessing-a-shell-at-osc",
    "href": "week1/w1_shell1.html#accessing-a-shell-at-osc",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "3 Accessing a shell at OSC",
    "text": "3 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nTo paste, use Ctrl+V.\nTo copy, simply select text and you should see a  icon.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if so, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):"
  },
  {
    "objectID": "week1/w1_shell1.html#getting-started-with-unix-ch.-1.3",
    "href": "week1/w1_shell1.html#getting-started-with-unix-ch.-1.3",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "4 Getting started with Unix (Ch. 1.3)",
    "text": "4 Getting started with Unix (Ch. 1.3)\n\nUnix directory structure\n\n\n\n\n\nGeneric example, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in OSC dir structure\n\n\n\n\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nFor example, the path to our OSC project’s main/project dir is /fs/ess/PAS2700. This means: the dir PAS2700 is located inside the dir ess, which in turn is located inside the dir fs, which in turn is located in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. For example, our Home dir is not /home/&lt;username&gt; like in the book, but /users/&lt;some-project&gt;/&lt;username&gt;, as we’ll see below."
  },
  {
    "objectID": "week1/w1_shell1.html#getting-started-with-the-shell-ch.-1.4",
    "href": "week1/w1_shell1.html#getting-started-with-the-shell-ch.-1.4",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "5 Getting started with the shell (Ch. 1.4)",
    "text": "5 Getting started with the shell (Ch. 1.4)\n\n5.1 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n5.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nWed Feb 7 09:11:51 EST 2024\nThe whoami (who-am-i) command prints your username:\nwhoami\njelmer\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nAll 3 of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n5.3 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\n\n\n\n\nA more complex type of option\n\n\n\nWhen we get to commands like cut later in this session, we’ll also see options that take an argument: e.g., the -f option allows you to select a column, where -f 3 will select the 3rd column.\n\n\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this general pattern — when you give it one argument, this is supposed to be the year it should show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSyntax-wise, arguments:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options3.\n\n\n\n\n\n5.4 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\n\n\n\n5.5 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2700\npwd\n/fs/ess/PAS2700\nSo:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2700\n-bash: cd: /fs/ess/PAs2700: No such file or directory\n\n\nWhat was the problem with the path we specified? (Click to see the answer)\n\nWe used a lowercase S in /PAs2700/ — this should have been /PAS2700/. Everything, including paths, is case-sensitive in the Unix shell!\n\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names.\nAvoid spaces in file and dir names (we’ll talk more about this next week).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n5.6 Keyboard shortcuts\nUsing keyboard shortcuts help you to work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Tab completion & command history practice\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2700).\nComplete it to /fs/ess/PAS2700 and Enter. What does the error mean?\nbash: /fs/ess/PAS2700/: Is a directory\n\n\n\nClick to see the solution\n\nEverything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you may have expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line.\nAdd cd and a space in front of the dir and press Enter again.\ncd /fs/ess/PAS2700\n\n\n\n Cancelling practice\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n5.7 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\n\nAssigning and printing the value of a variable in the Unix shell:\n\nx=5\necho $x\n5\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell4.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n5.8 Create your own dir & get the CSB data\nOur base OSC directory for the course is the /fs/ess/PAS2700 we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n5.9 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nPaths with a leading / begin from the computer’s root directory, and are called “absolute” or “full paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nPaths without a leading / begin from your current working directory, and are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nAbove, we just used an absolute path (cd /fs/ess/PAS2700/users/$USER) and a relative path (cd CSB/unix/sandbox) to change directories.\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2700/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2700/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week1/w1_shell1.html#footnotes",
    "href": "week1/w1_shell1.html#footnotes",
    "title": "Week 1: Unix Shell Basics part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\n Though some commands are flexible and accept either order.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎"
  },
  {
    "objectID": "week6/w6_overview.html",
    "href": "week6/w6_overview.html",
    "title": "Week 6",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/shell.html#basic-commands",
    "href": "ref/shell.html#basic-commands",
    "title": "Topic overview: Unix shell",
    "section": "1 Basic commands",
    "text": "1 Basic commands\n\n\n\nCommand\nDescription\nExamples / options\n\n\n\n\npwd\nPrint current working directory (dir).\npwd\n\n\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\n\n\ncd\nChange working dir. As with all commands, you can use an absolute path (starting from the root dir /) or a relative path (starting from the current working dir).\ncd /fs/ess/PAS1855 (With absolute path)  cd ../.. (Two levels up)  cd - (To previous dir)\n\n\ncp\nCopy files or, with -r, dirs and their contents (i.e., recursively).  If target is a dir, file will keep same name; otherwise, a new name can be provided.\ncp *.fq data/ (All .fq files into dir data)  cp my.fq data/new.fq (With new name)  cp -r data/ ~ (Copy dir and contents to home dir) \n\n\nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name)  mv my.fq my.fastq (Simple rename)  mv file1 file2 mydir/ (Last arg is destination)\n\n\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files)  rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\n\n\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir  mkdir -p new1/new2/new3\n\n\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\n\n\ncat\nPrint file contents to standard out (screen).\ncat my.txt  cat *.fa &gt; concat.fq (Concatenate files)\n\n\nhead\nPrint the first 10 lines of a file or specify number with -n &lt;n&gt; or shorthand -&lt;n&gt;.\nhead -n 40 my.fq (print 40 lines)  head -40 my.fq (equivalent)\n\n\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (“trick” to skip first line)  tail -f slurm.out (“follow” file)\n\n\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\n\n\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file:  column -s \",\" -t my.csv\n\n\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\n\n\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x).  ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable)  chmod a=r data/raw/* (Make data read-only)      -R recursive  ddddddddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#data-tools",
    "href": "ref/shell.html#data-tools",
    "title": "Topic overview: Unix shell",
    "section": "2 Data tools",
    "text": "2 Data tools\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples and options\n\n\n\n\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\n\n\ncut\nSelect one or more columns from a file.\nSelect columns 1-4:  cut -f 1-4 my.csv      -d \",\" comma as delimiter\n\n\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically,  column 2 reverse numerically:  sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\n\n\nuniq\nRemove consecutive duplicate lines (often from single-column selection): i.e., removes all duplicates if input is sorted.\nUnique values for column 2:  cut -f2 my.tsv | sort | uniq\n\n\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3:  cut -f3 my.tsv | sort | uniq -c\n\n\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV:  cat my.csv | tr \"\\t\" \",\"  Uppercase to lowercase: tr A-Z a-z &lt; in.txt &gt; out.txt      -d delete      -s squeeze\n\n\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines before match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC:  grep \"A[AG]C\" my.fa  Omit comment lines:  grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only"
  },
  {
    "objectID": "ref/shell.html#miscellaneous",
    "href": "ref/shell.html#miscellaneous",
    "title": "Topic overview: Unix shell",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nexample\n\n\n\n\n/\nRoot directory.\ncd /\n\n\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir)  Use ./ to execute script if not in $PATH:  ./myscript.sh\n\n\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n\n\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n\n\n$USER\nUser name.\nmkdir $USER\n\n\n&gt;\nRedirect standard out to a file.\necho \"My 1st line\" &gt; myfile.txt\n\n\n&gt;&gt;\nAppend standard out to a file.\necho \"My 2nd line\" &gt;&gt; myfile.txt\n\n\n2&gt;\nRedirect standard error to a file.\nSend standard out and standard error for a script to separate files:  myscript.sh &gt;log.txt 2&gt; err.txt\n\n\n&&gt;\nRedirect standard out and standard error to a file.\nmyscript.sh &&gt; log.txt\n\n\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nThe output of the sort command will be piped into head to show the first lines:  sort myfile.txt | head\n\n\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 =&gt; 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31)  touch fig1{A..F} (fig1A, fig1B, …, fig1F)  mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n\n\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files:  echo \"I see $(ls *fastq | wc -l) files\"  Substitute with date in YYYY-MM-DD format:  mkdir results_$(date +%F)  nlines=$(wc -l &lt; $infile)\n\n\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name.  ddddddddddddddddddddddddddddddddddddd\nAdd dir to path:  PATH=$PATH:/new/dir  (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#shell-wildcards",
    "href": "ref/shell.html#shell-wildcards",
    "title": "Topic overview: Unix shell",
    "section": "4 Shell wildcards",
    "text": "4 Shell wildcards\n\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n\n*\nAny number of any character, including nothing.\nls data/*fastq.gz (Matches any file ending in “fastq.gz”)  ls *R1* (Matches any file containing “R1” somewhere in the name.)\n\n\n?\nAny single character.\nls sample1_?.fastq.gz (Matches sample1_A.fastq.gz but not sample1_AA.fastq.gz)\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets.  ddddddddddddddddddddddddddddddddddddd\nls fig1[A-C] (Matches fig1A, fig1B, fig1C)  ls fig[0-3] (Matches fig0, fig1, fig2, fig3)  ls fig[^4]* (Does not match files with a “4” after “fig”)  ddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#regular-expressions",
    "href": "ref/shell.html#regular-expressions",
    "title": "Topic overview: Unix shell",
    "section": "5 Regular expressions",
    "text": "5 Regular expressions\n\n\n\n\n\n\n“ERE” = GNU Extended regular expressions\n\n\n\nWhere it says “yes” in the ERE column, the symbol in questions needs to have ERE turned on in order to work1: use a -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE.\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nERE2\nMatches\nExample\n\n\n\n\n.\n\nAny single character\nMatch Olfr with none or any characters after it:  grep -o \"Olfr.*\"\n\n\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n\n\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits:  grep -E [0-9]+\n\n\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit:  grep -E [0-9]?\n\n\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs:  grep -E \"G{50,100}\"\n\n\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines:  grep -v \"^$\"  Exclude lines beginning with a “#”:  grep -v \"^#\"\n\n\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\n\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\n\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed.\nMatch gene_id followed by a space and a “word”:  grep -E -o 'gene_id \"\\w+\"'  Change any word character to X:  sed s/\\w/X/\n\n\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either intron or exon:  grep -E \"intron|exon\"\n\n\n()\nyes\nGrouping\nFind “AAG” repeated 10 times:  grep (AAG){10}\n\n\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc.  ddddddddddddddddddddddddddddddddddddd\nInvert order of two words:  sed -E 's/(\\w+) (\\w+)/\\2 \\1/'  ddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#more-details-for-a-few-commands",
    "href": "ref/shell.html#more-details-for-a-few-commands",
    "title": "Topic overview: Unix shell",
    "section": "6 More details for a few commands",
    "text": "6 More details for a few commands\n\n6.1 less\n\n\n\n\n\n\n\nKey\nFunction\n\n\n\n\nq\nExit less\n\n\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\n\n\nd / u\nGo down / up half a page.\n\n\ng / G\nGo to the first / last line (home / end also work).\n\n\n/&lt;pattern&gt; or ?&lt;pattern&gt;\nSearch for &lt;pattern&gt; forwards / backwards: type your search after / or ?.\n\n\nn / N\nWhen searching, go to next / previous search match.\ndddddddddddddddddddddddddddddddddddddddddddddddddddd\n\n\n\n\n\n\n6.2 sed\n\nsed flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n-E\nUse extended regular expressions\n\n\n-e\nWhen using multiple expressions, precede each with -e\n\n\n-i\nEdit a file in place\n\n\n-n\nDon’t print lines unless specified with p modifier\n\n\n\n\n\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (&gt;1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#&gt; chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#&gt; words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#&gt; sampleA.fastq\n\n\n\n\n6.3 awk\n\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\n\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 &lt; 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\n\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 &gt; 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\n\n\nawk comparison and logical operators\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\na == b\na is equal to b\n\n\na != b\na is not equal to b\n\n\na &lt; b\na is less than b\n\n\na &gt; b\na is greater than b\n\n\na &lt;= b\na is less than or equal to b\n\n\na &gt;= b\na is greater than or equal to b\n\n\na ~ /b/\na matches regular expression pattern b\n\n\na !~ /b/\na does not match regular expression pattern b\n\n\na && b\nlogical and: a and b\n\n\na || b\nlogical or: a or b [note typo in Buffalo]\n\n\n!a\nnot a (logical negation)\n\n\n\n\n\nawk special variables and keywords\n\n\n\n\n\n\n\nkeyword/variable\nmeaning\n\n\n\n\nBEGIN\nUsed as a pattern that matches the start of the file\n\n\nEND\nUsed as a pattern that matches the end of the file\n\n\nNR\nNumber of Records (running count; in END: total nr. of lines)\n\n\nNF\nNumber of Fields (for each record)\n\n\n$0\nContains entire record (usually a line)\n\n\n$1 - $n\nContains one column each\n\n\nFS\nInput Field Separator (default: any whitespace)\n\n\nOFS\nOutput Field Separator (default: single space)\n\n\nRS\nInput Record Separator (default: newline)\n\n\nORS\nOutput Record Separator (default: newline)\n\n\n\n\n\nSome awk functions\n\n\n\n\n\n\n\nFunction\nMeaning\n\n\n\n\nlength(&lt;string&gt;)\nReturn number of characters\n\n\ntolower(&lt;string&gt;)\nConvert to lowercase\n\n\ntoupper(&lt;string&gt;)\nConvert to uppercase\n\n\nsubstr(&lt;string&gt;, &lt;start&gt;, &lt;end&gt;)\nReturn substring\n\n\nsub(&lt;from&gt;, &lt;to&gt;, &lt;string&gt;)\nSubstitute (replace) regex\n\n\ngsub(&lt;from&gt;, &lt;to&gt; &lt;string&gt;)\n&gt;1 substitution per line\n\n\nprint\nPrint, e.g. column: print $1\n\n\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\n\n\nnext\nDon’t process later fields: to next iteration"
  },
  {
    "objectID": "ref/shell.html#keyboard-shortcuts",
    "href": "ref/shell.html#keyboard-shortcuts",
    "title": "Topic overview: Unix shell",
    "section": "7 Keyboard shortcuts",
    "text": "7 Keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl+Shift+C\nCopy selected text\n\n\nCtrl+Shift+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line3\n\n\nCtrl+W\nCut word before before cursor4\n\n\nCtrl+Y\nPaste (“yank”)\n\n\nAlt+.\nLast argument of previous command (very useful!)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nKill (stop) currently active command\n\n\nCtrl+D\nExit (a program or the shell depending on the context)\n\n\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background."
  },
  {
    "objectID": "ref/shell.html#footnotes",
    "href": "ref/shell.html#footnotes",
    "title": "Topic overview: Unix shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen using the default regular expressions in grep and sed, Basic Regular Expressions (BRE), the symbol would need to be preceded by a backslash to work.↩︎\nGNU Extended Regular Expressions↩︎\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.↩︎"
  },
  {
    "objectID": "ref/further-resources.html",
    "href": "ref/further-resources.html",
    "title": "Further Resources",
    "section": "",
    "text": "An extended version of this introduction\nOSC’s online asynchronous courses\nOSC’s new User Resource Guide 1"
  },
  {
    "objectID": "ref/further-resources.html#footnotes",
    "href": "ref/further-resources.html#footnotes",
    "title": "Further Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC.↩︎"
  },
  {
    "objectID": "ref/glossary.html",
    "href": "ref/glossary.html",
    "title": "Glossary of common terms in this course",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\nBuffalo\nCourse book: Bioinformatics Data Skills (Buffalo 2015).\n\n\nCLI\nCommand-line Interface — a software interface in which one types commands (cf. “GUI”).\n\n\nCSB\nCourse book: Computing Skills for Biologists (Allesina & Wilmes 2019).\n\n\ncluster\nAnother word for supercomputer, a set of interconnected computers for high-performance computing\n\n\ndir\nShort for “directory”, which how a folder is often referred to in the Unix shell and Linux.\n\n\nGit\nSoftware for “version control”, a system to track changes in code and other text files, and collaborate on those.\n\n\nGitHub\nA website that hosts Git projects, which are known as repositories.\n\n\nGUI\nGraphical User Interface – a visual software interface with which one interacts by clicking (cf. “CLI”).\n\n\nHPC\nHigh-Performance Computing, for instance as can be done using a supercomputer.\n\n\nLinux\nThe operating system that OSC runs on. Free Linux distributions include Ubuntu.\n\n\nMarkdown\nA simple text markup language (think LaTeX or HTML but much simpler).\n\n\nOSC\nThe Ohio Supercomputer Center.\n\n\nshell\nThe Unix shell is a command-line interface to the operating system that runs within a terminal. There are several shell flavors, and in this course, we will be working with the bash shell.\n\n\nNextflow\nSoftware for automated analysis workflow (pipeline) management.\n\n\nSlurm\nSoftware that schedules “compute jobs” (access to the main parts of the system) at OSC.\n\n\nUnix\nA family of operating systems that includes Mac and Linux, but not Windows.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week6/w6_exercises.html",
    "href": "week6/w6_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week1/w1_shell2.html#basic-unix-commands-ch.-1.5",
    "href": "week1/w1_shell2.html#basic-unix-commands-ch.-1.5",
    "title": "Week 1: Unix Shell Basics part II",
    "section": "1 Basic Unix commands (Ch. 1.5)",
    "text": "1 Basic Unix commands (Ch. 1.5)\n\n1.1 ls to list files\nThe ls command, short for “list”, will list files and directories:\n# (You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nBy default, ls will list files and dirs in your current working dir in the way shown above. For which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nNotice that it lists the same items as above, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n1.2 cp to copy files\nThe cp command copies files and/or directories from one location to another. It has two required arguments: what you want to copy (the source), and where you want to copy it to (the destination). Its basic syntax can be summarized as cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive by default”. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck what you now have in your sandbox dir:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n1.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n1.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm buzz_copy.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nIf you use a trailing slash in the destination, you are making explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n1.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example1:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "week1/w1_shell2.html#advanced-unix-commands-ch.-1.6",
    "href": "week1/w1_shell2.html#advanced-unix-commands-ch.-1.6",
    "title": "Week 1: Unix Shell Basics part II",
    "section": "2 Advanced Unix commands (Ch. 1.6)",
    "text": "2 Advanced Unix commands (Ch. 1.6)\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n2.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n2.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument like in the earlier example.\n\n\n\n\n\n\nWhy pipes are useful\n\n\n\nPipes avoid the need to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n\n\n2.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files\n\n\n\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text, for example.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\nThe cut command will select/cut out one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\nUnfortunately, we can’t reorder columns using cut. (The awk command can do this.)\n\n\n\n\n\n\n\n\n\n2.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n2.5 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nDeletion and “squeezing”:\n# Delete all a's\necho \"aabbccddee\" | tr -d a\nbbccddee\n# Remove consecutive duplicates a's\necho \"aabbccddee\" | tr -s a\nabbccddee\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat inputfile.csv | tr \",\" \"\\t\" &gt; outputfile.tsv\nIn the example above, we converted a CSV file to a TSV file, where \\t is a regular expression meaning Tab (we’ll learn more about these in Week 4).\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n\n\n2.6 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern2\nIncomplete matches work: “Vombat” matches Vombatidae\n\n\n\ngrep has many other useful options — more in Week 4:\n\n-i to ignore case\n-r to search files recursively\n-B and -A to print lines surrounding matches\n-w to match “words”"
  },
  {
    "objectID": "week1/w1_shell2.html#wrap-up-the-unix-philosophy",
    "href": "week1/w1_shell2.html#wrap-up-the-unix-philosophy",
    "title": "Week 1: Unix Shell Basics part II",
    "section": "3 Wrap-up & the Unix philosophy",
    "text": "3 Wrap-up & the Unix philosophy\n\n3.1 Covered in the chapter but not in today’s lecture\n\nThe less pager to view files\nThe find command to find files\nShowing and changing file permissions\nBasic shell scripting\nfor loops\n$PATH and bash profile settings\n\nWe’ll cover all of these in class over the next few weeks.\n\n\n\n3.2 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n3.3 The Unix shell in the weeks ahead\n\nIn all course weeks, we will be working in the Unix shell, though our focus in several cases will be on a specific tool, such as Git in week 3.\nIn week 4, we’ll fully focus on the shell and shell scripting."
  },
  {
    "objectID": "week1/w1_shell2.html#footnotes",
    "href": "week1/w1_shell2.html#footnotes",
    "title": "Week 1: Unix Shell Basics part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expresssions, this is not strictly necessary, it’s good habit to always quote.↩︎"
  },
  {
    "objectID": "week1/w1_osc.html#goals-for-this-session",
    "href": "week1/w1_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide a introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically.\nThis is only meant as a brief introductory overview to give some context about the working environment that we will start using. During the course, you’ll learn a lot more about most topics touched on in this page — week 5 in particular focuses on OSC."
  },
  {
    "objectID": "week1/w1_osc.html#high-performance-computing",
    "href": "week1/w1_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs.\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\nOSC websites and “projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2700.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread\nComponents of a computer (node) that can each (semi-)indedepently be asked to perform a computing task like running a bioinformatics program. For our purposes, we can treat these terms as synonyms (!).\nNode\nA single computer that is a part of a supercomputer and has dozens of cores2.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n\n\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two clusters!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course’s OSC Project PAS2700: /fs/ess/PAS2700. (We’ll talk more about these different file systems in week 5.)\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, and they are shared among everyone and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use in week 5, will then assign resources to your request.\n\n\n\n\n\n\nInteractive and batch use of compute nodes\n\n\n\n\n\nRequests for compute node jobs can be made through the OnDemand website or with Slurm commands like srun and sbatch.\nJobs can either be interactive (like running Rstudio or interactive shell jobs) or be a “batch” job (sending a script away to be run on a compute node). Only with interactive jobs do you “move” to a compute node yourself.\n\n\n\nCompute nodes come in different shapes and sizes. You mostly don’t have to worry about this but sometimes non-standard nodes are needed, such as when you need a lot of RAM memory or need GPUs3."
  },
  {
    "objectID": "week1/w1_osc.html#osc-ondemand",
    "href": "week1/w1_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the box on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2700, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2700 project’s “scratch” directory (/fs/scratch/PAS2700)\nThe PAS2700 project’s “project” directory (/fs/ess/PAS2700)\n\nYou will only every have one Home directory, but for every additional project you are a member of, you should usually see additional /fs/ess and a /fs/scratch directories.\n Click on our focal directory /fs/ess/PAS2700.\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files4 — see the buttons across the top.\n\n\n\n4.2 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell in the Thursday session.\n\n\n\n4.3 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server."
  },
  {
    "objectID": "week1/w1_osc.html#what-works-differently-on-a-supercomputer-like-at-osc",
    "href": "week1/w1_osc.html#what-works-differently-on-a-supercomputer-like-at-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "5 What works differently on a supercomputer like at OSC?",
    "text": "5 What works differently on a supercomputer like at OSC?\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\nLogin versus compute nodes\n“Login nodes”, the nodes you end up on after logging in, are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses.\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system."
  },
  {
    "objectID": "week1/w1_osc.html#footnotes",
    "href": "week1/w1_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anthing for educational projects like this one. Otherwise, if you’re interested in OSC’s rates for academic research, see this page.↩︎\nI.e., these nodes tend to be more powerful than a personal laptop or desktop.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "week1/w1_overview.html#links",
    "href": "week1/w1_overview.html#links",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Course intro (link TBA)\nTue: OSC intro (link TBA)\nThu: Shell basics part I (link TBA)\nThu: Shell basics part II (link TBA)\n\n\n\nExercises\n\nWeek 1 exercises (link TBA)\n\n\n\n\n\n\n\n\nPre-course assignments (if you didn’t do this already)\n\n\n\n\nPre-course survey\nGet access to the Ohio Supercomputer Center (OSC)"
  },
  {
    "objectID": "week1/w1_overview.html#content-overview",
    "href": "week1/w1_overview.html#content-overview",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an overview of the course and a brief intro to the Ohio Supercomputer (OSC) during the Tuesday meeting, and will be taught the basics of working in a Unix shell environment during the Thursday meeting.\nMore specifically, some of the things you will learn this week:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nOSC Intro (Tuesday class)\n\nWhat is a supercomputer and why is it useful?\nOverview of the resources the Ohio Supercomputer Center (OSC) provides.\nHow to use OSC OnDemand and access a Unix Shell in your browser.\n\n\n\nUnix shell basics (Thursday class, Readings, and Exercises)\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing Unix commands, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week1/w1_overview.html#readings",
    "href": "week1/w1_overview.html#readings",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of the book Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book as well as this course.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell. I would recommend to read this this at least by Thursday’s class, when we will go through much of the chapter’s content.\n\nRequired readings\n\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "week1/removed.html",
    "href": "week1/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week1/removed.html#bonus-material",
    "href": "week1/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week0/osc-setup.html",
    "href": "week0/osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore the course starts, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2700).\n\n\nBackground\nMuch of the coding during this course will be done through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC Project, and membership of this specific project will allow you to access our shared files and reserve “compute nodes”.\n\n\nWhat you should do\nAfter completing the pre-course survey, you will receive an invitation email from OSC referencing the course project number PAS2700:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 6193 Spring 2024, second session\n\n\n\n\nWeek\nTue\nThu\nWeek overview  & exercises\nDue forfinal project\n\n\n\n\n\n(Pre-course work)\n\n\n\n\n\n1\nFeb 27: Course Intro & OSC Intro\nFeb 29: Shell basics Part I & Part II\n   \n\n\n\n2\nMar 5: Project organization\nMar 7: Markdown\n   \n\n\n\n\n\n\n\n\n\n\n3\nMar 19: Version Control I\nMar 21: Version Control II\n   \n\n\n\n4\nMar 26: Shell data tools\nMar 28: CLI software & shell scripting\n   \nProposal (3/25)\n\n\n5\nApr 2: Data & software management\nApr 4: OSC Slurm batch jobs\n   \n\n\n\n6\nApr 9: Nextflow I\nApr 11: Nextflow II\n   \nDraft (4/8)\n\n\n7\nApr 16: Recap\nApr 18: Student presentations\n\nSubmission (4/29)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_overview.html",
    "href": "week2/w2_overview.html",
    "title": "Week 2",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_exercises.html",
    "href": "week2/w2_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week5/w5_exercises.html",
    "href": "week5/w5_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/w1_removed.html",
    "href": "week4/w1_removed.html",
    "title": "Removed w1 exercises",
    "section": "",
    "text": "0.1 1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nShow hints\n\n\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n\n\n\n\n\n0.2 1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\nbash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\n\n\nShow hints\n\nTo build the script, you need to combine several commands:\n\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n\n\n\n\n\n0.3 1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\n\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\n\nFor example, running the script as below should produce the following output:\nbash explore.sh ../data/Buzzard2015_data.csv 7\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\n\n\nShow hints\n\n\nYou can select a given column from a csv file using the command cut. Then:\n\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\n\n\n\n\n0.4 Solutions\n\n\n0.5 1.10.2 Hormone Levels in Baboons\n\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\n\ncut -f 1 $1 | grep -c -w $2\n\nA slightly more verbose and readable example:\n\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\n\n\n\n\nNote\n\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\n\nTo run the script, assuming it is named count_baboons.sh:\n\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n\n\n\n0.6 1.10.3 Plant–Pollinator Networks\n\n\nSolution\n\n\nCounting rows:\n\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\n\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\n\nTo avoid printing the file name, we can either use cat or input redirection:\n\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l &lt; ../data/Saavedra2013/n10.txt \nCounting rows:\n\nCounting the number of columns is more work. First, we need only the first line:\n\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n\nNow we can remove all spaces and the line terminator using tr:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\n\nFinally, we can use wc -c to count the number of characters in the string:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\n\nTo run the script, assuming it is named counter.sh:\n\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\n\n\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n\n\n\n\n\n\n\n0.7 1.10.4 Data Explorer\n\n\nSolution\n\n\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\n\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\n\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\n\nHere is an example of what the script could look like:\n\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week3/w3_exercises.html",
    "href": "week3/w3_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  }
]