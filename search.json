[
  {
    "objectID": "week3/w3_overview.html#links",
    "href": "week3/w3_overview.html#links",
    "title": "Week 3: Git & GitHub",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – Getting started with Git.\nThursday – Remotes on GitHub.\nBonus (optional self-study) content: Branching, collaborating, and undoing.\n\n\n\nExercises & assignments\n\nMake sure you have done the assignment from week 2 by Tuesday: Create a GitHub account.\nExercises\n\n\n\n\n\n\n\n\nFinal project\n\n\n\nYou should start thinking about your final project for this course — recall that this is the only part of the course that is graded.\n\nTake a look at the page with general info on the final project.\nYour proposal (plan) is due on Monday, April 1st (week 5).\nI will also point this out on Tue, and we can talk about this in class if needed on Thursday."
  },
  {
    "objectID": "week3/w3_overview.html#content-overview",
    "href": "week3/w3_overview.html#content-overview",
    "title": "Week 3: Git & GitHub",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will learn about the why and how of using Git version control for your projects, and sharing your code on GitHub.\nBe aware that Git is a challenging topic. Therefore, if you can, complete the main reading before Tuesday’s lecture, and also read the Buffalo chapter at some point this week.\nA good way to get used to Git is to make dummy repositories where you’re just editing one or a few simple text files with dummy lines of text. That way, you can get used to the basic workflow, and freely experiment also with commands to undo things and move back in time. We’ll do this in our Zoom meetings and I recommend you do it outside of there, too.\nSome of the things you will learn this week:\n\nUnderstand why you should use a formal Version Control System (VCS) for research projects.\nLearn the basics of the most widely used VCS: Git.\nLearn how to put your local repositories online at GitHub, and how to keep local and online (“remote”) repositories in sync.\nLearn about single-user and multi-user workflows with Git and GitHub.\n\nOptional self-study content:\n\nLearn how to use Git branches to safely make experimental changes.\nLearn how to undo things and “travel back in time” for your project using Git."
  },
  {
    "objectID": "week3/w3_overview.html#readings",
    "href": "week3/w3_overview.html#readings",
    "title": "Week 3: Git & GitHub",
    "section": "3 Readings",
    "text": "3 Readings\nThis week’s main reading is the CSB chapter on Git, chapter 2. We will also roughly work our way through this chapter in the Zoom sessions.\nThe optional reading is the Buffalo chapter on Git, chapter 5. Like the CSB chapter, this starts with the very basics of Git; but it goes a bit further.\nThere are also some useful further resources mentioned below.\n\nRequired readings\n\nCSB Chapter 2: “Version Control” up until section 2.5 (the rest of the chapter is optional).\n\n\n\nOptional readings\n\nBuffalo Chapter 5: “Git for Scientists”.\n\n\n\nFurther resources\n\nGitHub has a nice little overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\nFor some more background on why to use version control, and another perspective on some Git basics, I recommend the article “Excuse me, do you have a moment to talk about version control?” by Jenny Bryan.\nEspecially if you work with R a lot, I would recommend checking out Happy Git and GitHub for the useR, also by Jenny Bryan. This is a very accessible introduction to Git.\nGit-it is a small application to learn and practice Git and GitHub basics.\nIf you want to try some online exercises with Git with helpful visuals of what Git commands do, try https://learngitbranching.js.org/. (But be aware that this does fairly quickly move to fairly advanced topics, including several that we will not touch on in the course.)\nA good list of even more Git resources…"
  },
  {
    "objectID": "week3/w3_git3.html",
    "href": "week3/w3_git3.html",
    "title": "Branching, collaborating, and undoing",
    "section": "",
    "text": "This page contains optional self-study material if you want to dig deeper into Git. Some of it may also be useful as a reference in case you run into problems while trying to use Git."
  },
  {
    "objectID": "week3/w3_git3.html#branching-merging",
    "href": "week3/w3_git3.html#branching-merging",
    "title": "Branching, collaborating, and undoing",
    "section": "1 Branching & merging",
    "text": "1 Branching & merging\nIn this section, you’ll learn about using so-called “branches” in Git. Branches are basically parallel versions of your repository, which allow you or your collaborators to experiment or create variants without affecting existing functionality or others’ work.\n\n\n1.1 A repo with a couple of commits\nFirst, you’ll create a dummy repo with a few commits by running a script (following CSB).\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\nTake a look at the script you will run to create your repo:\ncat ../data/create_repository.sh\n#!/bin/bash\n\n# function of the script:\n# sets up a repository and\n# immitates workflow of\n# creating and commiting two text files\n\nmkdir branching_example\ncd branching_example\ngit init\necho \"Some great code here\" &gt; code.txt\ngit add .\ngit commit -m \"Code ready\"\necho \"If everything would be that easy!\" &gt; manuscript.txt \ngit add .\ngit commit -m \"Drafted paper\"\nRun the script:\nbash ../data/create_repository.sh\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/CSB/git/sandbox/branching_example/.git/\n[main (root-commit) 3c59d8a] Code ready\n 1 file changed, 1 insertion(+)\n create mode 100644 code.txt\n[main 7ba8ca4] Drafted paper\n 1 file changed, 1 insertion(+)\n create mode 100644 manuscript.txt\nAnd move into the repository’s dir:\ncd branching_example\nLet’s see what has been done in this repo:\ngit log --oneline\n7ba8ca4 (HEAD -&gt; main) Drafted paper\n3c59d8a Code ready\nWe will later modify the file code.txt — let’s see what it contains now:\ncat code.txt\nSome great code here\n\n\n\n1.2 Using branches in Git\nYou now want to improve the code, but these changes are experimental, and you want to retain your previous version that you know works. This is where branching comes in. With a new branch, you can make changes that don’t affect the main branch, and can also keep working on the main branch:\n\n\n\nFigure modified after Allesino & Wilmes (2019).(Note that the main branch is here called “master”.)\n\n\n\nCreating a new branch\nFirst, create a new branch as follows, naming it fastercode:\ngit branch fastercode\nList the branches:\n# Without args, git branch will list the branches\ngit branch\n  fastercode\n* main\nIt turns out that you created a new branch but are still on the main branch, as the * indicates.\nYou can switch branches with git checkout:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nAnd confirm your switch with git branch:\ngit branch\n* fastercode\n  main\nNote that you can also tell from the git status output on which branch you are:\ngit status\nOn branch fastercode\nnothing to commit, working tree clean\n\n\n\nMaking experimental changes on the new branch\nYou edit the code, stage and commit the changes:\necho \"Yeah, faster code\" &gt;&gt; code.txt\ncat code.txt\nSome great code here\nYeah, faster code\ngit add code.txt\ngit commit -m \"Managed to make code faster\"\n[fastercode 21f1828] Managed to make code faster\n 1 file changed, 1 insertion(+)\nLet’s check the log again, which tells you that the last commit was made on the fastercode branch:\ngit log --oneline\n21f1828 (HEAD -&gt; fastercode) Managed to make code faster\n7ba8ca4 (main) Drafted paper\n3c59d8a Code ready\n\n\n\nMoving back to the main branch\nYou need to switch gears and add references to the paper draft. Since this has nothing to do with your attempt at faster code, you should make these changes back on the main branch:\n# Move back to the 'main' branch\ngit checkout main\nSwitched to branch 'main'\nWhat does code.txt, which we edited on fastercode, now look like?\ncat code.txt\nSome great code here\nSo, by switching between branches, your working dir contents has changed!\nNow, while still on the main branch, add the reference, stage and commit:\necho \"Marra et al. 2014\" &gt; references.txt\ngit add references.txt\ngit commit -m \"Fixed the references\"\n[main 1bf123f] Fixed the references\n 1 file changed, 1 insertion(+)\n create mode 100644 references.txt\nNow that you’ve made changes to both branches, let’s see the log in “graph” format with --graph, also listing all branches with --all — note how it tries to depict these branches:\ngit log --oneline --graph --all\n* 1bf123f (HEAD -&gt; main) Fixed the references\n| * 21f1828 (fastercode) Managed to make code faster\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nFinishing up on the experimental branch\nEarlier, you finished speeding up the code in the fastercode branch, but you still need to document your changes. So, you go back:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nDo you still have the references.txt file from the main branch?\nls\ncode.txt  manuscript.txt\nNope, your working dir has changed again.\nThen, add the “documentation” to the code, and stage and commit:\necho \"# My documentation\" &gt;&gt; code.txt\ngit add code.txt\ngit commit -m \"Added comments to the code\"\n[fastercode d09f611] Added comments to the code\n 1 file changed, 1 insertion(+)\nCheck the log graph:\ngit log --oneline --all --graph\n* d09f611 (HEAD -&gt; fastercode) Added comments to the code\n* 21f1828 Managed to make code faster\n| * 1bf123f (main) Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nMerging the branches\nYou’re happy with the changes to the code, and want to make the fastercode version the default version of the code. This means you should merge the fastercode branch back into main. To do so, you first have to move back to main:\ngit checkout main\nSwitched to branch 'main'\nNow you are ready to merge with the git merge command. You’ll also have to provide a commit message, because a merge is always accompanied by a commit:\ngit merge fastercode -m \"Much faster version of code\"\nMerge made by the 'ort' strategy.\n code.txt | 2 ++\n 1 file changed, 2 insertions(+)\nOnce again, check the log graph, which depicts the branches coming back together:\ngit log --oneline --all --graph\n*   5bb84cd (HEAD -&gt; main) Much faster version of code\n|\\  \n| * d09f611 (fastercode) Added comments to the code\n| * 21f1828 Managed to make code faster\n* | 1bf123f Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nCleaning up\nYou no longer need the fastercode branch, so you can delete it as follows:\ngit branch -d fastercode\nDeleted branch fastercode (was d09f611).\n\n\n\n\n1.3 Branching and merging – Workflow summary\n\n\n\nFigure from after Allesino & Wilmes (2019)\n\n\n\nOverview of commands used in the branching workflow\n# (NOTE: Don't run this)\n\n# Create a new branch:\ngit branch mybranch\n\n# Move to new branch:\ngit checkout mybranch\n\n# Add and commit changes:\ngit add --all\ngit commit -m \"my message\"\n\n# Done with branch - move back to main trunk and merge\ngit checkout main\ngit merge mybranch -m \"Message for merge\"\n\n# And [optionally] delete the branch:\ngit -d mybranch\n\n\n\n Exercise (Intermezzo 2.2)\n\n(a) Move to the directory CSB/git/sandbox.\n\n\n\nSolution\n\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\n\n\n(b) Create a directory thesis and turn it into a Git repository.\n\n\n\nSolution\n\nmkdir thesis\ncd thesis\ngit init\n\n\n(c) Create the file introduction.txt with the line “Best introduction ever.”\n\n\n\nSolution\n\necho \"The best introduction ever\" &gt; introduction.txt\n\n\n(d) Stage introduction.txt and commit with the message “Started introduction.”\n\n\n\nSolution\n\ngit add introduction.txt\ngit commit -m \"Started introduction\"\n\n\n\n(e) Create the branch newintro and change into it.\n\n\n\nSolution\n\ngit branch newintro\ngit checkout newintro\n\n\n(f) Overwrite the contents of introduction.txt, create a new file methods.txt, stage, and commit.\n\n\n\nSolution\n\necho \"A much better introduction\" &gt; introduction.txt\ntouch methods.txt\ngit add --all\ngit commit -m \"A new introduction and methods file\"\n\n\n(g) Move back to main. What does your working directory look like now?\n\n\n\nSolution\n\ngit checkout main\nls     # Changes made on the other branch are not visible here!\ncat introduction.txt\n\n\n(h) Merge in the newintro branch, and confirm that the changes you made there are now in your working dir.\n\n\n\nSolution\n\ngit merge newintro -m \"New introduction\"\nls\ncat introduction.txt\n\n\n(i) Bonus: Delete the branch newintro.\n\n\n\nSolution\n\ngit branch -d newintro"
  },
  {
    "objectID": "week3/w3_git3.html#collaboration-with-git-multi-user-remote-workflows",
    "href": "week3/w3_git3.html#collaboration-with-git-multi-user-remote-workflows",
    "title": "Branching, collaborating, and undoing",
    "section": "2 Collaboration with Git: multi-user remote workflows",
    "text": "2 Collaboration with Git: multi-user remote workflows\nIn a multi-user workflow, your collaborator can make changes to the repository (committing to local, then pushing to remote), and you need to make sure that you stay up-to-date with these changes.\nSynchronization between your and your collaborator’s repository happens via the remote, so now you will need a way to download changes from the remote that your collaborator made. This happens with the git pull command.\n\n\n\n\n\nAFirst, a second user, your collaborator, downloads (clones) the online repo. They should also receive admin rights on the repo (not shown - done on GitHub).\n\n\n\n\n\n\n\n\nBThen, your collaborator commits changes to their local copy of the repository.\n\n\n\n\n\n\n\n\n\n\nCBefore you can receive these changes, your collaborator will need to push their changes to the remote, which you can access too.\n\n\n\n\n\n\n\n\nDTo update your local repo with the changes made by your collaborator, you pull in the changes from the remote. Now all 3 copies of the repo are in sync again!\n\n\n\n\n\nIn a multi-user workflow, changes made by different users are shared via the online copy of the repo. But syncing is not automatic:\n\nChanges to your local repo remain local-only until you push to remote.\nSomeone else’s changes to the remote repo do not make it into your local repo until you pull from remote.\n\nHowever, when your collaborator has made changes, Git will tell you about “divergence” between your local repository and the remote when you run git status:\n# (Don't run this)\ngit status\n\n\n\n\n\nIn a multi-user workflow, you should use use git pull often, since staying up-to-date with your collaborator’s changes will reduce the chances of merge conflicts.\n\n\n2.1 Add a collaborator in GitHub\nYou can add a collaborator to a repository on GitHub as follows:\n\nGo to the repository’s settings:\n\n\n\n\n\n\n\nFind and click Manage access:\n\n\n\n\n\n\n\nClick Invite a collaborator:\n\n\n\n\n\n\n\n\n\n2.2 Merge conflicts\nA so-called merge conflict means that Git is not able to automatically merge two branches, which occurs when all three of the following conditions are met:\n\nYou try to merge two branches (including when pulling from remote: a pull includes a merge)\nOne or more file changes have been committed on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\n\nWhen this occurs, Git has no way of knowing which changes to keep, and it will report a merge conflict as follows:\n\n\n\n\n\n\nResolving a merge conflict\nWhen Git reports a merge conflict, follow these steps:\n\nUse git status to find the conflicting file(s).\n\n\n\n\n\n\n\nOpen and edit those file(s) manually to a version that fixes the conflict (!).\nNote below that Git will have changed these file(s) to add the conflicting lines from both versions of the file, and to add marks that indicate which lines conflict.\nYou have to manually change the contents in your text editor to keep the conflicting content that you want, and to remove the indicator marks that Git made.\nOn the Origin of Species       # Line preceding conflicting line\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD                   # GIT MARK 1: Next line = current branch\nLine 2 - from main             # Conflict line: current branch\n=======                        # GIT MARK 2: Dividing line\nLine 2 - from conflict-branch  # Conflict line: incoming branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; conflict-branch        # GIT MARK 3: Prev line = incoming branch\nUse git add to tell Git you’ve resolved the conflict in a particular file:\ngit add origin.txt\n\n\n\n\n\n\n\nOnce all conflicts are resolved, use git status to check that all changes have been staged. Then, use git commit to finish the merge commit:\ngit commit -m \"Solved the merge conflict\"\n\n\n\n\n\n\n\n\nVS Code functionality for resolving Merge Conflicts\n\n\n\nVS Code has some nice functionality to make Step 2 (resolving the conflict) easier:\ncode &lt;conflicting-file&gt;  # Open the file in VS Code\n\n\n\n\n\nIf you click on “Accept Current Change” or “Accept Incoming Change”, etc., it will keep the desired lines and remove the Git indicator marks. Then, save and exit."
  },
  {
    "objectID": "week3/w3_git3.html#contributing-to-repositories-forking-pull-requests",
    "href": "week3/w3_git3.html#contributing-to-repositories-forking-pull-requests",
    "title": "Branching, collaborating, and undoing",
    "section": "3 Contributing to repositories: Forking & Pull Requests",
    "text": "3 Contributing to repositories: Forking & Pull Requests\n\n3.1 What can you do with someone else’s GitHub repository?\nIn some cases, you may be interested in working in some way with someone else’s repository that you found on GitHub. If you do not have rights to push, you can:\n\nClone the repo and make changes locally (as we have been doing with the CSB repo). When you do this, you can also periodically pull to remain up-to-date with changes in the original repo.\nFork the repository on GitHub and develop it independently. Forking creates a new personal GitHub repo, to which you can push.\nUsing a forked repo, you can also submit a Pull Request with proposed changes to the original repo: for example, if you’ve fixed a bug in someone else’s program.\n\nIf you’re actually collaborating on a project, though, you should ask your collaborator to give you admin rights for the repo, which makes things a lot easier than working via Pull Requests.\n\n\n\n\n\n\nForking, Pull Requests, and Issues are GitHub functionality, and not part of Git.\n\n\n\n\n\n\n\nForking a GitHub repository\nYou can follow along by e.g. forking my originspecies repo.\n\nGo to a GitHub repository, and click the “Fork” button in the top-right:\n\n\n\n\n\n\n\nYou may be asked which account to fork to: select your account.\nNow, you have your own version of the repository, and it is labeled explicitly as a fork:\n\n\n\n\n\n\n\n\nForking workflow\nYou can’t directly modify the original repository, but you can:\n\nFirst, modify your fork (with local edits and pushing).\nThen, submit a so-called Pull Request to the owner of the original repo to pull in your changes.\nAlso, you can also easily keep your fork up-to-date with changes to the original repository.\n\n\n\n\nFigure from Happy Git and GitHub for the useR\n\n\n\n\n\nEditing the forked repository\nTo clone your forked GitHub repository to a dir at OSC, start by creating a dir there — for example:\nmkdir /fs/ess/PAS2700/users/$USER/week03/fork_test\ncd /fs/ess/PAS2700/users/$USER/week03/fork_test\nThen, find the URL for your forked GitHub repository by clicking the green Code button. Make sure you get the SSH URL (rather than the HTTPS URL), and click the clipboard button next to the URL to copy it:\n\n\n\n\n\nThen, type git clone and a space, and paste the URL, e.g.:\ngit clone git@github.com:jelmerp/originspecies.git\nCloning into 'originspecies'...\nremote: Enumerating objects: 31, done.\nremote: Counting objects: 100% (31/31), done.\nremote: Compressing objects: 100% (19/19), done.\nremote: Total 31 (delta 4), reused 30 (delta 3), pack-reused 0\nReceiving objects: 100% (31/31), done.\nResolving deltas: 100% (4/4), done.\nNow, you can make changes to the repository in the familiar way, for example:\necho \"# Chapter 1. Variation under domestication\" &gt; origin.txt\ngit add origin.txt\ngit commit -m \"Suggested title for first chapter.\"\nAnd note that you can push without any setup — because you cloned the repository, the remote setup is already done (and you have permission to push because its your own repo on GitHub and you have set up GitHub authentication):\ngit push\n\n\n\nCreating a Pull Request\nIf you then go back to GitHub, you’ll see that your forked repo is “x commit(s) ahead” of the original repo:\n\n\n\n\n\nClick Pull request, and check whether the right repositories and branches are being compared (and here you can also see the changes that were made in the commits):\n\n\n\n\n\nIf it looks good, click the green Create Pull Request button:\n\n\n\n\n\nGive your Pull Request a title, and write a brief description of your changes:\n\n\n\n\n\n\n\n\nKeeping your fork up-to-date\nAs you saw, you can’t directly push to original repo but instead have to submit a Pull Request (yes, this terminology is confusing!).\nBut, you can create an ongoing connection to the original repo, which you can use to periodically pull to keep your fork up-to-date. This works similarly to connecting your own GitHub repo, but you should give the remote a different nickname than origin — the convention is upstream:\n# Add the \"upstream\" connection\ngit remote add upstream git@github.com:jelmerp/originspecies.git\n\n# List the remotes:\ngit remote -v\norigin   git@github.com:pallass-boszanger/originspecies.git  (fetch)\norigin   git@github.com:pallass-boszanger/originspecies.git  (push)\nupstream   git@github.com:jelmerp/originspecies.git  (fetch)\nupstream   git@github.com:jelmerp/originspecies.git  (push)\n# Pull from the upstream repository:\ngit pull upstream main\n\n\n\n\n\n\n“upstream” is an arbitrary but convential name, compare with “origin” which is used for your own version of the online repo."
  },
  {
    "objectID": "week3/w3_git3.html#undoing-viewing-changes-that-have-been-committed",
    "href": "week3/w3_git3.html#undoing-viewing-changes-that-have-been-committed",
    "title": "Branching, collaborating, and undoing",
    "section": "4 Undoing (& viewing) changes that have been committed",
    "text": "4 Undoing (& viewing) changes that have been committed\nWhereas on the first Git page, we learned about undoing changes that have not been committed, here you’ll see how you can undo changes that have been committed.\n\n\n4.1 Viewing past versions of the repository\nBefore undoing committed changes, you may want to look at earlier states of your repo, e.g. to know what to revert to:\n\nFirst, print an overview of past commits and their messages:\n# (NOTE: example code in this and the next few boxes - don't run as-is)\ngit log --oneline\nFind a commit you want to go back to, and look around in the past:\ngit checkout &lt;sha-id&gt; # Replace &lt;sha-id&gt; by an actual hash\n\nless myfile.txt       # Etc. ...\nThen, you can go back to where you were originally as follows:\ngit checkout main\n\nThe next section will talk about strategies to move your repo back to an earlier state that you found this way.\n\n\n\n\n\n\nJust need to retrieve an older version of a single file?\n\n\n\nIf you just want to retrieve/restore an older version of a single file that you found while browsing around in the past, then a quick way can be: simply copy the file to a location outside of your repo, move yourself back to the “present”, and move the file back into your repo, now in the present.\n\n\n\n\n\nA visual of using git checkout to view files from older versions of your repo.Figure from https://software-carpentry.org.\n\n\n\n\n\n\n\n\nThe multiple uses of git checkout\n\n\n\nNote the confusing re-use of git checkout! We have now seen git checkout being used to:\n\nMove between branches\nMove to previous commits to explore (figure below)\n(Revert files back to previous states — as an alternative to git restore)\n\n\n\n\n\n\n4.2 Undoing entire commits\nTo undo commits, i.e. move the state of your repository back to how it was before the commit you want to undo, there are two main commands:\n\ngit revert: Undo the changes made by commits by reverting them in a new commit.\ngit reset: Delete commits as if they were never made.\n\n\nUndoing commits with git revert\nA couple of examples of creating a new commit that will revert all changes made in the specified commit:\n# Undo changes by the most recent commit:\ngit revert HEAD\n  \n# Undo changes by the second-to-last commit:\ngit revert HEAD^\n\n# Undo changes by a commit identified by its checksum:\ngit revert e1c5739\n\n\nUndoing commits with git reset\ngit reset is quite complicated as it has three modes (--hard, --mixed (default), and --soft) and can act either on individual files and on entire commits. To undo a commit, and:\n\nStage all changes made by that commit:\n# Resetting to the 2nd-to-last commit (HEAD^) =&gt; undoing the last commit\ngit reset --soft HEAD^\nPut all changes made by that commit as uncomitted working-dir changes:\n# Note that '--mixed' is the default, so you could omit that\ngit reset --mixed HEAD^\nCompletely discard all changes made by that commit:\ngit reset --hard HEAD^ \n\n\n\n\n\n\n\ngit reset erases history\n\n\n\nUndoing with git revert is much safer than with git reset, because git revert does not erase any history.\nFor this reason, some argue you should not use git reset on commits altogether. At any rate, you should never use git reset for commits that have already been pushed online.\n\n\n\n\n\n\n4.3 Viewing & reverting to earlier versions of files\nAbove, you learned to undo at a project/commit-wide level. But you can also undo things for specific files:\n\nGet a specific version of a file from a past commit:\n# Retrieve the version of README.md from the second-to-last commit\ngit checkout HEAD^^ -- README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit checkout e1c5739 -- README.md\nNow, your have the old version in the working dir & staged, which you can optionally check with:\n# Optional: check the file at the earlier state\ncat README.md\ngit status\nYou can go on to commit this version from the past, or go back to the current version, as we will do below:\ngit checkout HEAD -- README.md\n\n\n\n\n\n\n\nBe careful with git checkout\n\n\n\nBe careful with git checkout: any uncommitted changes to this file would be overwritten by the past version you retrieve!\n\n\n\nAn alternative method to view and revert to older versions of specific files is to use git show.\n\nView a file from any commit as follows:\n# Retrieve the version of README.md from the last commit\ngit show HEAD:README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit show ad4ca74:README.md\nRevert a file to a previous version:\ngit show ad4ca74:README.md &gt; README.md"
  },
  {
    "objectID": "week3/w3_git3.html#miscellaneous-git",
    "href": "week3/w3_git3.html#miscellaneous-git",
    "title": "Branching, collaborating, and undoing",
    "section": "5 Miscellaneous Git",
    "text": "5 Miscellaneous Git\n\n5.1 Amending commits\nLet’s say you forgot to add a file to a commit, or notice a silly typo in something we just committed. Creating a separate commit for this seems “wasteful” or even confusing, and including these changes along with others in a next commit is also likely to be inappropriate. In such cases, you can amend the previous commit.\nFirst, stage the forgotten or fixed file:\n# (NOTE: don't run this)\ngit add myfile.txt\nThen, amend the commit, adding --no-edit to indicate that you do not want change the commit message:\n# (NOTE: don't run this)\ngit commit --amend --no-edit\n\n\n\n\n\n\nAmending commits is a way of “changing history”\n\n\n\nBecause amending a commit “changes history”, some recommend avoiding this altogether. For sure, do not amend commits that have been published in (pushed to) the online counterpart of the repo.\n\n\n\n\n\n5.2 git stash\nGit stash can be useful when you need to pull from the remote, but have changes in your working dir that:\n\nAre not appropriate for a separate commit\nAre not worth starting a new branch for\n\nHere is an example of the sequence of commands you can use in such cases.\n\nStash changes to tracked files with git stash:\n# (Note: add option '-u' if you need to include untracked files) \ngit stash\nPull from the remote repository:\ngit pull\n“Apply” (recover) the stashed changes back to your working dir:\ngit stash apply\n\n\n\n\n5.3 A few more tips\n\nGit will not pay attention to empty directories in your working dir.\nYou can create a new branch and move to it in one go using:\ngit checkout -b &lt;new-branch-name&gt;\nTo show commits in which a specific file was changed, you can simply use:\ngit log &lt;filename&gt;\n“Aliases” (command shortcuts) can be useful with Git, and can be added in two ways:\n\nBy adding lines like the below to the ~/.gitconfig file:\n[alias]\n  hist = log --graph --pretty=format:'%h %ad | %s%d [%an]' --date=short\n  last = log -1 HEAD  # Just show the last commit\nWith the git config command:\ngit config --global alias.last \"log -1 HEAD\""
  },
  {
    "objectID": "week3/w3_git1.html#an-introduction-to-version-control",
    "href": "week3/w3_git1.html#an-introduction-to-version-control",
    "title": "Getting started with Git",
    "section": "1 An introduction to version control",
    "text": "1 An introduction to version control\n\n1.1 Why use a Version Control System (VCS)?\nHere are some “versioning”- and backup-related challenges for your research project files that you may run into when not using a formal Version Control System (VCS):\n\nWhat to save periodic copies of?\n\nDo you only save versions of individual files?\nSpace-efficient, but doesn’t allow you to go back to the state of other project files at the same point in time.\nDo you save a copy of the full project periodically?\nBetter than the above option, but can become prohibitive in terms of disk storage.\n\nHow to know what changes were made between saved versions?\nHow to collaborate, especially when working simultaneously?\nHow to restore an accidentally deleted, modified, or overwritten file? This can especially be an issue at OSC where there is no recycle bin or undo button.\nHow to manage simultaneous variants of files, such as when making experimental changes?\n\nA formal VCS can help you with these challenges. With a VCS:\n\nYou can easily see your history of changes.\nYou have a time machine: you can go back to past states of your project (and not just of individual files!).\nYou can do simultaneous collaborative work — you can always track down who made which changes.\nSharing your code and other aspects of your project is easy.\nYou can make experimental changes without affecting current functionality.\n\n\nOr, as the CSB book puts it:\n\nVersion control is a way to keep your scientific projects tidily organized, collaborate on science, and have the whole history of each project at your fingertips.\n— CSB Chapter 2\n\n\n\n\n1.2 How Git roughly works\nGit is the most widely used Version Control System1. With Git, you save “snapshots” of your project with every minor piece of progress. Git manages this cleverly without having to create full copies of the project for every snapshot:\n\n\n\nThe boxes with dashed lines depict files that have not changed: these will not be saved repeatedly.Figure from https://git-scm.com.\n\n\n\nAs illustrated above, files that haven’t changed between snapshots are not saved again and again with every snapshot. But Git doesn’t even save full copies of files that have changed: it tracks changes on a line-by-line basis, and saves changed lines (!).\nNote that one Git database (repository) manages files inside a single directory structure, so to use Git, it’s important that your projects are properly organized or at least kept in separate dirs, as discussed last week.\n\n\nKey Git term 1: Repository (repo)\nA Git “repository” (or “repo”) is the version-control database for a project. Note that:\n\nYou can start a Git repository in any dir on your computer.\nThe Git database is saved in a hidden dir .git in the dir in which you started the repo.\nIt is typical (& recommended) that you should have one Git repository for each research project.\nYou can also download any public online Git repository. (We already did this in week 1 of the course, when we used git clone to download the CSB book’s repository.)\n\n\n\n\n\n\n\nHidden files and dirs\n\n\n\nWhen a file or dir name has a leading ., it’s “hidden”. These don’t show up in file browsers by default, nor in ls file listings unless you use the -a (“all”) option. Hidden files and dirs are often generated automatically by software.\n\n\n\n\nKey Git term 2: Commit\nA Git “commit” is a saved snapshot of the project. For now, note that:\n\nIt is always possible to go the exact state of the entire project or individual files for any commit.\nWhenever you create a commit, you also include a message describing the changes you made.\n\n\n\n\n\n1.3 What do I put under version control?\nThe primary files to put under version control are:\n\nScripts2.\nProject documentation files.\nMetadata.\nManuscripts, if you write them in a plain text format.\n\nWhat about data and results?\n\nRaw data may or may not be included — for omics data, this is generally not feasible due to large file sizes.\nResults from analyses should generally not be included.\n\n\n\nSource versus derived files\nThe general idea behind what you should and should not include is that you should version-control the source, but not derived files. For instance:\n\nVersion-control your Markdown file, not the HTML it produces.\nVersion-control your script, not the output it produces.\n\n\n\n\n\n\n\nDerived files\n\n\n\nRecall last week’s point that results and other derived files are (or should be) dispensable, because they can be regenerated using the raw data and the scripts.\n\n\n\n\n\nFile limitations\nThere are some limitations to the types and sizes of files that can be committed with Git:\n\nFile type: binary (non-text) files, such a Word or Excel files, or compiled software, can be included but can’t be tracked in quite the same way as plain-text files3.\nRepository size: for performance reasons, it’s best to keep individual repositories under about 1 GB.\nFile size: while you can have them in your Git repo, GitHub will not allow you to upload files &gt;100 MB.\n\nAs such, omics data is usually too large to be version-controlled. To make your data available to others, you can use dedicated repositories like the NCBI’s Sequence Read Archive (SRA).\n\n\n\n\n1.4 User Interfaces for Git\n\n\n\nBy xkcd\n\n\nYou can work with Git in several different ways — using:\n\nThe native command-line interface (CLI).\nThird-party graphical user interfaces (GUIs) such as Git Kraken.\nIDEs/editors with Git integration like RStudio and VS Code.\n\nIn this course, we will mainly focus on the CLI because it’s the most universal and powerful interface. But it’s absolutely fine to switch to GUI usage later, which will not be hard if you’ve learned the basics with the CLI.\nGit takes some getting used to, regardless of the interface. Many people have one or more “false starts” with it. I hope that being “forced” to use it in a course4 will take you past that!"
  },
  {
    "objectID": "week3/w3_git1.html#the-basic-git-workflow",
    "href": "week3/w3_git1.html#the-basic-git-workflow",
    "title": "Getting started with Git",
    "section": "2 The basic Git workflow",
    "text": "2 The basic Git workflow\nGit commands always start with git followed by a second command/subcommand or “verb”: git add, git commit, etc. Only three commands tend to make up the vast majority of your Git work:\n\ngit add does two things:\n\n\nStart “tracking” files (i.e., files in your directory structure are not automatically included in the repo).\nMark changed/new files as ready to be committed, which is called “staging” files.\n\ngit commit\nCreate a new snapshot of the project by commiting all currently staged files (changes).\ngit status\nGet the status of your repo: which files have changed, which new files are present, tips on next steps, etc.\n\n\n\n\nAdding and committing changes with Git commands.The Git database, which is in a hidden folder .git, is depicted with a gray background.\n\n\n\n\n\n\nAnother way of visualizing the adding and committing of changes in Git.Note that git add has a dual function: it starts tracking files and stages them."
  },
  {
    "objectID": "week3/w3_git1.html#getting-set-up",
    "href": "week3/w3_git1.html#getting-set-up",
    "title": "Getting started with Git",
    "section": "3 Getting set up",
    "text": "3 Getting set up\nWe will start with loading Git at OSC5 and then do some one-time personal Git configuration:\n\nLaunch VS Code at https://ondemand.osc.edu as before, at the dir /fs/ess/PAS2700/users/$USER, and open a terminal in VS Code.\n“Load” the most recent version of Git that is available at OSC with the module load command6:\nmodule load git/2.39.0\nUse git config to make your (actual, not user) name known to Git:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known (use the same email address you signed up for GitHub with):\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\n(Occasionally7, Git will open up a text editor for you. Even though we’re using VS Code, here it is better to select a text editor that runs directly in the shell, like nano.)\ngit config --global core.editor \"nano -w\"\nActivate Git output with colors:\ngit config --global color.ui true\nChange the default “branch” name to main:\ngit config --global init.defaultbranch main\nCheck whether you successfully changed the settings:\ngit config --global --list\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\n# init.defaultbranch=main"
  },
  {
    "objectID": "week3/w3_git1.html#your-first-git-repository",
    "href": "week3/w3_git1.html#your-first-git-repository",
    "title": "Getting started with Git",
    "section": "4 Your first Git repository",
    "text": "4 Your first Git repository\nYou’ll create a Git repository for a mock book project: writing Charles Darwin’s “On the Origin of Species”.\n\n4.1 Start a new Git repository\nCreate a new dir for a mock project that we will version-control with Git, and move there:\n# Before starting, you should be in /fs/PAS2700/users/$USER, cd there first if needed\nmkdir -p week03/originspecies\ncd week03/originspecies\nThe command to initialize a new Git repository is git init — use that to start a repo for the originspecies dir:\ngit init\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/week03/originspecies/.git/\nCan we confirm that the Git repo dir is there?\n# The -a option to ls will also show hidden files\nls -a\n.  ..  .git\nNext, check the status of your new repository with git status:\ngit status\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\nGit reports that you:\n\nAre on a “branch” that is called ‘main’. We won’t talk about Git branches in class, but this is discussed in the optional self-study material and CSB Chapter 2.6. Basically, these are “parallel versions” of your repository.\nHave not created any commits yet.\nHave “nothing to commit” because there are no files in this dir.\n\n\n\n\n4.2 Your first Git commit\nYou will start writing the book (😉) by echo-ing some text into a new file called origin.txt:\necho \"An Abstract of an Essay on ...\" &gt; origin.txt\nNow, check the status of the repository again:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        origin.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit has clearly detected the new file. But as mentioned, Git does not automatically start “tracking” files, which is to say it won’t automatically include files in the repository. Instead, it tells you the file is “Untracked” and gives a hint on how to add it to the repository.\nSo, start tracking the file and stage it all at once with git add:\n# (Note that tab-completion on file names will work here, too)\ngit add origin.txt\nCheck the status of the repo again:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   origin.txt\nNow, your file has been added to the staging area (also called the Index) and is listed as a “change to be committed”8. This means that if you now run git commit, the file would be included in that commit.\nSo, with your file tracked & staged, let’s make your first commit. Note that you must add the option -m followed by a “commit message”: a short description of the changes you are including in the current commit.\n# We use the commit message (option '-m') \"Started the book\" to describe our commit\ngit commit -m \"Started the book\"\n[main (root-commit) 3df4361] Started the book\n 1 file changed, 1 insertion(+)\n create mode 100644 origin.txt\nNow that you’ve made your first Git commit, check the status of the repo again:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\n\nTry to get used to using git status a lot — as a sanity check before and after other git actions.\n\n\n\n\n\n\nAlso look at the commit history of the repo with git log:\ngit log\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21 (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\nNote the “hexadecimal code” (using numbers and the letters a-f) on the first line — this is a unique identifier for each commit, called the SHA-1 checksum. You can reference and access each past commit with these checksums.\n\n\n\n4.3 Your second commit\nStart by modifying the book file — you’ll actually overwrite the earlier content:\necho \"On the Origin of Species\" &gt; origin.txt\nCheck the status of the repo:\ngit status\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   origin.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit has noticed the changes, because the file is being tracked: origin.txt is listed as “modified”. But changes to tracked files aren’t automatically staged — use git add to stage the file as a first step to committing these changes:\ngit add origin.txt\nNow, make your second commit:\ngit commit -m \"Changed the title as suggested by Murray\"\n[main f106353] Changed the title as suggested by Murray\n 1 file changed, 1 insertion(+), 1 deletion(-)\nGit gives a brief summary of the changes that were made: you changed 1 file (origin.txt), and since you replaced the line of text in that file, it is interpreting that as 1 insertion (the new line) and 1 deletion (the removed/replace line).\nCheck the history of the repo again — you’ll see that there are now 2 commits:\ngit log\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\n\n\n\n\n\n\n\nOne-line commit log\n\n\n\nAs you start accumulating commits, you might prefer git log --oneline for a one-line-per-commit summary:\ngit log --oneline\n1e2bba4 Changed the title as suggested by Murray\n4fd04af Started the book\n\n\n\n\n\n\n\n\n\nStaging files efficiently\n\n\n\nWhen you have multiple files that you would like to stage, you don’t need to add them one-by-one:\n# NOTE: Don't run any of this - these are hypothetical examples\n\n# Stage all files in the project (either option works):\ngit add --all\ngit add *\n\n# Stage all files in a specific dir (here: 'scripts') in the project:\ngit add scripts/*\n\n# Stage all shell scripts *anywhere* in the project:\ngit add *sh   \nFinally, you can use the -a option for git commit as a shortcut to stage and commit all changes with a single command (but note that this will not add untracked files):\n# Stage & commit all tracked files:\ngit commit -am \"My commit message\"\n\n\n\n\n\n4.4 What to include in individual commits\nThe last example in the box above showed the -a option to git commit, which allows you to at once stage & commit all changes since the last commit. That seems more convenient than separately git adding files before committing.\nHowever, it’s good practice not to simply and only commit, say, at the end of each day, but instead to try and create commits for units of progress worth saving and as such create separate commits for distinct changes.\nFor example, let’s say that you use git status to check which files you’ve changed since your last commit, and you find that you have:\n\nUpdated a README file to include more information about your samples.\nWorked on a script to run quality control of sequence files.\n\nThese are completely unrelated changes, and it would not be recommended to include both in a single commit.\n\n\n Exercise (CSB Intermezzo 2.1)\n\nCreate a new file todo.txt containing the line: “June 18, 1858: read essay from Wallace”.\n\n\n\nClick to see the solution\n\necho \"June 18, 1858: read essay from Wallace\" &gt; todo.txt\n\n\nUse a Git command to stage the file.\n\n\n\nClick to see the solution\n\ngit add todo.txt\n\n\nCreate a Git commit with the commit message “Added to-do list”.\n\n\n\nClick to see the solution\n\ngit commit -m \"Added to-do list\""
  },
  {
    "objectID": "week3/w3_git1.html#file-states-and-showing-changes",
    "href": "week3/w3_git1.html#file-states-and-showing-changes",
    "title": "Getting started with Git",
    "section": "5 File states and showing changes",
    "text": "5 File states and showing changes\n\n5.1 File states (and Git’s three “trees”)\nTracked files can be in one of three states:\n\nUnchanged since the last commit: committed (latest version is in the repo/commits).\nModified and staged since the last commit: staged (latest version is in the Index).\nModified but not staged since the last commit: modified (latest version is in the working dir).\n\n\n\n\n\n\n\n\nThe three trees of Git (Click to expand)\n\n\n\n\n\nThese three states correspond to the three “trees” of Git:\n\nHEAD: State of the project in the most recent commit9.\nIndex (Stage): State of the project ready to be committed.\nWorking directory (Working Tree): State of the project as currently on your computer.\n\n\n\n\n\nThe three “trees” of Git: HEAD, the index, and the working dir.The hexadecimals in the Commits rectangles are abbreviated checksums for each commit.\n\n\n\nOr consider this table for a hypothetical example in which HEAD, the Index, and the working dir all differ with regards to the the version of file 1, and there also is an untracked file in the working dir:\n\n\n\n\n\n\n\n\nFile state\nVersion\nWhich tree\n\n\n\n\nCommitted\nfile 1 version X\nHEAD\n\n\nStaged\nfile 1 version Y\nIndex (stage)\n\n\nModified\nfile 1 version Z\nWorking dir\n\n\nUntracked\nfile 2 version X\nWorking dir\n\n\n\n\n\n\n\n\n\n\n\n\nWays to refer to past commits (Click to expand)\n\n\n\n\n\nTo refer to specific past commits, you can:\n\nUsing the hexadecimal checksum (either the full ID or the 7-character abbreviation)\nUse HEAD notation: HEAD is the most recent commit, and there are two ways of indicating ancestors of HEAD:\n\n\n\n\n\nTo refer to past commits, you can use checksums (e.g. dab0dc4 for the second-to-last commit)or HEAD notation (HEAD^^ or HEAD~2 for the second-to-last commit).\n\n\n\n\n\n\n\n\n5.2 Showing changes\nYou can use the git diff command to show changes that you have made. By default, it will show all changes between the working dir and:\n\nThe last commit if nothing has been staged.\nThe stage (Index) if something has been staged.\n\n\n\n\n\n\n\n“Working dir” in the context of Git\n\n\n\nNote that when I talk about the “working dir” in the context of Git, I mean not just your top-level project/repository directory, or any specific dir within there that you may have cd-ed into, but the entire project/repository directory hierarchy.\nIt is mainly used to distinguish the state of your project on your computer (“working dir”) versus that in the repository (“index” and “commits”), and should technically be referred to as the “working dir tree”.\n\n\nRight now, there are no differences to report in our originspecies repository, because our working dir, the stage/Index, and the last commit are all the same:\n# Git diff will not have output if there are no changes to report\ngit diff\nChange the to-do list (note: for this to work, you should have done the exercise above!), and check again:\necho \"June 20, 1858: Send first draft to Huxley\" &gt;&gt; todo.txt\n\ngit diff\ndiff --git a/todo.txt b/todo.txt\nindex e3b5e55..9aca508 100644\n--- a/todo.txt\n+++ b/todo.txt\n@@ -1 +1,2 @@\n June 18, 1858: read essay from Wallace\n+June 20, 1858: Send first draft to Huxley\nWe won’t go into the details of the above “diff format”, but at the bottom of the output above, you can see some specific changes: the line “Send first draft to Huxley” was added (hence the + sign) in our latest version of the file.\n\n\n\n\n\n\n\nVS Code can show file differences in a nicer way\n\n\n\n\nClick on the Git symbol in the narrow side bar (below the search icon) to open the Source Control side bar.\nIn the source control sidebar, you should see not just the originspecies repository listed, but also the CSB repo10. If needed, click on originspecies to expand it:\n\n\n\n\n\n\n\nWithin the originspecies listing, you should see todo.txt: click on the M next to the file todo.txt, and the following should appear in your editor pane:\n\n\n\n\n\n\nThat’s a much more intuitive overview that makes it clear which line was added.\n\n\n\n\n\n\n\n\nMore git diff (Click to expand)\n\n\n\n\n\n\nTo show changes between the Index (stage) and the last commit, use the --staged option to git diff.\nIf you have changed multiple files, but just want to see differences for one of them, you can specify the filename — in our case here, that will give the same output as the plain git diff command above, since we only changed one file:\ngit diff todo.txt\n# Output not shown, same as above\nYou can also compare your repo or individual files between any two arbitrary commits (for the HEAD notation, see the boxes on the “three trees” of Git above.):\n# Last commit vs second-to-last commit - full repo:\ngit diff HEAD HEAD^\n\n# Last commit vs a specified commit - specific file: \ngit diff HEAD d715c54 todo.txt \n\n\n\n\n\n Exercise: another commit\nStage and commit the changes to todo.txt, then check what you have done.\n\n\nClick to see the solution\n\n\nStage the file:\ngit add todo.txt\nCommit:\ngit commit -m \"Update the TODO list\"\n[main 8ec8103] Update the TODO list\n1 file changed, 1 insertion(+)\nCheck the log:\ngit log\ncommit 8ec8103e8d01b342f9470908b87f0649be53edd5\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 12:30:35 2024 -0400\n\n    Update the TODO list\n\ncommit 9715ab5325429526a90ea49e9d40a923c93ccb72\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:37:32 2024 -0400\n\n    Added a gitignore file\n\ncommit 603d1792619bf628d66cd91a45cd7114e3d6b95b\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:21:36 2024 -0400\n\n    Added to-do list\n\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book"
  },
  {
    "objectID": "week3/w3_git1.html#ignoring-files-and-directories",
    "href": "week3/w3_git1.html#ignoring-files-and-directories",
    "title": "Getting started with Git",
    "section": "6 Ignoring files and directories",
    "text": "6 Ignoring files and directories\nAs discussed above, it’s best not to track some files, such as very bulky data files, temporary files, and results.\nWe’ve seen that Git will notice and report any “untracked” files in your project whenever you run git status. This can get annoying and can make it harder to spot changes and untracked files that you do want to add — and you might even accidentally start tracking these files such as with git add --all.\nTo deal with this, you can tell Git not to pay attention to certain files by adding file names and wildcard selections to a .gitignore file. This way, these files won’t be listed as untracked files when you run git status, and they wouldn’t be added even when you use git add --all.\nTo see this in action, let’s start by adding some content that we don’t want to commit to our repository: a dir data, and a file ending in a ~ (a temporary file type that e.g. text editors can produce):\nmkdir data\ntouch data/drawings_1855-{01..12} todo.txt~\nWhen we check the status of the repo, we can see that Git has noticed these files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        todo.txt~\nIf we don’t do anything about this, Git will keep reporting these untracked files whenever we run git status. To prevent this, we will we create a .gitignore file:\n\nThis file should be in the project’s root dir and should be called .gitignore.\n.gitignore is a plain text file that contains dir and file names/patterns, all of which will be ignored by Git.\nAs soon as such a file exists, Git will automatically check and process its contents.\nIt’s a good idea add and commit this file to the repo.\n\nWe will create our .gitignore file and add the following to it to instruct Git to ignore everything in the data/ dir, and any file that ends in a ~:\necho \"data/\" &gt; .gitignore\necho \"*~\" &gt;&gt; .gitignore\ncat .gitignore\ndata/\n*~\nWhen we check the status again, Git will have automatically processed the contents of the .gitignore file, and the files we want to ignore should no longer be listed as untracked files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\nHowever, we do now have an untracked .gitignore file, and we should track and commit this file:\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\n[main 9715ab5] Added a gitignore file\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\n\n\n\n\n\n\n\nGood project file organization helps with version control\n\n\n\nGood project file organization, as discussed last week, can make your life with Git a lot easier. This is especially true when it comes to files that you want to ignore.\nSince you’ll generally want to ignore data and results files, if you keep all of those in their own top-level directories, it will be easy and not error-prone to tell Git to ignore them. But if you were -for example- mixing scripts and either results or data within dirs, it would be much harder to keep this straight."
  },
  {
    "objectID": "week3/w3_git1.html#moving-and-removing-tracked-files",
    "href": "week3/w3_git1.html#moving-and-removing-tracked-files",
    "title": "Getting started with Git",
    "section": "7 Moving and removing tracked files",
    "text": "7 Moving and removing tracked files\nWhen wanting to remove, move, or rename files that are tracked by Git, it is good practice to preface regular rm and mv commands with git: so, git rm &lt;file&gt; and git mv &lt;source&gt; &lt;dest&gt;.\nWhen removing or moving/renaming a tracked file with git rm / git mv, changes will be made to your working dir just like with a regular rm/mv, and the operation will also be staged. For example:\n# (NOTE: Don't run this, hypothetical examples)\ngit rm file-to-remove.txt\ngit mv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n        deleted:    file-to-remove.txt\n\n\n\n\n\n\nWhat if I forget to use git rm/git mv? (Click to expand)\n\n\n\n\n\nIt is inevitable that you will occasionally forget about this and e.g. use rm instead of git rm. Fortunately, Git will eventually figure out what happened. For example:\n\nFor a renamed file, Git will first be confused and register both a removed file and an added file:\n# (Don't run this, this is a hypothetical example)\nmv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    myoldname.txt\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        mynewname.txt\nBut after you stage both changes (the new file and the deleted file), Git realizes it was renamed instead:\ngit add myoldname.txt\ngit add mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n\nSo, there is no need to stress if you forget this, but when you remember, use git mv and git rm.\n\n\n\n\n\n Exercises: .gitignore and git rm\nA) Create a new directory results with files Galapagos.txt and Ascencion.txt. Add a line to the .gitignore file to ignore these results, and commit the changes to the .gitignore file.\n\n\nClick to see the solution\n\n\nCreate the dir and files:\nmkdir results\ntouch results/Galapagos.txt results/Ascencion.txt\nOptional - check that they are detected by Git (note: only the dir will be shown, not its contents):\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        results/\n\nnothing added to commit but untracked files present (use \"git add\" to track\nAdd the string “results/” to the .gitignore file:\necho \"results/\" &gt;&gt; .gitignore\nOptional - check the status again:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   .gitignore\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLooks good, results/ is no longer listed. But we do need to commit the changes to .gitignore.\nCommit the changes to .gitignore:\ngit add .gitignore\ngit commit -m \"Add results dir to gitignore\"\n[main 33b6576] Add results dir to gitignore\n1 file changed, 1 insertion(+)\n\n\nB) Create and commit an empty new file notes.txt. Then, remove it with git rm and commit your file removal.\n\n\nClick to see the solution\n\n\nCreate the file and add and commit it:\ntouch notes.txt\ngit add notes.txt\ngit commit -m \"Add notes\"\n[main 44a37f9] Add notes\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 notes.txt\nOptional - check that the file is there:\nls\ndata  notes.txt  origin.txt  README.md  todo.txt  todo.txt~\nRemove the file with git rm and commit the removal:\ngit rm notes.txt\ngit commit -m \"These notes were made in error\"\n[main 058fd47] These notes were made in error\n 1 file changed, 0 insertions(+), 0 deletions(-)\n delete mode 100644 notes.txt\nOptional - check that the file is no longer there:\nls\ndata  origin.txt  README.md  todo.txt  todo.txt~"
  },
  {
    "objectID": "week3/w3_git1.html#undoing-changes-that-have-not-been-committed",
    "href": "week3/w3_git1.html#undoing-changes-that-have-not-been-committed",
    "title": "Getting started with Git",
    "section": "8 Undoing changes that have not been committed",
    "text": "8 Undoing changes that have not been committed\nHere, you’ll learn how to undo changes that have not been committed, like undoing an accidental file removal or overwrite. (In the optional self-study Git material, there is a section on undoing changes that have been committed.)\n\n8.1 Recovering a version from the repo\nWe’ll practice with undoing changes to your working dir (that have not been staged) by recovering a version from the repo: in other words, using Git as an “undo button” after accidental file changes or removal.\n\nLet’s say you accidentally overwrote instead of appended to a file:\necho \"Finish the taxidermy of the finches from Galapagos\" &gt; todo.txt\nAlways start by checking the status:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYou’ll want to “discard changes in working directory”, and Git told you how to do this — with git restore:\ngit restore todo.txt\n\n\nIf you accidentally deleted a file, you can similarly retrieve it with git checkout:\n\nAccidental removal of todo.txt\nrm todo.txt\nUse git restore to get the file back!\ngit restore todo.txt\n\n\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git checkout (Click to expand)\n\n\n\n\n\nUntil recently, this action used to be done with with the git checkout command, for example:\ngit checkout -- README.md\ngit restore is a relatively new command designed to avoid confusion with the git checkout and git reset commands, which have multiple functions. The CSB book still uses the git checkout command for a similar example11.\n\n\n\n\n\n\n8.2 Unstaging a file\ngit restore can also unstage a file, which is most often needed when you added a file that was not supposed to be part of the next commit. For example:\n\nYou modify two files and use git add --all:\necho \"Variation under domestication\" &gt;&gt; origin.txt\necho \"Prepare for the next journey\" &gt;&gt; todo.txt\n\ngit add --all\nThen you realize that those two file changes should be part of separate commits. Again, check the status first:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n        modified:   todo.txt\nAnd use git restore --staged as suggested by Git:\ngit restore --staged todo.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nNow, you can go ahead and add these changes to separate commits: see the exercise below.\n(Finally: in case you merely staged a file prematurely, you can just continue editing the file and re-add it.)\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git reset (Click to expand)\n\n\n\n\n\nLike with discarding changes in the working dir, this action used to be done with another command, this time git reset. For example:\ngit reset HEAD README.md\nThe CSB book uses this but note that there is a mistake in the book: git reset will only unstage and not revert the file back to its state at the last commit. (git reset --hard does revert things back to the state of a desired commit, but only works on commits and not individual files.)\n\n\n\n\n\n Exercise: Commit the changes 1-by-1\n\nCommit the currently staged changes to origin.txt.\nStage and commit the changes to todo.txt.\n\n\n\nClick for the solution\n\n\nCommit the currently staged changes to origin.txt.\ngit commit -m \"Start writing about artificial selection\"\nStage and commit the changes to todo.txt.\ngit add todo.txt\ngit commit -m \"Update the TODO file\"\n\n\n\n\n\n\n\n\n\n\nUndoing staged changes\n\n\n\nWhat if you had made mistaken changes (like an accidental deletion) and also staged those changes? You can simply follow both of the two steps described above in order:\n\nFirst unstage the file with git restore --staged &lt;file&gt;.\nThen discard changes in the working dir with git restore &lt;file&gt;.\n\nFor instance, you overwrote the contents of the book and then staged the misshapen file:\necho \"Instincts of the Cuckoo\" &gt; origin.txt\ngit add origin.txt\n\ncat origin.txt\nInstincts of the Cuckoo\nYou can undo all of this as follows:\ngit restore --staged origin.txt\ngit restore origin.txt\n\ncat origin.txt\nOn the Origin of Species\nVariation under domestication"
  },
  {
    "objectID": "week3/w3_git1.html#some-git-best-practices",
    "href": "week3/w3_git1.html#some-git-best-practices",
    "title": "Getting started with Git",
    "section": "9 Some Git best-practices",
    "text": "9 Some Git best-practices\n\nWrite informative commit messages.\nImagine looking back at your project in a few months, after finding an error that you introduced a while ago.\n\nNot-so-good commit message: “Updated file”\nGood commit message: “In file x, updated function y to include z”\n\n\n\n\n\nImage source\n\n\n\n\n\n\n\n\nCommit messages for the truly committed\n\n\n\nIt is often argued that commit messages should preferably be in the form of completing the sentence “This commit will…”: When adhering to this, the above commit message would instead say “In file x, update function y to include z.”.\n\n\n\nCommit often, using small commits.\nThis will also help to keep commit messages informative!\nBefore committing, check what you’ve changed.\nUse git diff [--staged] or VS Code functionality.\nAvoid including unrelated changes in commits.\nSeparate commits if your working dir contains work from disparate edits: use git add + git commit separately for two sets of files.\nDon’t commit unnecessary files.\nThese can also lead to conflicts — especially automatically generated, temporary files.\n\n\n\n\n\n\n\n\nA more advanced tip: tags\n\n\n\nIf you have a repo with general scripts, which you continue to develop and use in multiple projects, and you publish a paper in which you use these scripts, it is a good idea to add a “tag” to a commit to mark the version of the scripts used in your analysis:\ngit tag -a v1.2.0 -m \"Clever release title\"\ngit push --follow-tags"
  },
  {
    "objectID": "week3/w3_git1.html#footnotes",
    "href": "week3/w3_git1.html#footnotes",
    "title": "Getting started with Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOthers include SVN and Mercurial.↩︎\nAnd if you’re writing software, all its source code.↩︎\nGit will just save an entirely new version whenever there’s been a change rather than tracking changes in individual lines.↩︎\nE.g., you’ll have to use Git for you final project.↩︎\n It is available by default, but that’s a very ancient version.↩︎\n We will talk much more about “loading” (and installing) software at OSC in week 5↩︎\nWhen you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line.↩︎\n You also get a hint on how to “unstage” the file: i.e., reverting what you just did with git add and leaving the file untracked once again↩︎\nOn the current “branch” – see the optional self-study page or CSB chapter 2.6 to learn about branches.↩︎\n This is because our VS Code working dir is not originspecies but two levels up from there. Typically, your VS Code working dir should be your project dir which would be the same as the repo dir.↩︎\n In that example, the CSB book example omits the dashes --. These indicate that the checkout command should operate on a file, but since the file name is provided too, this is not strictly necessary.↩︎"
  },
  {
    "objectID": "week4/w4_overview.html#links",
    "href": "week4/w4_overview.html#links",
    "title": "Week 4: Shell scripting & CLI tools",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – Shell scripts.\nThursday – Running CLI tools with shell scripts.\nBonus (optional self-study) content: While loops, arrays, and more.\n\n\n\nExercises & assignments\n\nExercises for this week\nYour final project proposal will be due on Monday of week 6 (April 8th)"
  },
  {
    "objectID": "week4/w4_overview.html#content-overview",
    "href": "week4/w4_overview.html#content-overview",
    "title": "Week 4: Shell scripting & CLI tools",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we will talk about shell scripting. So far, we have been running commands interactively in the shell, one line at a time. When you need to repeat a certain sequence of commands regularly, or run a bioinformatics program that may take a while, it becomes useful to put your shell commands in a script. Such a script can be easily and quickly (re-)executed, or submitted to a queue on a cluster (the latter is next week’s topic).\nWe will also start practicing with running programs/tools with a command-line interface (CLI), focusing on bioinformatics/genomics tools, and doing so inside shell scripts.\nSome of the things you will learn this week:\n\nWhy it is useful to collect your commands into shell scripts that can be rerun easily.\nThe basics of shell scripts including hell script header lines.\nWhy and how to adorn scripts with tests and echo statements.\nMore on shell variables and how to use them.\nUsing command-line arguments with your own scripts.\nif statements and true/false tests.\nRunning command-line programs (we focus on bioinformatics tools), and running them using shell scripts.\n\n\n2.1 Readings\nThis week’s reading is Chapter 12 from the Buffalo book.\nThe latter part of this chapter is about using find, xargs, and Makefiles. These are somewhat tangential to the week’s topic of scripts, and we will not talk about them in class.\nI would recommend to read that part of the chapter only if you want. As for Makefiles specifically, it will be good to understand the principle behind them, but there is no need to fully understand the syntax, since we will learn about Nextflow, an alternative approach to workflow management, later in the course.\n\nRequired readings\n\nBuffalo Chapter 12: “Bioinformatics Shell Scripting, Writing Pipelines, and Parallelizing Tasks”"
  },
  {
    "objectID": "week4/w4_3_bonus.html#while-loops",
    "href": "week4/w4_3_bonus.html#while-loops",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "1 While loops",
    "text": "1 While loops\nIn bash, while loops are mostly useful in combination with the read command, to loop over each line in a file. If you use while loops, you’ll very rarely need Bash arrays (next section), and conversely, if you like to use arrays, you may not need while loops much.\nwhile loops will run as long as a condition is true and this condition can include constructs such as read -r which will read input line-by-line, and be true as long as there is a line left to be read from the file. In the example below, while read -r will be true as long as lines are being read from a file fastq_files.txt — and in each iteration of the loop, the variable $fastq_file contains one line from the file:\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt\nseq/zmaysA_R1.fastq\nseq/zmaysA_R2.fastq\nseq/zmaysB_R1.fastq\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt | while read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nA more elegant but perhaps confusing syntax variant used input redirection instead of cat-ing the file:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; fastq_files.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nWe can also process each line of the file inside the while loop, like when we need to select a specific column:\n# [ Don't run this - hypothetical example]\nhead -n 2 samples.txt\nzmaysA  R1      seq/zmaysA_R1.fastq\nzmaysA  R2      seq/zmaysA_R2.fastq\n# [ Don't run this - hypothetical example]\nwhile read -r my_line; do\n    echo \"Have read line: $my_line\"\n    fastq_file=$(echo \"$my_line\" | cut -f 3)\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nHave read line: zmaysA  R1      seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R1.fastq\nHave read line: zmaysA  R2      seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysA_R2.fastq\nAlternatively, you can operate on file contents before inputting it into the loop:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; &lt;(cut -f 3 samples.txt)\nFinally, you can extract columns directly as follows:\n# [ Don't run this - hypothetical example]\nwhile read -r sample_name readpair_member fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq"
  },
  {
    "objectID": "week4/w4_3_bonus.html#arrays",
    "href": "week4/w4_3_bonus.html#arrays",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "2 Arrays",
    "text": "2 Arrays\nBash “arrays” are basically lists of items, such as a list of file names or samples IDs. If you’re familiar with R, they are like R vectors1.\nArrays are mainly used with for loops: you create an array and then loop over the individual items in the array. This usage represents an alternative to looping over files with a glob. Looping over files with a glob is generally easier and preferable, but sometimes this is not the case; or you are looping e.g. over samples and not files.\n\nCreating arrays\nYou can create an array “manually” by typing a space-delimited list of items between parentheses:\n# The array will contain 3 items: 'zmaysA', 'zmaysB', and 'zmaysC'\nsample_names=(zmaysA zmaysB zmaysC)\nMore commonly, you would populate an array from a file, in which case you also need command substitution:\n\nSimply reading in an array from a file with cat will only work if the file simply contains a list of items:\nsample_files=($(cat fastq_files.txt))\nFor tabular files, you can include e.g. a cut command to extract the focal column:\nsample_files=($(cut -f 3 samples.txt))\n\n\n\n\n\n\n\n\nAlternatively, use the mapfile command\n\n\n\nTODO\n\n\n\n\n\nAccessing arrays\nFirst off, it is useful to realize that arrays are closely related to regular variables, and to recall that the “full” notation to refer to a variable includes curly braces: ${myvar}. When referencing arrays, the curly braces are always needed.\n\nUsing [@], we can access all elements in the array (and arrays are best quoted, like regular variables):\necho \"${sample_names[@]}\"\nzmaysA zmaysB zmaysC\nWe can also use the [@] notation to loop over the elements in an array:\nfor sample_name in \"${sample_names[@]}\"; do\n    echo \"Processing sample: $sample_name\"\ndone\nProcessing sample: zmaysA\nProcessing sample: zmaysB\nProcessing sample: zmaysC\n\n\n\n\n\n\n\n\nOther array operations (Click to expand)\n\n\n\n\n\n\nExtract specific elements (note: Bash arrays are 0-indexed!):\n# Extract the first item\necho ${sample_names[0]}\nzmaysA\n# Extract the third item\necho ${sample_names[2]}\nzmaysC\nCount the number of elements in the array:\necho ${#sample_names[@]}\n3\n\n\n\n\n\n\n\nArrays and filenames with spaces\nThe file files.txt contains a short list of file names, the last of which has a space in it:\ncat files.txt\nfile_A\nfile_B\nfile_C\nfile D\nWhat will happen if we read this list into an array, and then loop over the array?\n# Populate an array with the list of files from 'files.txt'\nall_files=($(cat files.txt))\n\n# Loop over the array:\nfor file in \"${all_files[@]}\"; do\n    echo \"Current file: $file\"\ndone\nCurrent file: file_A\nCurrent file: file_B\nCurrent file: file_C\nCurrent file: file\nCurrent file: D\nUh-oh! The file name with the space in it was split into two items! And note that we did quote the array in \"${all_files[@]}\", so clearly, this doesn’t solve that problem.\nFor this reason, it’s best not to use arrays to loop over filenames with spaces (though there are workarounds). Direct globbing and while loops with the read function (while read ..., see below) are easier choices for problematic file names.\nAlso, this example once again demonstrates you should not have spaces in your file names!\n\n\n\n Exercise: Bash arrays\n\nCreate an array with the first three file names (lines) listed in samples.txt.\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nCheck whether you created your files.\n\n\n\nSolutions\n\n\nCreate an array with the first three file names (lines) listed in samples.txt.\n\ngood_files=($(head -n 3 files.txt))\n\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nfor good_file in \"${good_files[@]}\"; do\n    touch \"$good_file\"\ndone\nCheck whether you created your files.\nls\nfile_A  file_B  file_C"
  },
  {
    "objectID": "week4/w4_3_bonus.html#miscellaneous",
    "href": "week4/w4_3_bonus.html#miscellaneous",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n3.1 More on the && and || operators\nAbove, we saw that we can combine tests in if statements with && and ||. But these shell operators can be used to chain commands together in a more general way, as shown below.\n\nOnly if the first command succeeds, also run the second:\n# Move into the data dir and if that succeeds, then list the files there:\ncd data && ls data\n# Stage all changes =&gt; commit them =&gt; push the commit to remote:\ngit add --all && git commit -m \"Add README\" && git push\nOnly if the first command fails, also run the second:\n# Exit the script if you can't change into the output dir:\ncd \"$outdir\" || exit 1\n# Only create the directory if it doesn't already exist:\n[[ -d \"$outdir\" ]] || mkdir \"$outdir\"\n\n\n\n\n3.2 Parameter expansion to provide default values\nIn scripts, it may be useful to have optional arguments that have a default value if they are not specified on the command line. You can use the following “parameter expansion” syntax for this.\n\nAssign the value of $1 to number_of_lines unless $1 doesn’t exist: in that case, set it to a default value of 10:\nnumber_of_lines=${1:-10}\nSet true as the default value for $3:\nremove_unpaired=${3:-true}\n\nAs a more worked out example, say that your script takes an input dir and an output dir as arguments. But if the output dir is not specified, you want it to be the same as the input dir. You can do that like so:\ninput_dir=$1\noutput_dir=${2:-$input_dir}\nNow you can call the script with or without the second argument, the output dir:\n# Call the script with 2 args: input and output dir\nsort_bam.sh results/bam results/bam\n# Call the script with 1 arg: input dir (which will then also be the output dir)\nsort_bam.sh results/bam\n\n\n\n3.3 Standard output and standard error\nAs you’ve seen, when commands run into errors, they will print error messages. Error messages are not part of “standard out”, but represent a separate output stream: “standard error”.\nWe can see this when we try to list a non-existing directory and try to redirect the output of the ls command to a file:\nls -lhr solutions/ &gt; solution_files.txt \nls: cannot access solutions.txt: No such file or directory\nEvidently, the error was printed to screen rather than redirected to the output file. This is because &gt; only redirects standard out, and not standard error. Was anything at all printed to the file?\ncat solution_files.txt\n# We just get our prompt back - the file is empty\nNo, because there were no files to list, only an error to report.\nThe figure below draws the in- and output streams without redirection (a) versus with &gt; redirection (b):\n\n\n\nFigure from Buffalo.\n\n\nTo redirect the standard error, use 2&gt; 2:\nls -lhr solutions/ &gt; solution_files.txt 2&gt; errors.txt\nTo combine standard out and standard error, use &&gt;:\n# (&&gt; is a bash shortcut for 2&gt;&1)\nls -lhr solutions/ &&gt; out.txt\ncat out.txt\nls: cannot access solutions.txt: No such file or directory\nFinally, if you want to “manually” designate an echo statement to represent standard error instead of standard out in a script, use &gt;&2:\necho \"Error: Invalid line number\" &gt;&2\necho \"Number should be &gt;0 and &lt;= the file's nr. of lines\" &gt;&2\necho \"File contains $(wc -l &lt; $2) lines; you provided $1.\" &gt;&2\nexit 1"
  },
  {
    "objectID": "week4/w4_3_bonus.html#footnotes",
    "href": "week4/w4_3_bonus.html#footnotes",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Or if you’re familiar with Python, they are like Python lists.↩︎\n Note that 1&gt; is the full notation to redirect standard out, and the &gt; we’ve been using is merely a shortcut for that.↩︎"
  },
  {
    "objectID": "week4/w4_exercises.html#exercise-1-a-shell-script-that-prints-a-specific-line",
    "href": "week4/w4_exercises.html#exercise-1-a-shell-script-that-prints-a-specific-line",
    "title": "Week 4 exercises",
    "section": "Exercise 1: A shell script that prints a specific line",
    "text": "Exercise 1: A shell script that prints a specific line\nWrite a shell script scripts/printline.sh that accepts two arguments, a file name and a line number, and prints the requested line from the file to screen. Additional notes:\n\nSince this is a simple utility script, I suggest to make the script not print anything other than the requested line from the file (i.e., no echo statements).\nDon’t forget the best-practice shell script header lines we discussed.\nWith an if statement, let the script check whether the correct number of arguments were passed to it, and if not, exit the script.\nTest your script by printing a couple of different lines from garrigos_data/meta/metadata.tsv. Also test that your argument-number-check works.\n\n\n\nHint 1: an overview of the steps to take (Click to expand)\n\n\nOpen a new text file and save it as scripts/printline.sh.\nIn the script, start with the shebang and set lines.\nYour script takes two arguments: a file name ($1) and a line number ($2) .\nCheck the number of arguments in an if statement like we did in class.\nCopy the $1 and $2 placeholder variables to descriptively named variables.\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the next hint.\n\n\n\n\nHint 2: how to print a specific line number (Click to expand)\n\nFor example, to print line 4 of the Garrigos et al. metadata file:\nhead -n 4 garrigos_data/meta/metadata.tsv | tail -n 1\nERR10802879     10dpi   cathemerium\nHow this command works:\n\nhead -n 4 garrigos_data/meta/metadata.tsv prints the first 4 lines of that file.\nThose 4 lines are then piped into the tail command.\nWith -n 1, tail will only print the last line of its input: this will be line 4 of the original input file."
  },
  {
    "objectID": "week4/w4_exercises.html#exercise-2-a-script-to-run-trimgalore",
    "href": "week4/w4_exercises.html#exercise-2-a-script-to-run-trimgalore",
    "title": "Week 4 exercises",
    "section": "Exercise 2: A script to run TrimGalore",
    "text": "Exercise 2: A script to run TrimGalore\n\nIntroduction to TrimGalore\nTrimGalore is a tool that can trim and filter FASTQ files, removing:\n\nAny adapter sequences that are present in the reads1.\nPoor-quality bases at the start and end of the reads.\nReads that have become very short after the prior two steps.\n\nTrimGalore takes FASTQ files as input, and outputs filtered FASTQ files. When you have paired-end reads (as you do here), a single TrimGalore run should include both the R1 and R2 file for 1 sample.\n\n\n\n\n\n\nMore about TrimGalore (Click to expand)\n\n\n\n\n\nSeveral largely equivalent tools exist for this kind of FASTQ preprocessing — Trimmomatic and fastp are two other commonly used ones. TrimGalore itself is “just” a wrapper around yet another tool called Cutadapt, but it is simpler to use. Two advantages of TrimGalore are:\n\nIt will auto-detect the adapters that are present in your reads.\nIt can automatically run FastQC on the trimmed sequences.\n\n\n\n\nTrimGalore isn’t installed at OSC, but you can use a so-called “Conda environment”2 that I have created for it:\n# First load OSC's (mini)Conda module\nmodule load miniconda3/23.3.1-py310\n# Then load ('activate') the Conda environment with TrimGalore\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\nAfter running those two lines, check that you can run TrimGalore and which version you’re working with, as follows:\n# Note: the command is 'trim_galore' with an underscore\ntrim_galore --version\n            Quality-/Adapter-/RRBS-/Speciality-Trimming\n                    [powered by Cutadapt]\n                        version 0.6.10\nThen print the help info — you’ll be using this below!\ntrim_galore --help\n[output not shown]\n\n\n\nYour TrimGalore shell script\nWrite a shell script scripts/trimgalore.sh that runs TrimGalore on paired end-FASTQ files as follows:\n\nThe script should only run TrimGalore once, i.e. for one sample (two FASTQ files).\nYour script should accept and process arguments that specify the input FASTQ files and the output dir. (For the FASTQ files, I suggest you follow the strategy used in class where the script takes only the R1 file name as an argument3 and infers the name of the corresponding R2 file.)\nFor each of the items below, figure out the relevant TrimGalore option, and use that option:\n\nTell the program that the reads are paired-end4.\nSet the output dir.\nMake the program run FastQC on the trimmed FASTQ files.\n\nCheck what the default values are for the Phred quality score and read length thresholds. Do you understand what these do? You don’t have to change them here, the defaults will work fine for us.\n\n\n\nHint: See the relevant parts of the TrimGalore help pages (Click to expand)\n\n\ntrim_galore --help\n USAGE:\ntrim_galore [options] &lt;filename(s)&gt;\n\n--paired                This option performs length trimming of quality/adapter/RRBS trimmed reads for\n                        paired-end files.\n\n-o/--output_dir &lt;DIR&gt;   If specified all output will be written to this directory instead of the current\n                        directory. If the directory doesn't exist it will be created for you.\n\n-j/--cores INT          Number of cores to be used for trimming [default: 1].\n\n--fastqc                Run FastQC in the default mode on the FastQ file once trimming is complete.\n\n--fastqc_args \"&lt;ARGS&gt;\"  Passes extra arguments to FastQC.\n\n-a/--adapter &lt;STRING&gt;   Adapter sequence to be trimmed. If not specified explicitly, Trim Galore will\n                        try to auto-detect whether the Illumina universal, Nextera transposase or Illumina\n                        small RNA adapter sequence was used.\n\n-q/--quality &lt;INT&gt;      Trim low-quality ends from reads in addition to adapter removal. [...]\n                        Default Phred score: 20.\n\n--length &lt;INT&gt;          Discard reads that became shorter than length INT because of either\n                        quality or adapter trimming. A value of '0' effectively disables\n                        this behaviour. Default: 20 bp.\n\n\n\n\n\n\n\nWarning: Don’t use/adjust TrimGalore’s --cores option here (Click to expand)\n\n\n\n\n\nSince you’re running the program interactively on our single-core VS Code compute job, you only have 1 core at your disposal — and that’s TrimGalore’s default. Next week we’ll be submitting our scripts as batch jobs: that will give us the opportunity to use multiple cores with programs like this."
  },
  {
    "objectID": "week4/w4_exercises.html#exercise-3-run-your-trimgalore-script-once",
    "href": "week4/w4_exercises.html#exercise-3-run-your-trimgalore-script-once",
    "title": "Week 4 exercises",
    "section": "Exercise 3: Run your TrimGalore script once",
    "text": "Exercise 3: Run your TrimGalore script once\n\nCreate a separate (“runner”) script and in it, write code to run your TrimGalore script for just the ERR10802863 sample in garrigos_data/fastq — and run that code.\nCheck if everything went well; also take a look at TrimGalore’s output files, which should include new (filtered) FASTQ files as well as FastQC outputs and a text file for each sample. If something went wrong, go back to your trimgalore.sh script and try to fix it.\nTake a closer look at the output that TrimGalore printed to screen (note that most of that is also saved in the *_trimming_report.txt file in the output dir):\n\nIn each file (R1 and R2), in what percentage of reads were adapters detected?\nIn each file (R1 and R2), what percentage of bases were quality trimmed?\nWhat percentage of reads were removed because they were too short?5"
  },
  {
    "objectID": "week4/w4_exercises.html#exercise-4-run-your-trimgalore-script-for-all-samples",
    "href": "week4/w4_exercises.html#exercise-4-run-your-trimgalore-script-for-all-samples",
    "title": "Week 4 exercises",
    "section": "Exercise 4: Run your TrimGalore script for all samples",
    "text": "Exercise 4: Run your TrimGalore script for all samples\nOnce your single-sample run works, write a for loop in your runner script to run your TrimGalore script on all samples in the garrigos_data/fastq dir, and run that. (Don’t forget to take into account that you should loop over samples or R1 FASTQ files, not over all FASTQ files.)\n\n\n\n\n\n\nThis will take some time (~20 minutes) to run!\n\n\n\nAnd note that these FASTQ files are much smaller than regular ones. You’ll hopefully agree that we need to start using more computing power, and have the script run simultaneously rather than consecutively for each sample — we can do all of that by submitting the script as batch jobs, as we’ll see next week."
  },
  {
    "objectID": "week4/w4_exercises.html#solutions",
    "href": "week4/w4_exercises.html#solutions",
    "title": "Week 4 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\nSolution (Click to expand)\n\n\nHere is the full script scripts/printline.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Check the number of command-line arguments\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printline.sh &lt;file&gt; &lt;line-number&gt;\"\n    exit 1\nfi\n\n# Copy the command-line arguments\ninput_file=$1\nline_nr=$2\n\n# Only print the requested line\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\nTo run the script and make it print the 4th line of the Garrigos et al. metadata file:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv 4\nERR10802879     10dpi   cathemerium\nOr the 7th line:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv 7\nERR10802884     10dpi   control\n\n\n\n\n\nExercise 2\n\n\nCode solution (Click to expand)\n\nThese are the TrimGalore options you were looking for:\n\n--paired to indicate that the FASTQ files are paired-end.\n--output_dir (or equivalently, -o) to specify the output directory.\n--fastqc to run FastQC on the trimmed FASTQ files.\n\nHere is the full script scripts/trimgalore.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Load TrimGalore\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1_in\"\necho \"# Input R2 FASTQ file:      $R2_in\"\necho \"# Output dir:               $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --fastqc \\\n    --output_dir \"$outdir\" \\\n    \"$R1_in\" \\\n    \"$R2_in\"\n\n# Report\necho\necho \"# Done with script trimgalore.sh\"\ndate\n\n\n\nQuality and length threshold options & interpretation (Click to expand)\n\n\nThe option -q (short notation) or --quality (long notation) can be used to adjust the base quality score threshold, which has the “Phred” unit. The default threshold is 20, which corresponds to an estimated 0.01 error rate. TrimGalore will trim bases with a lower quality score than this threshold from the ends of the reads. It will not remove or mask bases with lower quality scores elsewhere in the read.\nThe option -l (short notation) or --length (long notation) can be used to adjust the read length threshold: any reads that are shorter than this after adapter and quality trimming will be removed. The default threshold is 20 bp. (When the input is paired-end, the corresponding second read will also be removed, regardless of its length: paired-end FASTQ files always need to contain both the R1 and R2 for each pair; orphan reads would need to be stored in a separate file.)\n\n\n\n\n\nExercise 3\n\n\nCode solution (Click to expand)\n\n\nCreate a runner script for the code below:\ntouch run/run_exercises.sh\nRun the script for one sample:\nR1=garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/trimgalore.sh \"$R1\" results/trimgalore\n# Starting script trimgalore.sh\nThu Mar 28 10:34:24 EDT 2024\n# Input R1 FASTQ file:      garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 FASTQ file:      garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:               results/trimgalore\n\nMulticore support not enabled. Proceeding with single-core trimming.\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\nCutadapt version: 4.4\n\n# [...output truncated...]\nCheck the output files:\nls -lh results/trimgalore\ntotal 42M\n-rw-rw----+ 1 jelmer PAS0471 2.4K Mar 28 10:49 ERR10802863_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 674K Mar 28 10:49 ERR10802863_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 10:49 ERR10802863_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  20M Mar 28 10:49 ERR10802863_R1_val_1.fq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.3K Mar 28 10:49 ERR10802863_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 676K Mar 28 10:50 ERR10802863_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 341K Mar 28 10:50 ERR10802863_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 10:49 ERR10802863_R2_val_2.fq.gz\n\n\n\n\nTrimming results (Click to expand)\n\n\nNote that the adapter- and quality trimming results summary for the R1 and R2 files are separated widely among all the output that TrimGalore prints — they are printed below (first for the R1, then for the R2 file), and the answers are:\n\n13.0% (R1) and 10.1% (R2) of reads had adapters\n3.6% (R1) and 3.5% (R2) of bases were quality-trimmed\n\n=== Summary ===\n\nTotal reads processed:                 500,000\nReads with adapters:                    65,070 (13.0%)\nReads written (passing filters):       500,000 (100.0%)\n\nTotal basepairs processed:    35,498,138 bp\nQuality-trimmed:               1,274,150 bp (3.6%)\nTotal written (filtered):     34,138,461 bp (96.2%)\n=== Summary ===\n\nTotal reads processed:                 500,000\nReads with adapters:                    50,357 (10.1%)\nReads written (passing filters):       500,000 (100.0%)\n\nTotal basepairs processed:    36,784,563 bp\nQuality-trimmed:               1,283,793 bp (3.5%)\nTotal written (filtered):     35,440,230 bp (96.3%)\nJust before the FastQC logs is a line that reports how many reads were removed due to the length filter — the answer is 6.79%.\nNumber of sequence pairs removed because at least one read was shorter than the length cutoff (20 bp): 33955 (6.79%)\n\n\n\n\n\nExercise 4\n\n\nSolution (Click to expand)\n\n\nAdd this loop code to your runner script (run/run_exercises.sh) and then run the loop:\nfor R1 in garrigos_data/fastq/*_R1.fastq.gz; do\n    bash scripts/trimgalore.sh \"$R1\" results/trimgalore\ndone\n# (Output not shown, same as earlier but should repeat for each sample)\nCheck the output files:\nls -lh results/trimgalore\ntotal 949M\n-rw-rw----+ 1 jelmer PAS0471  20M Mar 28 11:18 ERR10802863_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4K Mar 28 11:17 ERR10802863_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 674K Mar 28 11:18 ERR10802863_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 11:18 ERR10802863_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:18 ERR10802863_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.3K Mar 28 11:18 ERR10802863_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 676K Mar 28 11:18 ERR10802863_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 341K Mar 28 11:18 ERR10802863_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:19 ERR10802864_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.7K Mar 28 11:18 ERR10802864_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 675K Mar 28 11:19 ERR10802864_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 11:19 ERR10802864_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  22M Mar 28 11:19 ERR10802864_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.6K Mar 28 11:19 ERR10802864_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 671K Mar 28 11:19 ERR10802864_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 336K Mar 28 11:19 ERR10802864_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:20 ERR10802865_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.6K Mar 28 11:19 ERR10802865_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 682K Mar 28 11:20 ERR10802865_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 361K Mar 28 11:20 ERR10802865_R1_val_1_fastqc.zip\n[...output truncated...]"
  },
  {
    "objectID": "week4/w4_exercises.html#footnotes",
    "href": "week4/w4_exercises.html#footnotes",
    "title": "Week 4 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Adapters are always added to DNA/cDNA fragments of interest prior to Illumina sequencing, and the last bases of a read can “read-through” into the adapter if the fragment is shorter than the read length.↩︎\n We’ll talk more about Conda environments next week.↩︎\n Alternatively, it could take both R1 and R2 files as arguments.↩︎\n If you don’t do this, TrimGalore will process the two FASTQ files independently.↩︎\n Hint: this info is printed at the very end, and for both files at once.↩︎"
  },
  {
    "objectID": "week5/w5_1_osc.html",
    "href": "week5/w5_1_osc.html",
    "title": "A closer look at OSC",
    "section": "",
    "text": "In this short session, we will touch on some aspects of the Ohio Supercomputer Center (OSC) that we did not talk about during our initial OSC introduction in week 1 of this course."
  },
  {
    "objectID": "week5/w5_1_osc.html#self-study-material-more-on-file-transfer",
    "href": "week5/w5_1_osc.html#self-study-material-more-on-file-transfer",
    "title": "A closer look at OSC",
    "section": "Self-study material: More on file transfer",
    "text": "Self-study material: More on file transfer\n\nRemote transfer commands\nFor small transfers, you can also use a remote transfer command like scp, or a more advanced one like rsync or rclone. Such commands can provide a more convenient transfer method than OnDemand if you want to keep certain directories synced between OSC and your computer. The reason you shouldn’t use this for very large transfers is that the transfer will happen using a login node.\n\nscp\nOne option is scp (secure copy), which works much like the regular cp command, including that you’ll need -r for recursive transfers.\nThe key difference is that we have to somehow refer to a path on a remote computer, and we do so by starting with the remote computer’s address, followed by :, and then the path:\n# Copy from remote (OSC) to local (your computer):\nscp &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt; &lt;local-path&gt;\n\n# Copy from local (your computer) to remote (OSC)\nscp &lt;local-path&gt; &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt;\nHere are two examples of copying from OSC to your local computer:\n# Copy a file from OSC to a local computer - namely, to your current working dir ('.'):\nscp jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts/misc/fastqc.sh .\n\n# Copy a directory from OSC to a local computer - namely, to your home dir ('~'):\nscp -r jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts ~\nAnd two examples of copying from your local computer to OSC:\n# Copy a file from your computer to OSC --\n# namely, a file in from your current working dir to your home dir at OSC:\nscp fastqc.sh jelmer@pitzer.osc.edu:~\n\n# Copy a file from my local computer's Desktop to the Scratch dir for PAS0471:\nscp /Users/poelstra.1/Desktop/fastqc.sh jelmer@pitzer.osc.edu:/fs/scratch/PAS0471\nSome nuances for remote copying:\n\nAs the above code implies, in both cases (remote-to-local and local-to-remote), you will issue the copying commands from your local computer.\nFor the remote computer (OSC), the path should always be absolute, whereas that for your local computer can be either relative or absolute.\nSince all files can be accessed at the same paths at Pitzer and at Owens, it doesn’t matter whether you use @pitzer.osc.edu or @owens.osc.edu in the scp command.\n\n\n\n\n\n\n\nTransferring directly to and from OneDrive\n\n\n\nIf your OneDrive is mounted on or synced to your local computer (i.e., if you can see it in your computer’s file brower), you can also transfer directly between OSC and OneDrive.\nFor example, the path to my OneDrive files on my laptop is:\n/Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity.\nSo if I had a file called fastqc.sh in my top-level OneDrive dir, I could transfer it to my Home dir at OSC as follows:\nscp /Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity jelmer@pitzer.osc.edu:~\n\n\n\n\n\nrsync\nAnother option, which I can recommend, is the rsync command, especially when you have directories that you repeatedly want to sync: rsync won’t copy any files that are identical in source and destination.\nA useful combination of options is -avz --progress:\n\n-a enables archival mode (among other things, this makes it work recursively).\n-v increases verbosity — tells you what is being copied.\n-z enables compressed file transfer (=&gt; generally faster).\n--progress to show transfer progress for individual files.\n\nThe way to refer to remote paths is the same as with scp. For example, I could copy a dir_with_results in my local Home dir to my OSC Home dir as follows:\nrsync -avz --progress ~/dir_with_results jelmer@owens.osc.edu:~\n\n\n\n\n\n\n\nTrailing slashes in rsync (Click to expand)\n\n\n\n\n\nOne tricky aspect of using rsync is that the presence/absence of a trailing slash for source directories makes a difference for its behavior. The following commands work as intended — to create a backup copy of a scripts dir inside a dir called backup4:\n# With trailing slash: copy the *contents* of source \"scripts\" into target \"scripts\":\nrsync -avz scripts/ backup/scripts\n\n# Without trailing slash: copy the source dir \"scripts\" into target dir \"backup\"\nrsync -avz scripts backup\nBut these commands don’t:\n# This would result in a dir 'backup/scripts/scripts':\nrsync -avz scripts backup/scripts\n\n# This would copy the files in \"scripts\" straight into \"backup\":\nrsync -avz scripts/ backup\n\n\n\n\n\n\n\nSFTP\nThe first of two options for larger transfers is SFTP. You can use the sftp command when you have access to a Unix shell on your computer, and this what I’ll cover below.\n\n\n\n\n\n\nSFTP with a GUI\n\n\n\nIf you have Windows without e.g. WSL or Git Bash, you can use a GUI-based SFTP client instead like WinSCP, Cyberduck, or FileZilla. CyberDuck also works on Mac, and FileZilla works on all operating systems, if you prefer to do SFTP transfers with a GUI, but I won’t cover their usage here.\n\n\n\nLogging in\nTo log in to OSC’s SFTP server, issue the following command in your local computer’s terminal, substituting &lt;user&gt; by your OSC username:\nsftp &lt;user&gt;@sftp.osc.edu   # E.g., 'jelmer@sftp.osc.edu'\nThe authenticity of host 'sftp.osc.edu (192.148.247.136)' can't be established.\nED25519 key fingerprint is SHA256:kMeb1PVZ1XVDEe2QiSumbM33w0SkvBJ4xeD18a/L0eQ.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nIf this is your first time connecting to OSC SFTP server, you’ll get a message like the one shown above: you should type yes to confirm.\nThen, you may be asked for your OSC password, and after that, you should see a “welcome” message like this:\n******************************************************************************\n\nThis system is for the use of authorized users only.  Individuals using\nthis computer system without authority, or in excess of their authority,\nare subject to having all of their activities on this system monitored\nand recorded by system personnel.  In the course of monitoring individuals\nimproperly using this system, or in the course of system maintenance,\nthe activities of authorized users may also be monitored.  Anyone using\nthis system expressly consents to such monitoring and is advised that if\nsuch monitoring reveals possible evidence of criminal activity, system\npersonnel may provide the evidence of such monitoring to law enforcement\nofficials.\n\n******************************************************************************\nConnected to sftp.osc.edu.\nNow, you will have an sftp prompt (sftp&gt;) instead of a regular shell prompt.\nFamiliar commands like ls, cd, and pwd will operate on the remote computer (OSC, in this case), and there are local counterparts for them: lls, lcd, lpwd — for example:\n# NOTE: I am prefacing sftp commands with the 'sftp&gt;' prompt to make it explicit\n#       these should be issued in an sftp session; but don't type that part.\nsftp&gt; pwd\nRemote working directory: /users/PAS0471/jelmer\nsftp&gt; lpwd\nLocal working directory: /Users/poelstra.1/Desktop\n\n\n\nUploading files to OSC\nTo upload files to OSC, use sftp’s put command.\nThe syntax is put &lt;local-path&gt; &lt;remote-path&gt;, and unlike with scp etc., you don’t need to include the address to the remote (because in an stfp session, you are simultaneously connected to both computers). But like with cp and scp, you’ll need the -r flag for recursive transfers, i.e. transferring a directory and its contents.\n# Upload fastqc.sh in a dir 'scripts' on your local computer to the PAS0471 Scratch dir:\nsftp&gt; put scripts/fastqc.sh /fs/scratch/PAS0471/sandbox\n\n# Use -r to transfer directories:\nsftp&gt; put -r scripts /fs/scratch/PAS0471/sandbox\n\n# You can use wildcards to upload multiple files:\nsftp&gt; put scripts/*sh /fs/scratch/PAS0471/sandbox\n\n\n\n\n\n\nsftp is rather primitive\n\n\n\nThe ~ shortcut to your Home directory does not work in sftp!\nsftp is generally quite primitive and you also cannot use, for example, tab completion or the recalling of previous commands with the up arrow.\n\n\n\n\n\nDownloading files from OSC\nTo download files from OSC, use the get command, which has the syntax get &lt;remote-path&gt; &lt;local-path&gt; (this is the other way around from put in that the remote path comes first, but the same in that both use the order &lt;source&gt; &lt;target&gt;, like cp and so on).\nFor example:\nsftp&gt; get /fs/scratch/PAS0471/mcic-scripts/misc/fastqc.sh .\n\nsftp&gt; get -r /fs/scratch/PAS0471/sandbox/ .\n\n\n\nClosing the SFTP connection\nWhen you’re done, you can type exit or press Ctrl+D to exit the sftp prompt.\n\n\n\n\nGlobus\nThe second option for large transfers is Globus, which has a browser-based GUI, and is especially your best bet for very large transfers. Some advantages of using Globus are that:\n\nIt checks whether all files were transferred correctly and completely\nIt can pause and resume automatically when you e.g. turn off your computer for a while\nIt can be used to share files from OSC directly with collaborators even at different institutions.\n\nGlobus does need some setup, including the installation of a piece of software that will run in the background on your computer.\n\nGlobus installation and configuration instructions: Windows / Mac / Linux\nGlobus transfer instructions\nOSC’s page on Globus"
  },
  {
    "objectID": "week5/w5_1_osc.html#footnotes",
    "href": "week5/w5_1_osc.html#footnotes",
    "title": "A closer look at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And file sharing / collaborating is also a bit more difficult with home dirs.↩︎\nCopying of files back-and-forth, and making sure your results are not lost upon some kind of failure.↩︎\nBut the initial setup for Globus is quite involved and a bit counter-intuitive.↩︎\nFor simplicity, these commands are copying between local dirs, which is also possible with rsync.↩︎"
  },
  {
    "objectID": "week5/w5_2_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "week5/w5_2_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Using software at OSC",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its sub-commands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module commands:\n\nmodule spider lists all installed modules.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail spits out the complete lists of installed/available programs — it is more useful to add a search term as an argument. Below, we’ll search for the Conda distribution “miniconda”:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n\n\n\nBoth of these search commands are case-insensitive, but module load (below) is not\n\n\n\n\n\n\n\n\n\n1.2 Loading and unloading software\nAll other Lmod software functionality is also accessed using module commands. For instance, to make a program available to us we use the load command:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nIt can be good to specify the version even when you want the default (Click to expand)\n\n\n\n\n\nWhen we use the module load command inside a script, always specifying a version would:\n\nEnsure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to reload any modules you want to use!\n\n\nTo check which modules are loaded, use module list. Its output also includes automatically loaded modules — for example, after loading miniconda3/23.3.1-py310, it should list miniconda3 as the 9th entry2:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do with module unload or module purge:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n\n\n\n\nAlways include the module load command in your shell script\n\n\n\nWhen you run a program that is loaded with Lmod in your shell script, always include the module load command in the script, best way at the top:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc/0.11.8\n\n\n\n Exercise: Load a BLAST module\nBLAST is a very widely used alignment tool, often used to identify sequences that are similar to a query sequence. There is not just a web version on NCBI’s website, but also a command-line tool.\n\nUse module avail to check if BLAST is installed at OSC, and if so, which versions. (Note: you’ll also see results for the module blast-database — ignore those.)\nLoad the default BLAST version by not specifying a version, and then check which version was loaded and if that matches the module avail output.\nLoad the latest version of BLAST without unloading the earlier version first. What output do you get?\n\n\n\nClick here for the solutions\n\n\nCheck the BLAST modules:\nmodule avail BLAST\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core -- --------------------------------------------------------------------------------\n   blast-database/2018-08 (D)    blast-database/2020-04    blast-database/2022-06    blast/2.8.0+         blast/2.11.0+\n   blast-database/2019-09        blast-database/2021-05    blast-database/2023-06    blast/2.10.0+ (D)    blast/2.13.0+\n\n  Where:\n   D:  Default Module\nLoad the default version:\nmodule load BLAST\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3  10) blast/2.10.0+\nblast/2.10.0+ was loaded, which matches the module avail output.\nLoad the latest version:\nmodule load blast/2.13.0+\nThe following have been reloaded with a version change:\n  1) blast/2.10.0+ =&gt; blast/2.13.0+\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it.\n\n\n\n\n Bonus exercise: STAR and module availability\n\nUse module spider to check which versions of STAR, an RNA-seq read alignment program, have been installed at OSC. Compare this output with that of module avail.\nTry to load the most recent version of STAR that module spider listed (this should fail).\nFollow the instructions in the error message to again try and load OSC’s most recent version of STAR.\nSearch the internet to see what the most recent version of STAR is.\n\n\n\nClick here for the solutions\n\n\nCheck the versions of STAR:\nmodule spider star\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star:\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n     Versions:\n        star/2.5.2a\n        star/2.7.9a\nmodule avail star\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core    ----------------------------------------------------------------------------------\n   star/2.5.2a\nFirst attempt to load the most recent one:\nmodule load star/2.7.9a\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"star/2.7.9a\"\n   Try: \"module spider star/2.7.9a\" to see how to load the module(s).\nFollow the instructions to try and load it again:\nmodule spider star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star: star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"star/2.7.9a\" module is available to load.\n\n      gnu/10.3.0\nmodule load gnu/10.3.0\nLmod is automatically replacing \"intel/19.0.5\" with \"gnu/10.3.0\".\n\nThe following have been reloaded with a version change:\n  1) mvapich2/2.3.3 =&gt; mvapich2/2.3.6\nmodule load star/2.7.9a\nThe last command prints no output, which is generally good news.\nSTAR --version\n2.7.9a\nMost recent version of STAR:\nAs of March 2024, it looks like that’s version 2.7.11b (https://github.com/alexdobin/STAR)."
  },
  {
    "objectID": "week5/w5_2_software.html#when-software-isnt-installed-at-osc",
    "href": "week5/w5_2_software.html#when-software-isnt-installed-at-osc",
    "title": "Using software at OSC",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software “environments” you can activate much like we did with Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. Docker containers are most well-known, but OSC uses Apptainer (formerly known as Singularity).\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible software environments. They also make it easy to access different versions of the same software, or use mutually incompatible software.\nIn this session, you will learn how to use Conda, and the self-study reading at the bottom of the page covers using containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”3 and will often have difficulties with “dependencies”4."
  },
  {
    "objectID": "week5/w5_2_software.html#conda-basics",
    "href": "week5/w5_2_software.html#conda-basics",
    "title": "Using software at OSC",
    "section": "3 Conda basics",
    "text": "3 Conda basics\nThe Conda software can create so-called environments in which you can install one or more software packages.\nAs you’ll learn below, as long as a program is available in one of the online Conda repositories (and this is nearly always the case for open-source bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nA Conda environment is “just” a directory that includes the executable (binary) files for the program(s) in question. I have a collection of Conda environments that anyone can use, and we can list these environments simply with ls:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  clonalframeml        kraken2            picard                        salmon\nagat-0.9.1      codan-1.2            kraken-biom        pilon-1.24                    samtools\nalv             cogclassifier        krona              pkgs                          scoary\namrfinderplus   cutadapt             liftoff-1.6.3      plasmidfinder-2.1.6           seqkit\nantismash       deeploc              links-2.0.1        plink2                        seqtk\nariba-2.14.6    deeptmhmm            lissero            porechop                      shoot\nastral-5.7.8    deeptmhmm2           longstitch-1.0.3   prokka                        signalp-6.0\naswcli          diamond              mafft              pseudofinder                  sistr-1.1.1\nbactopia        dwgsim               maskrc-svg         purge_dups-1.2.6              smap\nbactopia3       eggnogmapper         mbar24             pycoqc-2.5.2                  smap_dev\nbactopia-dev    emboss               medaka-1.7.2       qiime2-2023.7                 smartdenovo-env\nbakta           entap-0.10.8         metaxa-2.2.3       qiime2-amplicon-2024.2        snippy-4.6.0\nbase            entrez-direct        methylpy           qualimap-env                  snpeff\nbbmap           evigene              minibusco          quast-5.0.2                   snp-sites-2.5.1\nbcftools        fastp                minimap2-2.24      quickmerge-env                soapdenovo-trans-1.0.4\nbedops          fastqc               mlst               racon-1.5.0                   sortmerna-env\nbedtools        fastq-dl             mlst_check         ragtag-2.1.0                  sourmash\nbioawk          fasttree-2.1.11      mobsuite           rascaf                        spades-3.15.5\nbioconvert      filtlong-env         multiqc            rcorrector-1.0.5              sra-tools\nbiopython       flye-2.9.1           mummer4            r-dartr                       star\nbit             fmlrc2-0.1.7         muscle             r-deseq                       subread-2.0.1\nblast           gcta                 nanolyse-1.2.1     recognizer-1.8.3              taxonkit\nbowtie1         geofetch             nanoplot           repeatmasker-4.1.2.p1         tgsgapcloser\nbowtie2         gffread-0.12.7       nanopolish-0.13.2  repeatmodeler-2.0.3           tracy-0.7.1\nbracken         gget                 ncbi-datasets      resfinder                     transabyss-2.0.1\nbraker2-env     gubbins              nextdenovo-env     resistomeanalyzer-2018.09.06  transdecoder-5.5.0\nbusco           hisat2               nextflow           rgi-5.2.1                     treetime\nbusco2          hmmer                nextflow-22.10     r-metabar                     trimgalore\nbusco3          interproscan-5.55    nf-core            rnaquast-2.2.1                trimmomatic-0.39\nbwa-0.7.17      iqtree               orna-2.0           roary-3.13                    trinity-2.13.2\ncabana          justorthologs-0.0.2  orthofinder        r-rnaseq                      unicycler\ncactus          kallisto-0.48.0      orthofisher        rsem-1.3.3                    virema\ncgmlst          kat-2.4.2            panaroo            rseqc-env                     virulencefinder\ncheckm-1.2.0    knsp-3.1             parsnp             r_tree                        wtdbg-2.5\nclinker         kofamscan            phylofisher        sabre-1.0\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program, and the environment is named after that program. (The naming of these environments is unfortunately not entirely consistent: many environments include the version number of the program, but others do not. For environments without version numbers, I try to have them contain the most recent version of a software5.)\n\n\n3.1 Activating Conda environments\nBefore you can activate Conda environments, you always first need to load OSC’s Miniconda module:\nmodule load miniconda3/23.3.1-py310\nAs mentioned above, these environments are (de)activated much like with the Lmod system. But while the term “load” is used for Lmod modules, the term “activate” is used for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several sub-commands (deactivate, create, install, update). For example, to activate an environment:\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(multiqc) [jelmer@p0085 rnaseq-intro]$\n\n\n\n\n\n\nConda environment indicator!\n\n\n\nWhen we have an active Conda environment, its name is displayed in front of our prompt, as depicted above with (multiqc).\n\n\nAfter you have activated the MultiQC environment, you should be able to use the program. To test this, simply run the multiqc command with the --help option like we did with FastQC:\nmultiqc --help\n /// MultiQC 🔍 | v1.17\n                                                                                              \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]\n \n MultiQC aggregates results from bioinformatics analyses across many samples into a    \n single report.                                                                        \n[...output truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another, you will switch environments:\n# After running this command, the multiqc env will be active\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# After running his command, the trimgalore env will be active...\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# ...but the multiqc env will no longer be:\nmultiqc --help\nbash: multiqc: command not found...\n\n\n\n\n\n\n\nThe --stack option\n\n\n\nThe conda activate --stack option does enable you to have multiple Conda environments active at once:\n# Assuming you had trimgalore activated, now add the multiqc env:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\n\nmultiqc --help\n# (Output not shown, but this should print help info)\n\ntrim_galore --help\n# (Output not shown, but this should print help info)\n\n\n\n\n\n3.2 Lines to add to your shell script\nAlso like Lmod modules, you’ll have to load Conda environments in every shell session that you want to use them, they don’t automatically reload.\nConda environments loaded in your interactive shell environment do “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch). However, it is good practice to always include the necessary code to load/activate programs in your shell scripts:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\nProblems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment.\nTherefore, it is generally a good idea to not have any Conda environments active in your interactive shell when submitting batch jobs6. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "week5/w5_2_software.html#creating-your-own-conda-environments",
    "href": "week5/w5_2_software.html#creating-your-own-conda-environments",
    "title": "Using software at OSC",
    "section": "4 Creating your own Conda environments",
    "text": "4 Creating your own Conda environments\n\n4.1 One-time Conda configuration\nBefore you can create our own environments, you first have to do some one-time configuration7. The configuration will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config sub-command — run the following in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n4.2 Example: Creating an environment for TrimGalore\nTo practice using Conda, we will now create a Conda environment with the program TrimGalore installed, which you used in last week’s exercises, and which does not have a system-wide installation at OSC. Here is the command to all at once create a new Conda environment and install TrimGalore into that environment:\n# [Don't run this - we'll modify this a bit below]\nconda create -y -n trim-galore -c bioconda trim-galore\nLet’s break that command down:\n\ncreate is the Conda sub-command to create a new environment.\nWhen adding -y, Conda will not ask us for confirmation to install.\nFollowing the -n option, you can specify the name you would like the environment to have: we used trim-galore. You can use whatever name you like for the environment, but a descriptive yet concise name is a good idea. For single-program environments, it makes sense to simply name it after the program.\nThe -c option is to specify a “channel” (repository) from which to install, here bioconda8.\nThe trim-galore argument at the end of the line simply tells Conda to install the package of that name.\n\n\nBy default, Conda will install the latest available version of a program. If you create an entirely new environment for a program, like we’re doing here, that default should always apply — but if you’re installing into an environment that already contains programs, it’s possible that due to compatibility issues, it will install a different version.\nIf you want to be explicit about the version you want to install, add the version number after = following the package name, and you may then also want to include that version number in the Conda environment’s name — try this:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\nThere should be a lot of output, with many packages that are being downloaded (these are all “dependencies” of TrimGalore), but if it works, you should see this before you get your prompt back:\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done                                                                                                                   \n#                        \n# To activate this environment, use                          \n#\n#     $ conda activate trim-galore-0.6.10                          \n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\nNow, you should be able to activate the environment (using just its name – see the box below):\nconda activate trim-galore\nLet’s test if we can run TrimGalore — note, the command is trim_galore:\ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\n\nSpecifying the full path to the environment dir (Click to expand)\n\n\n\n\n\nYou may have noticed above that we merely gave the environment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option — for example:\n# [Don't run this]\nconda create -y -p /fs/scratch/PAS2700/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading one of my Conda environments above.\n\n\n\n\n\n\n4.3 Finding Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its Conda package’s name is\nWhich Conda channel we should use\nWhich versions are available\n\nMy strategy to finding this out is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the CutAdapt program. Let’s see that in action:\n\n\n\n\n\nClick on that first link (in my experience, it is always the first Google hit):\n\n\n\n\n\n\n\n\n4.4 Building the installation command from the online info\nYou can take the top of the two example installation commands as a template, here: conda install -c bioconda cutadapt. You may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here is to create a separate environment for each program, just installing a program into whatever environment is currently active is not a great idea.\nYou can use the install command with a new environment, but then you would first have to create an “empty” environment, and then run the install command. However, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version of the software will be installed by default, and to see which older versions are available:\n\n\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nMore Conda commands to manage your environments (Click to expand)\n\n\n\n\n\n\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\n\n\n\n\n\n\n\n4.5 Organizing your Conda environments\nThere are two reasonable alternative way to organize your Conda environments:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later."
  },
  {
    "objectID": "week5/w5_2_software.html#self-study-using-apptainer-containers",
    "href": "week5/w5_2_software.html#self-study-using-apptainer-containers",
    "title": "Using software at OSC",
    "section": "5 Self-study: Using Apptainer containers",
    "text": "5 Self-study: Using Apptainer containers\nContainers are an alternative to Conda to use programs that don’t have system-wide installations at OSC.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, pre-existing container images are available for most bioinformatics programs, and these can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\n\n5.1 Finding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers (https://biocontainers.pro/registry) or Quay.io (https://quay.io/biocontainers).\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nThe website also includes Conda installation instructions — to see the container results, scroll down to:\n\n\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image.\nNote that the command shown is singularity run, but we will use the more up-to-date apptainer run.\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\nWhenever you find both a Singularity/Apptainer and a Docker image for your program, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\n5.2 Running a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of March 2024: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways, using:\n\nThe more up-to-date apptainer command9\nThe exec subcommand instead of run, allowing us to enter a custom command to run in the container10.\n\nAs such, our “base” command to run TrimGalore in the container will be as follows:\n# [Don't run this, we'll need to add a TrimGalore command]\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n\n\n\n\n\n\nNeed to use a Docker container? You can’t use the Docker URL as-is. (Click to expand)\n\n\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with “quay.io”. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\n\n\n\n\n\n\nSo, all that is different from running a program inside a container instead of a a locally installed program, is that you prefix your command with apptainer exec &lt;URL&gt;.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\n(You will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.)\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environment or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "week5/w5_2_software.html#footnotes",
    "href": "week5/w5_2_software.html#footnotes",
    "title": "Using software at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And with Git we saw another kind of behavior, where the automatically available version is very old, but we can load a more recent version.↩︎\n This may vary over time and also depends on whether you run this in the VS Code Server terminal — some of the loaded modules are related to that.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\n It isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\n Unless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\n Given that you’ve done some config above, this is not always necessary, but it can be good to be explicit.↩︎\nThough note that as of March 2024, the singularity command does still work, and it will probably continue to work for a while.↩︎\n The run subcommand would only run some preset default action, which is rarely useful for our purposes.↩︎"
  },
  {
    "objectID": "week5/w5_3_slurm.html#basics-of-slurm-batch-jobs",
    "href": "week5/w5_3_slurm.html#basics-of-slurm-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "1 Basics of Slurm batch jobs",
    "text": "1 Basics of Slurm batch jobs\nWhen you request a batch job, you ask the Slurm scheduler to run a script “out of sight” on a compute node. While that script will run on a compute node, your shell prompt stays in your current shell at your current node regardless of whether that is a login or compute node.\nWith batch jobs, note that:\n\nAfter submitting a batch job, it will continue running even if we log off from OSC or shut down our computer.\nScript output that would normally be printed to screen will end up in a text file.\nThe Slurm scheduler has commands not only to submit jobs but also to monitor and cancel jobs.\n\n\n\n1.1 The sbatch command\nYou can use Slurm’s sbatch command to submit a batch job. But first, recall from last week that you can run a Bash script as follows:\nbash sandbox/printname.sh Jane Doe\nFirst name: Jane\nLast name: Doe\nThe above command ran the script on our current node. To instead submit the script to the Slurm queue, simply replace bash by sbatch:\nsbatch sandbox/printname.sh Jane Doe\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\nHowever, as the above error message “Must specify account for job” tells us, we need to indicate which OSC Project (or as Slurm puts it, “account”) we want to use for this compute job. Use the --account= option to sbatch to do this:\nsbatch --account=PAS0471 sandbox/printname.sh Jane Doe\nSubmitted batch job 12431935\nThis output means that our job was successfully submitted (no further output will be printed to your screen — we’ll talk more about that below). The job has a unique identifier among all compute jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\nAs you perhaps noticed in the command above, we can use sbatch options and script arguments in one command, in the following order:\nsbatch [sbatch-options] myscript.sh [script-arguments]\nBut, depending on the details of the script itself, all combinations of using sbatch options and script arguments are possible:\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS0471 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS0471 printname.sh Jane Doe  # Both sbatch option and script arguments\n(Omitting the --account option is possible when we specify this option inside the script, as we’ll see below.)\n\n\n\n\n\n1.2 Adding sbatch options in scripts\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified. Defaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1 core).\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script. This is handy because you often want to specify several options, and this can lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself, so you don’t have to remember them.\nWe add the options in the script using another type of special comment line akin to the shebang (#!/bin/bash) line, marked by #SBATCH. Just like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n#!/bin/bash\n#SBATCH --account=PAS0471\n\nset -euo pipefail\nSo, the equivalent of adding --account=PAS0471 after sbatch on the command line is a line in your script that reads #SBATCH --account=PAS0471.\nAfter adding this to the script, we are now able to run the sbatch command without options (which failed earlier):\nsbatch printname.sh Jane Doe\nSubmitted batch job 12431942\nTODO CLEAN SLURM LOG FILES\nAfter we submit the batch job, we immediately get our prompt back. The job will run outside of our immediate view, and we can continuee doing other things while it does — or log off. This behavior allows us to submit many jobs at the same time — we don’t have to wait for other jobs to finish (or even to start).\n\n\n\n\n\n\nsbatch option precedence!\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH lines in non-Slurm contexts (Click to expand)\n\n\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n\n1.3 Where does the script’s output go?\nAbove, you saw that when you ran printname.sh directly with bash, its output was printed to the screen, whereas when you submitted it as a batch job, only Submitted batch job was printed to screen. Where did our output go?\nThe output ended up in a file called slurm-12431942.out (i.e., slurm-&lt;job-number&gt;.out; since each job number is unique to a given job, your file would have a different number in its name). We will call this type of file a Slurm log file.\n\n\nAny idea why we may not want batch job output printed to screen, even if it was possible? (Click for the answer)\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\nLet’s take a look at the contents of the Slurm log file:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431942.out\nFirst name: Jane  \nLast name: Doe\nThis file contains the script’s output that was printed to screen when we ran it with bash — nothing more or less.\nIt’s important to conceptually distinguish between two overall types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, and that ends up in the Slurm log file when we submit the script as a batch job. This includes output produced by echo statements, by any errors that may occur, and logging output by any program that we run in the script.\nOutput of commands inside the script that is redirected to a file (&gt; myfile.txt) or that a program writes to an output file. This type of output will end up in the exact same files regardless of whether we run the script directly (with bash) or as a batch job (with sbatch).\n\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same."
  },
  {
    "objectID": "week5/w5_3_slurm.html#monitoring-batch-jobs",
    "href": "week5/w5_3_slurm.html#monitoring-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "2 Monitoring batch jobs",
    "text": "2 Monitoring batch jobs\n\n2.1 A sleepy script for practice\nLet’s use the following short script to practice monitoring and managing batch jobs. Create a file for it:\ntouch sandbox/sleep.sh\nOpen the file in the VS Code editor and copy the following into it:\n#!/bin/bash\n#SBATCH --account=PAS0471\n\necho \"I will sleep for 30 seconds\" &gt; sleep.txt\nsleep 30s\necho \"I'm awake! Done with script sleep.sh\"\n\n\n Exercise: Batch job output recap\nPredict what would happen if you submit the sleep.sh script as a batch job using sbatch sandbox/sleep.sh:\n\nHow many output files will this batch job produce?\nWhat will be in each of those files?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, using the command bash sandbox/sleep.sh?\n\nThen, test your predictions by running the script.\n\n\nClick for the solutions\n\n\nThe job will produce 2 files:\n\nslurm-&lt;job-number&gt;.out: The Slurm log file, containing output normally printed to screen.\nsleep.txt: Containing output that was redirected to this file in the script.\n\nThe those files will contain the following:\n\nslurm-&lt;job-number&gt;.out: I’m awake! Done with script sleep.sh\nsleep.txt: “I will sleep for 30 seconds”\n\nBoth files will end up in your current working directory. Slurm log files always go to the directory from which you submitted the job. Slurm jobs also run from the directory from which you submitted your job, and since we redirected the output simply to sleep.txt, that file was created in our working directory.\nIf we had run the script directly, sleep.txt would have also been created with the same content, but “All done!” would have been printed to screen.\n\n\n\n\n\n\n2.2 Checking the job’s status\nAfter you submit a job, it may be initially be waiting to be allocated resources: i.e., it may be queued (“pending”). Then, the job will start running — you’ve seen all of this with the VS Code Interactive App job as well.\nWhereas Interactive App jobs will keep running until they’ve reached the end of the allocated time1, batch jobs will stop as soon as the script has finished, either successfully or due to an error. And if the script is still running when the job runs out of its allocated time, it will be killed (stopped) right away.\n\nThe squeue command\nYou can check the status of your batch job using the squeue Slurm command:\nsqueue -u $USER -l\nThu Apr 4 15:47:51 2023\n        JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n     23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\nIn the command above:\n\nYou specify your username with the -u option (without this, you’d see everyone’s jobs!).\nIn this example, I used the environment variable $USER to get your user name, just so that the very same code will work for everyone (you can also simply type your username if that’s shorter or easier).\nThe option -l (lowercase L, not the number 1) will produce more verbose (“long”) output.\n\nIn the output, after a line with the date and time, and a header line, you should see information about a single compute job, as shown above: this is the Interactive App job that runs VS Code. That’s not a “batch” job, but it is a compute job, and all compute jobs are listed here.\nThe following pieces of information about each job are listed:\n\nJOBID — The job ID number\nPARTITION — The type of queue\nNAME — The name of the job\nUSER — The user name of the user who submitted the job\nSTATE — The job’s state, usually PENDING (queued) or RUNNING. Finished jobs do not appear on the list.\nTIME — For how long the job has been running (here as minutes:seconds)\nTIME_LIMIT — the amount of time you reserved for the job (here as hours:minutes:seconds)\nNODES — The number of nodes reserved for the job\nNODELIST(REASON) — When running: the ID of the node on which it is running. When pending: why it is pending.\n\n\n\n\nsqueue example\nNow, let’s see a batch job in the squeue listing. Start by submitting the sleep.sh script as a batch job:\nsbatch sandbox/sleep.sh\nSubmitted batch job 12431945\nIf we’re quick enough, we may be able to catch the STATE as PENDING before the job starts:\nsqueue -u $USER -l\nThu Apr 4 15:48:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n      23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\nThu Apr 4 15:48:39 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n      23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\nThe script should finish after 30 seconds (because your command was sleep 30s), after which the job will immediately disappear from the squeue listing — only pending and running jobs are shown:\nsqueue -u $USER -l\nMon Aug 21 15:49:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\n\n\n\nChecking the output files\nWhenever you’re running a script as a batch job, even if you’ve been monitoring it with squeue, you should also make sure it ran successfully. You typically do so by checking the output file(s). As is usual also when submitting scripts that run bioinformatics programs, you’ll have two types of output from a batch job:\n\nFile(s) directly created by the command inside the script (here, sleep.sh).\nA Slurm log file with the script’s standard output and standard error (i.e. output that is normally printed to screen).\n\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-12520046.out\nI'm awake! Done with script sleep.sh\nText will be added to the Slurm log file in real time as the running script (or the program ran by the script) outputs it. However, the output that commands like cat and less print are static. Therefore, if you find yourself opening/printing the contents of the Slurm log file again and again to keep track of progress, then instead use tail -f, which will “follow” the file and will print new text as it’s added to the Slurm log file:\n# See the last lines of the file, with new contents added in real time\ntail -f slurm-12520046.out\nTo exit the tail -f livestream, press Ctrl+C.\n\n\n\n\n2.3 Cancelling jobs\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script, or because you used the wrong input files as arguments. You can do so using scancel:\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your running and queued jobs (careful with this!)\n\n\n\n\n\n\n\nAdditional job management commands and options (Click to expand)\n\n\n\n\n\n\nOnly show running (not pending) jobs:\nsqueue -u $USER -t RUNNING\nYou can see more details about any running or finished job, including the amount of time it ran for:\nscontrol show job &lt;jobID&gt;\nUserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\nPriority=200005206 Nice=0 Account=pas0471 QOS=pitzer-default\nJobState=RUNNING Reason=None Dependency=(null)\nRequeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\nRunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\nSubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\nAccrueTime=2020-12-14T14:32:44\nStartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\nPartition=serial-40core AllocNode:Sid=pitzer-login01:57954\n[...]\nUpdate directives for a job that has already been submitted (this can only be done before the job has started running):\nscontrol update job=&lt;jobID&gt; timeLimit=5:00:00\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\nscontrol hold &lt;jobID&gt;       # Job won't start running until released\nscontrol release &lt;jobID&gt;    # Job is free to start"
  },
  {
    "objectID": "week5/w5_3_slurm.html#common-sbatch-options",
    "href": "week5/w5_3_slurm.html#common-sbatch-options",
    "title": "Slurm batch jobs at OSC",
    "section": "3 Common sbatch options",
    "text": "3 Common sbatch options\nFirst, note that many Slurm options have a corresponding long (--account=PAS0471) and short format (-A PAS0471), and these can generally be used interchangeably. For clarity, we’ll stick to long format options here.\n\n3.1 --account: The OSC project\nAs seen above. When submitting a batch job, always specify the OSC project (“account”).\n\n\n3.2 --time: Time limit (“wall time”)\nUse the --time option to specify the maximum amount of time your job will run for. Your job will be killed (stopped) as soon as it hits the specified time limit! Some notes:\n\nYou will only be charged for the time your job actually used, not what you reserved.\nWall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes (e.g. 60 =&gt; 60 minutes)\nhours:minutes:seconds (e.g. 1:00:00 =&gt; 60 minutes)\ndays-hours (e.g. 2-12 =&gt; two-and-a-half days)\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\nAn example, where we ask for 2 hours in the “minute-format”:\n#!/bin/bash\n#SBATCH --time=120\nOr for 12 hours in the “hour-format”:\n#!/bin/bash\n#SBATCH --time=12:00:00\n\n\n\n\n\n\nWhen in doubt, ask for more time\n\n\n\nIf you are uncertain about how much time your job will take (i.e., how long it will take for your script / the program in your script to finish), then ask for more (or much more) time than you think you will need. This is because queuing times are generally not long at OSC, and because you won’t be charged for reserved-but-not-used time.\nThat said, in general, the “bigger” (more time, more cores, more memory) your job is, the more likely it is that it will be pending for an appreciable amount of time. Smaller jobs (requesting up to a few hours and cores) will almost always start running nearly instantly. Even big jobs (requesting, say, a day or more and 10 or more cores) will often do so, but during busy times, you might have to wait for a while, sometomes hours.\n\n\n\n\n Exercise: exceed the time limit\nModify the sleep.sh script to make it run longer than the time you request for it with --time. (Take into account that it does not seem to be possible to effectively request a job that runs for less than 1 minute.)\nIf you succeed in exceeding the time limit, an error message will be printed — where do you think this error message will go? After waiting for the job to be killed after 60 seconds, check if you were correct and what the error message is.\n\n\nClick for the solution\n\nThis script would do the trick, where we request 1 minute of wall-time while we let the script sleep for 80 seconds:\n#!/bin/bash\n#SBATCH --account=PAS0471\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" &gt; sleep.txt\nsleep 80s\necho \"I'm awake! Done with script sleep.sh\"\nThis would result in the following type of error:\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2023-08-21T16:35:24 DUE TO TIME LIMIT ***\n\n\n\n\n\n3.3 Cores (& nodes and tasks)\nThere are several options to specify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nFirst, note that Slurm for the most part uses the terms “core” and “CPU” interchangeably2. More generally with bioinformatics tools, “thread” is also commonly used interchangeably with core/CPU3.\n\n\nSecond, running a program with multiple threads/cores/CPUs (“multi-threading”) is very common, while running a program with multiple tasks or nodes is rare.\n\nThe most appropriate way of specifying the number of threads/cores/CPUs is with --cpus-per-task=n (the short notation is -c). The program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\nAn example, where we ask for 8 CPUs/cores/threads:\n#!/bin/bash\n#SBATCH --cpus-per-task=8\n\n\n\n\n\n\nRare cases: multiple nodes or tasks (Click to expand)\n\n\n\n\n\n\nFirst, the defaults for the number of nodes (--nodes) and the number of tasks --ntasks are 1.\nOnly ask for more than one node when a program is parallelized with e.g. “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n — this is also quite rare!\n\nHere is an overview of the options related to cores, tasks, and nodes:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n\n\n\n\n\n3.4 --mem: RAM memory\nUse the --mem option to specify the maximum amount of RAM (Random Access Memory) that your job can use:\n\nThe default amount is 4 GB per reserved core. This is often enough, so it is common to omit the --mem option.\nThe default unit is MB (MegaBytes) — append G for GB (i.e. 100 means 100 MB, 10G means 10 GB).\nLike with the time limit, your job gets killed when it hits the memory limit.\n\nFor example, to request 20 GB of RAM:\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n\n\n\nIt is not always clear what happened when your job ran out of memory (Click to expand)\n\n\n\n\n\nWhereas you get a very clear Slurm error message when you hit the time limit (as seen in the exercise above), hitting the memory limit can result in a variety of errors.\nBut look for keywords such as “Killed”, “Out of Memory” / “OOM”, and “Core Dumped”, as well as actual “dumped cores” in your working dir (large files with names like core.&lt;number&gt;, these can be deleted).\n\n\n\n\n\n\n3.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-&lt;job-number&gt;.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the bioinformatics program that the script runs, so that it’s easier to recognize this file later. We can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nBut you’ll generally want to keep the batch job number in the file name too4. Since we won’t know the batch job number in advance, we need a trick here — and that is to use %j, which represents the batch job number:\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\nThe output streams stdout and stderr, and separateing them (Click to expand)\n\n\n\n\n\nBy default, two output streams from commands and programs called “standard output” (stdout) and “standard error” (stderr) are printed to screen. Without discussing this in detail, we have seen this several times: any regular output by a command is stdout and any error messages we’ve seen were stderr. Both of these streams by default also end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file.\n\n\n\n\n\n\n3.6 --mail-type: Receive emails\nYou can use the --mail-type option to have Slurm email you for example when a job begins, completes or fails. You don’t have to specify your email address: you’ll be automatically emailed on the email address that is linked to your OSC account. I tend to use:\n\nFAIL for shorter-running jobs (roughly up to a few hours)\nFAIL will email you upon job failure, e.g. when the scripts exits with an error or times out. This is especially useful when submitting many jobs for the same script with a loop: this way you know immediately whether any of the jobs failed.\nEND and FAIL for longer-running jobs\nThis is helpful because you don’t want to have to keep checking in on jobs that run for many hours.\n\nI would avoid having Slurm send you emails upon regular completion for shorter jobs, because you may get inundated with emails and may quickly start ignoring the emails altogether.\n#!/bin/bash\n#SBATCH --mail-type=END,FAIL\n#!/bin/bash\n#SBATCH --mail-type=FAIL\n\n\n\n\n\n\nGet warned when nearing the time limit\n\n\n\nYou may also find the values TIME_LIMIT_90, TIME_LIMIT_80, and TIME_LIMIT_50 useful for very long-running jobs, which will warn you when the job is at 90/80/50% of the time limit. For example, it is possible to email OSC to ask for an extension on individual jobs. You shouldn’t do this often, but if you have a job that ran for 6 days and it looks like it may time out, this may well be worth it.\n\n\n\n\n Exercise: Submit your FastQC script as a batch job\nLast week, we created a shell script to run FastQC, and ran it as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/fastqc.sh \"$fastq_file\" results/fastqc\n\nAdd an #SBATCH lines to the script to specify the course’s OSC project PAS2700, and submit the modified script as a batch job with the same arguments as above.\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\nSubmit the script as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\nSubmitted batch job 12431988\n\n\n\nMonitor your job with squeue.\nWhen it has finished, check the Slurm log file in your working dir and the main FastQC output files in results/fastqc.\nBonus — add these #SBATCH options, then resubmit:\n\nLet the Slurm log file include ‘fastqc’ in the file name as well as the job ID number.\nLet Slurm email you both when the job completes normally and when it fails. Check that you received the email.\n\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --output=slurm-fastqc-%j.out\n#SBATCH --mail-type=END,FAIL"
  },
  {
    "objectID": "week5/w5_3_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "href": "week5/w5_3_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "title": "Slurm batch jobs at OSC",
    "section": "4 In closing: making sure your jobs ran successfully",
    "text": "4 In closing: making sure your jobs ran successfully\nHere are some summarizing notes on the overall strategy to monitor your batch jobs:\n\nTo see whether your job(s) have started, check the queue (with squeue) or check for Slurm log files (with ls).\nOnce the jobs are no longer listed in the queue, they will have finished: either successfully or because of an error.\nWhen you’ve submitted many jobs that run the same script for different samples:\n\nCarefully read the full Slurm log file, and check other output files, for at least 1 one of them.\nCheck whether no jobs have failed via email when using --mail-type=END, or by checking the tail of each log for “Done with script” messages5.\nCheck that you have the expected number of output files and that no files have size zero (run ls -lh)."
  },
  {
    "objectID": "week5/w5_3_slurm.html#self-study-material",
    "href": "week5/w5_3_slurm.html#self-study-material",
    "title": "Slurm batch jobs at OSC",
    "section": "5 Self-study material",
    "text": "5 Self-study material\n\nSlurm environment variables\nInside a shell script that will be submitted as a batch job, you can use a number of Slurm environment variables that will automatically be available, such as:\n\n\n\n\n\n\n\n\nVariable\nCorresponding option\nDescription\n\n\n\n\n$SLURM_JOB_ID\nN/A\nJob ID assigned by Slurm\n\n\n$SLURM_JOB_NAME\n--job-name\nJob name\n\n\n$SLURM_CPUS_PER_TASK\n-c / --cpus-per-task\nNumber of CPUs (~ cores/threads) available\n\n\n$SLURM_MEM_PER_NODE\n--mem\nAmount of memory available (per node)\n\n\n$TMPDIR\nN/A\nPath to the Compute storage available during the job\n\n\n$SLURM_SUBMIT_DIR\nN/A\nPath to dir from which job was submitted.\n\n\n\nAs an example of how these environment variables can be useful, the command below uses $SLURM_CPUS_ON_NODE in its call to the program STAR inside the script:\nSTAR --runThreadN \"$SLURM_CPUS_ON_NODE\" --genomeDir ...\nWith this strategy, you will automatically use the correct (requested) number of cores, and don’t risk having a mismatch. Also, if you need to change the number of cores, you’ll only have to modify it in one place: in the resource request to Slurm.\n\n\n\n5.1 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. We’ve been working in a shell in VS Code Server, which means that we already have interactive shell access on a compute node!\nHowever, we only have access to 1 core and 4 GB of memory in this VS Code shell, and there is no way of changing this. If you want an interactive shell job with more resources, you’ll have to start one with Slurm commands.\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command6, which we can use with --pty /bin/bash added to get an interactive Bash shell.\nsrun --account=PAS0471 --pty /bin/bash\nsrun: job 12431932 queued and waiting for resources  \nsrun: job 12431932 has been allocated resources\n\n[...regular login info, such as quota, not shown...]\n\n[jelmer@p0133 PAS0471]$\nThere we go! First some Slurm scheduling info was printed to screen: initially, the job was queued, and then it was “allocated resources”: that is, computing resources such as a compute node were reserved for the job. After that:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nWe have now moved to the compute node at which our interactive job is running, so you should have a different p number in your prompt.\n\n\n\n\n5.2 Table with sbatch options\nThis includes all the discussed options, and a couple more useful ones:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial  --partition=hugemem\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456"
  },
  {
    "objectID": "week5/w5_3_slurm.html#footnotes",
    "href": "week5/w5_3_slurm.html#footnotes",
    "title": "Slurm batch jobs at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnless you actively “Delete” to job on the Ondemand website.↩︎\nEven though technically, one CPU often contains multiple cores.↩︎\nEven though technically, one core often contains multiple threads.↩︎\nFor instance, we might be running the FastQC script multiple times, and otherwise those would all have the same name and be overwritten.↩︎\nThe combination of using strict Bash settings (set -euo pipefail) and printing a line that marks the end of the script (echo \"Done with script\") makes it easy to spot scripts that failed, because they won’t have that marker line at the end of the Slurm log file.↩︎\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎"
  },
  {
    "objectID": "week2/w2_github-account.html",
    "href": "week2/w2_github-account.html",
    "title": "Assignment: Create a GitHub account",
    "section": "",
    "text": "What?\nCreate a personal GitHub account.\n\n\nWhy?\nGitHub is a website that hosts Git repositories, i.e. version-controlled projects. In Week 3 of this course, you will be learning how to use Git together with GitHub. In addition, you will submit your final project assignments through GitHub.\n\n\nHow?\nIf you already have a GitHub account, log in and start at step 6.\n\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFollow the prompts to create your account — some notes:\n\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\n\nCheck your email and click the link to verify your email address.\nBack on GitHub website, now logged in: in the far top-right of the page, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Public profile” tab, enter your name.\nStill in the “Profile tab”, upload a Profile picture. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_overview.html#links",
    "href": "week2/w2_overview.html#links",
    "title": "Week 2: Project organization and Markdown",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Project organization\nTue: Markdown & VS Code\nThu: Managing files in the shell\n\n\n\nExercises & assignments\n\nAssignment: Create a GitHub account\nWeek 2 exercises"
  },
  {
    "objectID": "week2/w2_overview.html#content-overview",
    "href": "week2/w2_overview.html#content-overview",
    "title": "Week 2: Project organization and Markdown",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we’ll talk about some best practices for project organization, managing your project’s files in the Unix shell, and documenting your project with Markdown files. We’ll also spend a bit of time getting to know VS Code, the text editor that you will spend a lot of time in during this course.\nSome of the things you will learn this week:\n\nA number of best practices for project organization, documentation, and management.\nHow to apply these best practices this in the shell.\nHow to use Markdown for documentation (and beyond).\nGet to know our text editor for the course, VS Code, a bit better."
  },
  {
    "objectID": "week2/w2_overview.html#readings",
    "href": "week2/w2_overview.html#readings",
    "title": "Week 2: Project organization and Markdown",
    "section": "3 Readings",
    "text": "3 Readings\nThis week’s reading is Chapter 2 of our secondary book, “Bioinformatics Data Skills” (“Buffalo” for short – the author is Vince Buffalo), which deals with project organization and (file) management.\nThe chapter also contains several tips and tricks for using the shell for project organization, which we will work through in the Zoom sessions. For more material along these lines, optional reading is Buffalo Chapter 3.\n\nRequired readings\n\nBuffalo Chapter 2: “Setting up and Managing a Bioinformatics Project”\n\n\n\nOptional readings\n\nBuffalo Chapter 3: “Remedial Unix Shell”\n\n\n\nFurther resources\n\nWilson et al. 2017, PLOS Computational Biology: “Good enough practices in scientific computing”\nKieran Healy: “The Plain Person’s Guide to Plain Text Social Science”"
  },
  {
    "objectID": "week2/w2_shellfiles.html#overview-setting-up",
    "href": "week2/w2_shellfiles.html#overview-setting-up",
    "title": "Managing files in the shell",
    "section": "1 Overview & setting up",
    "text": "1 Overview & setting up\nIn this session, we will learn some more Unix shell skills, focusing on commands to manage files with an eye on organizing your research projects.\nSpecifically, we will learn about:\n\nWildcard expansion to select and operate on multiple files at once\nBrace expansion to help create regular series of files and dirs\nCommand substitution to save the output of commands\nFor loops to repeat operations across, e.g., file\nRenaming multiple files using for loops\n\nAnd for those that are interested, there is some optional at-home reading about changing file permissions (e.g. to make your raw data read-only) and creating symbolic links (to access files across different projects).\n\n\n1.1 VS Code setup\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2700\nThe starting directory: /fs/ess/PAS2700/users/&lt;user&gt; (replace &lt;user&gt; with your OSC username)\nNumber of hours: 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nType pwd to check where you are.\nIf you are not in /fs/ess/PAS2700/users/&lt;user&gt; click      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS2700/users/&lt;user&gt; and press OK.\n\n\n\n\n\n\n\n1.2 Create a dummy project – following Buffalo\nGo into the dir for this week that you created earlier:\n# You should be in /fs/ess/PAS2700/users/$USER/\ncd week02\nFirst, we’ll create a set of directories representing a dummy research project:\nmkdir zmays-snps\ncd zmays-snps\n\n# The -p option for mkdir will allow for 'recursive' (nested) dir creation\nmkdir -p data/fastq scripts results/figs\nThe touch command will create one or more empty files. We will use it to create some empty files that are supposed to represent sequence files with forward (“R1”) and reverse (“R2”) DNA sequence reads for 3 samples:\ncd data/fastq\n\ntouch sample1_R1.fastq.gz sample1_R2.fastq.gz\ntouch sample2_R1.fastq.gz sample2_R2.fastq.gz\ntouch sample3_R1.fastq.gz sample3_R2.fastq.gz\nFor a nice recursive overview of your directory structure, use the tree command (with option -C to show colors):\n# \"../..\" tells tree to start two levels up\n# (Output colors are not shown on this webpage)\ntree -C ../..\n../..\n├── data\n│   └── fastq\n│       ├── sample1_R1.fastq.gz\n│       ├── sample1_R2.fastq.gz\n│       ├── sample2_R1.fastq.gz\n│       ├── sample2_R2.fastq.gz\n│       ├── sample3_R1.fastq.gz\n│       └── sample3_R2.fastq.gz\n├── results\n│   └── figs\n└── scripts\n\n5 directories, 6 files"
  },
  {
    "objectID": "week2/w2_shellfiles.html#wildcard-expansion",
    "href": "week2/w2_shellfiles.html#wildcard-expansion",
    "title": "Managing files in the shell",
    "section": "2 Wildcard expansion",
    "text": "2 Wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\n\nThe * wildcard\nIn globbing, the * wildcard matches any number of any character, including nothing.\nWith the following files in our directory:\nls\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz\nsample2_R2.fastq.gz  sample3_R1.fastq.gz  sample3_R2.fastq.gz\nWe can match both “sample1” files as follows:\nls sample1*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nls sample1*fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\n\nTo match only files with forward reads (contain “_R1”):\nls *_R1*\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\nls *R1.fastq.gz\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\n\nWhen globbing, the pattern has to match the entire file name, so this doesn’t match anything:\n# There are no files that _end in_ R1: we'd need another asterisk at the end\nls *R1\nls: cannot access *R1: No such file or directory\n\nIn summary:\n\n\n\n\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files)1\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nsample1*\nStart with “sample1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n\n Exercise: File matching 1\n\nList only the FASTQ files for sample 3.\n\n\n\nClick for the solution\n\nls sample3*\nsample3_R1.fastq.gz  sample3_R2.fastq.gz\n\n\nWhich files would ls samp*le* match?\n\n\n\nClick for the solution\n\nAll of them, since all file names start with sample, and because * also matches “zero characters”, there is no requirement for there to be a character between the p and the l.\n\n\n\n\nOther shell wildcards\nThere are two more shell wildcards, and here is a complete overview of shell wildcards:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\nUsing the ? wildcard to match both R1 and R2:\nls sample1_R?.fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nTo match files for sample1 and sample2 using only a character class with []:\n\nMethod 1 — List all possible characters (1 and 2 in this case):\nls sample[12]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 2 – Use a range like [0-9], [A-Z], [a-z]:\nls sample[1-2]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 3 – Exclude the unwanted sample ID:\nls sample[^3]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\n\n\n\n\n\n\n\n[] works on single character ranges only: 0-9 works but 10-13 does not.\n\n\n\n\n\n\nThe examples so far may seem trivial, but you can use these techniques to easily operate on selections among 100s or 1000s of files.\n\n\n\nExpansion is done by the shell itself\nThe expansion –to all matching file names– is done by the shell, not by ls or another command you might be using wildcards with. Therefore, ls will “see”/“receive” the list of files after the expansion has already happened.\nFor example: we can copy (cp command), move (mv) or delete (rm) files with shell expansion, and we can also first check which files those command will “see” by first using echo (or ls) with the exact same globbing pattern:\n# Check which files are selected \necho sample[12]*\nsample1_R1.fastq.gz sample1_R2.fastq.gz sample2_R1.fastq.gz sample2_R2.fastq.gz\n# Remove the files with rm\n# (The -v option will make rm report what it's removing)\nrm -v sample[12]*\nremoved ‘sample1_R1.fastq.gz’\nremoved ‘sample1_R2.fastq.gz’\nremoved ‘sample2_R1.fastq.gz’\nremoved ‘sample2_R2.fastq.gz’\n\n\n\n\n\n\nWildcards vs. regular expressions\n\n\n\nDon’t confuse wildcards with regular expressions! You may have used regular expressions before, for example with R or a text editor. They are similar to but not the same as shell wildcards. We’ll talk about regular expressions in Week 4."
  },
  {
    "objectID": "week2/w2_shellfiles.html#brace-expansion",
    "href": "week2/w2_shellfiles.html#brace-expansion",
    "title": "Managing files in the shell",
    "section": "3 Brace expansion",
    "text": "3 Brace expansion\nWhereas wildcard expansion looks for corresponding files and expands to whichever files are present, brace expansion with {}, is another type of shell expansion that expands to whatever you tell it to.\n# First move up to zmays-snps\ncd ../..\nUse .. within {} to indicate ranges of numbers or letters:\n# Here we'll create 31 _dirs_ for different dates\nmkdir -p data/obs/2024-03-{01..31}\n\nls data/obs\n2024-03-01  2024-03-04  2024-03-07  2024-03-10  2024-03-13  2024-03-16  2024-03-19  2024-03-22  2024-03-25  2024-03-28  2024-03-31\n2024-03-02  2024-03-05  2024-03-08  2024-03-11  2024-03-14  2024-03-17  2024-03-20  2024-03-23  2024-03-26  2024-03-29\n2024-03-03  2024-03-06  2024-03-09  2024-03-12  2024-03-15  2024-03-18  2024-03-21  2024-03-24  2024-03-27  2024-03-30\n# Here we'll create 6 empty _files_\ntouch results/figs/fig-1{A..F}.png\n\nls results/figs\nfig-1A.png  fig-1B.png  fig-1C.png  fig-1D.png  fig-1E.png  fig-1F.png\n\nFinally, you can also use a comma-separated list, and multiple brace expansions — with the latter, you will get all combinations among values in the expansions:\nmkdir -p data/obs2/treatment-{Kr,Df,Tr}_temp-{lo,med,hi}\n\nls data/obs2\ntreatment-Df_temp-hi   treatment-Kr_temp-hi   treatment-Tr_temp-hi\ntreatment-Df_temp-lo   treatment-Kr_temp-lo   treatment-Tr_temp-lo\ntreatment-Df_temp-med  treatment-Kr_temp-med  treatment-Tr_temp-med\n\n\n Exercise: File matching 2\n\nMove back into data/fastq/ and remove all (remaining) files in there in one go.\n\n\n\nClick for the solution\n\ncd data/fastq/   # Assuming you were still in /fs/ess/PAS2700/users/$USER/week02/zmays-snps\n\n# (You don't have to use the -v flag)\nrm -v *fastq.gz\nremoved ‘sample3_R1.fastq.gz’\nremoved ‘sample3_R2.fastq.gz’\n\n\nUsing brace expansion and the touch command, create empty R1 and R2 FASTQ files for 100 samples with IDs from 001 to 100: sample&lt;ID&gt;_R1.fastq and sample&lt;ID&gt;_R2.fastq.\n\n\n\nClick for the solution\n\ntouch sample{001..100}_R{1,2}.fastq\n\nls\nsample001_R1.fastq  sample026_R1.fastq  sample051_R1.fastq  sample076_R1.fastq\nsample001_R2.fastq  sample026_R2.fastq  sample051_R2.fastq  sample076_R2.fastq\nsample002_R1.fastq  sample027_R1.fastq  sample052_R1.fastq  sample077_R1.fastq\nsample002_R2.fastq  sample027_R2.fastq  sample052_R2.fastq  sample077_R2.fastq\nsample003_R1.fastq  sample028_R1.fastq  sample053_R1.fastq  sample078_R1.fastq\nsample003_R2.fastq  sample028_R2.fastq  sample053_R2.fastq  sample078_R2.fastq\nsample004_R1.fastq  sample029_R1.fastq  sample054_R1.fastq  sample079_R1.fastq\nsample004_R2.fastq  sample029_R2.fastq  sample054_R2.fastq  sample079_R2.fastq\nsample005_R1.fastq  sample030_R1.fastq  sample055_R1.fastq  sample080_R1.fastq\nsample005_R2.fastq  sample030_R2.fastq  sample055_R2.fastq  sample080_R2.fastq\nsample006_R1.fastq  sample031_R1.fastq  sample056_R1.fastq  sample081_R1.fastq\nsample006_R2.fastq  sample031_R2.fastq  sample056_R2.fastq  sample081_R2.fastq\nsample007_R1.fastq  sample032_R1.fastq  sample057_R1.fastq  sample082_R1.fastq\nsample007_R2.fastq  sample032_R2.fastq  sample057_R2.fastq  sample082_R2.fastq\nsample008_R1.fastq  sample033_R1.fastq  sample058_R1.fastq  sample083_R1.fastq\nsample008_R2.fastq  sample033_R2.fastq  sample058_R2.fastq  sample083_R2.fastq\nsample009_R1.fastq  sample034_R1.fastq  sample059_R1.fastq  sample084_R1.fastq\nsample009_R2.fastq  sample034_R2.fastq  sample059_R2.fastq  sample084_R2.fastq\nsample010_R1.fastq  sample035_R1.fastq  sample060_R1.fastq  sample085_R1.fastq\nsample010_R2.fastq  sample035_R2.fastq  sample060_R2.fastq  sample085_R2.fastq\nsample011_R1.fastq  sample036_R1.fastq  sample061_R1.fastq  sample086_R1.fastq\nsample011_R2.fastq  sample036_R2.fastq  sample061_R2.fastq  sample086_R2.fastq\nsample012_R1.fastq  sample037_R1.fastq  sample062_R1.fastq  sample087_R1.fastq\nsample012_R2.fastq  sample037_R2.fastq  sample062_R2.fastq  sample087_R2.fastq\nsample013_R1.fastq  sample038_R1.fastq  sample063_R1.fastq  sample088_R1.fastq\nsample013_R2.fastq  sample038_R2.fastq  sample063_R2.fastq  sample088_R2.fastq\nsample014_R1.fastq  sample039_R1.fastq  sample064_R1.fastq  sample089_R1.fastq\nsample014_R2.fastq  sample039_R2.fastq  sample064_R2.fastq  sample089_R2.fastq\nsample015_R1.fastq  sample040_R1.fastq  sample065_R1.fastq  sample090_R1.fastq\nsample015_R2.fastq  sample040_R2.fastq  sample065_R2.fastq  sample090_R2.fastq\nsample016_R1.fastq  sample041_R1.fastq  sample066_R1.fastq  sample091_R1.fastq\nsample016_R2.fastq  sample041_R2.fastq  sample066_R2.fastq  sample091_R2.fastq\nsample017_R1.fastq  sample042_R1.fastq  sample067_R1.fastq  sample092_R1.fastq\nsample017_R2.fastq  sample042_R2.fastq  sample067_R2.fastq  sample092_R2.fastq\nsample018_R1.fastq  sample043_R1.fastq  sample068_R1.fastq  sample093_R1.fastq\nsample018_R2.fastq  sample043_R2.fastq  sample068_R2.fastq  sample093_R2.fastq\nsample019_R1.fastq  sample044_R1.fastq  sample069_R1.fastq  sample094_R1.fastq\nsample019_R2.fastq  sample044_R2.fastq  sample069_R2.fastq  sample094_R2.fastq\nsample020_R1.fastq  sample045_R1.fastq  sample070_R1.fastq  sample095_R1.fastq\nsample020_R2.fastq  sample045_R2.fastq  sample070_R2.fastq  sample095_R2.fastq\nsample021_R1.fastq  sample046_R1.fastq  sample071_R1.fastq  sample096_R1.fastq\nsample021_R2.fastq  sample046_R2.fastq  sample071_R2.fastq  sample096_R2.fastq\nsample022_R1.fastq  sample047_R1.fastq  sample072_R1.fastq  sample097_R1.fastq\nsample022_R2.fastq  sample047_R2.fastq  sample072_R2.fastq  sample097_R2.fastq\nsample023_R1.fastq  sample048_R1.fastq  sample073_R1.fastq  sample098_R1.fastq\nsample023_R2.fastq  sample048_R2.fastq  sample073_R2.fastq  sample098_R2.fastq\nsample024_R1.fastq  sample049_R1.fastq  sample074_R1.fastq  sample099_R1.fastq\nsample024_R2.fastq  sample049_R2.fastq  sample074_R2.fastq  sample099_R2.fastq\nsample025_R1.fastq  sample050_R1.fastq  sample075_R1.fastq  sample100_R1.fastq\nsample025_R2.fastq  sample050_R2.fastq  sample075_R2.fastq  sample100_R2.fastq\n\n\n\nBonus: Count the number of “R1” files by first using ls with a globbing pattern that only selects R1 files, and then piping the ls output into wc -l.\n\n\n\nClick for the solution\n\n# wc -l will count the number of lines, i.e. the number of files\n# (Note that this works properly even though raw ls output may\n# put multiple files on 1 line.)\nls *R1*fastq | wc -l\n100\n\n\nBonus: Copy all files except the two for “sample100” into a new directory called selection — use a wildcard to do the move with a single command. (You will first need to create the new dir separately.)\n\n\n\nClick for the solution\n\nFirst create the selection dir:\nmkdir selection\nMethod 1 — Exclude sample numbers starting with a 1:\ncp sample[^1]* selection/\nMethod 2 — Other way around; include sample numbers starting with a 0:\ncp sample0* selection/"
  },
  {
    "objectID": "week2/w2_shellfiles.html#variables-and-command-substitution",
    "href": "week2/w2_shellfiles.html#variables-and-command-substitution",
    "title": "Managing files in the shell",
    "section": "4 Variables and command substitution",
    "text": "4 Variables and command substitution\n\n4.1 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs. Using variables makes it easier to change such settings. We also need to understand variables to work with loops and scripts.\n\nAssigning and referencing variables\nTo assign a value to a variable in the shell, use the syntax variable_name=value:\n# Assign the value \"beach\" to a variable with the name \"location\":\nlocation=beach\n\n# Assign the value \"200\" to a variable with the name \"n_lines\":\nn_lines=200\n\n\n\n\n\n\nRecall that there can’t be spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value):\n\nYou need to put a dollar sign $ in front of its name.\nIt is good practice to double-quote (\"...\") variable names2.\n\nAs before with the environment variable $HOME, we’ll use the echo command to see what values our variables contain:\necho \"$location\"\nbeach\necho \"$n_lines\"\n200\nConveniently, we can use variables in lots of contexts, as if we had directly typed their values:\ninput_file=results/figs/fig-1A.png\n\nls -lh \"$input_file\"\n-rw-rw----+ 1 jelmer PAS0471 0 Mar  7 13:17 results/figs/fig-1A.png\n\n\n\n\n\n\nRules and tips for naming variables (Click to expand)\n\n\n\n\n\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\n\n\n\n\n\n\n\n4.2 Command substitution\nCommand substitution allows you to store and pass the output of a command to another command. Let’s see an example. As you know, the date command will print the current date and time:\ndate\nThu Mar  7 14:52:22 EST 2024\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored instead:\ntoday=date\necho \"$today\"\ndate\nThat’s why we need command substitution, which we can use by wrapping the command inside $():\ntoday=$(date)\necho \"$today\"\nThu Mar  7 14:53:11 EST 2024\n\nOne practical example of using command substitution is when you want to automatically include the current date in a file name. First, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\ndate +%F\n2024-03-07\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\ntouch README_\"$(date +%F)\".txt\n\nls\nREADME_2024-03-07.txt\n\n\n Bonus exercise: Command substitution\nSay we wanted to store and report the number of lines in a FASTQ file, which tells us how many sequence “reads” are in it (because FASTQ files contain 4 lines per read).\nHere is how we can get the number of lines of a compressed FASTQ file:\n\nUse zcat (instead of regular cat) to print the contents despite the file compression\nAs we’ve seen before, wc -l gets you the number of lines, but note here that if you pipe input into wc -l, it won’t include the file name in the output:\n\nzcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l\n2000000\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print the following:\nThe file has 2000000 lines\n\n\nClick for the solution\n\nn_lines=$(zcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l)\n\necho \"The file has $n_lines lines\"\nThe file has 2000000 lines\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n4.3 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\nWhat was actually run under the hood is the following:\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/sample001_R1.fastq...\nRunning an analysis for file data/fastq/sample001_R2.fastq...\nRunning an analysis for file data/fastq/sample002_R1.fastq...\nRunning an analysis for file data/fastq/sample002_R2.fastq...\nRunning an analysis for file data/fastq/sample003_R1.fastq...\nRunning an analysis for file data/fastq/sample003_R2.fastq...\n# [...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "week2/w2_shellfiles.html#renaming-files-with-loops",
    "href": "week2/w2_shellfiles.html#renaming-files-with-loops",
    "title": "Managing files in the shell",
    "section": "5 Renaming files with loops",
    "text": "5 Renaming files with loops\nThere are many different ways to rename many files in a programmatic way in the shell – admittedly none as easy as one might have hoped.\nHere, we’ll use the basename command and a for loop. for loops are a verbose method for tasks like renaming, but are relatively intuitive and good to get practice with.\n\nbasename\nFirst, we’ll have to learn about the basename command, which removes any dir name that may be present in a file name (path), and optionally, removes a suffix too:\n# Just remove the directories:\nbasename data/fastq/sample001_R1.fastq\nsample001_R1.fastq\n# Also remove a suffix by specifying it after the file name:\nbasename data/fastq/sample001_R1.fastq .fastq\nsample001_R1\n\n\n\n\n\n\nDon’t have these FASTQ files? (Click to expand)\n\n\n\n\n\nWe made these in one of the exercises above, but if you don’t have them:\n# You should be in /fs/ess/PAS2700/users/$USER/week02\ntouch data/fastq/sample{001..100}_R{1,2}.fastq\n\n\n\n\n\n\nRenaming a single file\nLet’s say that we wanted to rename these files so that they have the suffix .fq instead of .fastq. Here’s how we could do that for one file in a way that we can use in a loop:\nThe original file name will be contained in a variable:\noldname=sample001_R1.fastq\nWe can also save the new name in a variable\nnewname=$(basename \"$oldname\" .fastq).fq\nBefore actually renaming, note this trick with echo to just print the command instead of executing it:\necho mv -v \"$oldname\" \"$newname\"\nmv -v sample001_R1.fastq sample001_R1.fq\nLooks good? Then we remove echo and rename the file (we’re using the -v to make mv report what it’s doing):\nmv -v \"$oldname\" \"$newname\"\nsample001_R1.fastq -&gt; sample001_R1.fq\n\n\n\nLooping over all files\nHere’s how we can loop over these files, saving each file name (one at a time) in the variable $oldname:\nfor oldname in *.fastq; do\n    # ...\ndone\nNext, we assign a new name for each file:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\ndone\nWe build and check the renaming command:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    echo mv -v \"$oldname\" \"$newname\"\ndone\nmv -v sample001_R1_001.fastq sample001_R1_001.fq\nmv -v sample001_R2_001.fastq sample001_R2_001.fq\nmv -v sample002_R1_001.fastq sample002_R1_001.fq\nmv -v sample002_R2_001.fastq sample002_R2_001.fq\n# [...output truncated...]\nWe do the renaming by removing echo:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    mv -v \"$oldname\" \"$newname\"\ndone\n‘sample001_R1_001.fastq’ -&gt; ‘sample001_R1_001.fq’\n‘sample001_R2_001.fastq’ -&gt; ‘sample001_R2_001.fq’\n‘sample002_R1_001.fastq’ -&gt; ‘sample002_R1_001.fq’\n‘sample002_R2_001.fastq’ -&gt; ‘sample002_R2_001.fq’\n‘sample003_R1_001.fastq’ -&gt; ‘sample003_R1_001.fq’\n‘sample003_R2_001.fastq’ -&gt; ‘sample003_R2_001.fq’\n# [...output truncated...]"
  },
  {
    "objectID": "week2/w2_shellfiles.html#bonus-content",
    "href": "week2/w2_shellfiles.html#bonus-content",
    "title": "Managing files in the shell",
    "section": "6 Bonus content",
    "text": "6 Bonus content\n\n6.1 Viewing and modifying file permissions\nFile “permissions” are the types of things (e.g. reading, writing) that different groups of users (creator, group, anyone else) are permitted to do with files and dirs.\nThere are a couple of reasons you may occasionally need to view and modify file permissions:\n\nYou may want to make your data read-only\nYou may need to share files with other users at OSC\n\n\n\nViewing file permissions\nTo show file permissions, use ls with the -l (long format) option that we’ve seen before. The command below also uses the -a option to show all files, including hidden ones (and -h to show file sizes in human-readable format):\n\n\n\n\n\nHere is an overview of the file permission notation in ls -l output:\n\n\n\n\n\nIn the two lines above:\n\nrwxrwxr-x means:\nread + write + execute permissions for both the owner (first rwx) and the group (second rwx), and read + execute but not write permissions for others (r-x at the end).\nrw-rw-r-- means:\nread + write but not execute permissions for both the owner (first rw-) and the group (second rw-), and only read permissions for others (r-- at the end).\n\nLet’s create a file to play around with file permissions:\n# Create a test file\ntouch testfile.txt\n\n# Check the default permissions\nls -l testfile.txt\n-rw-rw----+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\nChanging file permissions\nThis can be done in two different ways with the chmod command. Here, we’ll focus on the method with = (set permission to), + (add permission), and - (remove permission).\nFor example, to add read (r) permissions for all (a):\n# chmod &lt;who&gt;+&lt;permission-to-add&gt;:\nchmod a+r testfile.txt\n\nls -l testfile.txt\n-rw-rw-r--+ 1 jelmer PAS0471 0 Mar  7 13:40 testfile.txt\nTo set read + write + execute (rwx) permissions for all (a):\n# chmod &lt;who&gt;=&lt;permission-to-set&gt;`:\nchmod a=rwx testfile.txt\n\nls -l testfile.txt\n-rwxrwxrwx+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\nTo remove write (w) permissions for others (o):\n# chmod &lt;who&gt;-&lt;permission-to-remove&gt;:\nchmod o-w testfile.txt\n\nls -l testfile.txt\n-rwxrwxr-x+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\n\n\n\nAlternative: changing file permissions with numbers (Click to expand)\n\n\n\n\n\nYou can also use a series of 3 numbers (for user, group, and others) to set permissions, where each number can take on the following values:\n\n\n\nNr\nPermission\nNr\nPermission\n\n\n\n\n1\nx\n5\nr + x\n\n\n2\nw\n6\nr + w\n\n\n4\nr\n7\nr + w + x\n\n\n\nFor example, to set read + write + execute permissions for all:\nchmod 777 testfile.txt\nTo set read + write + execute permissions for yourself, and only read permission for the group and others:\nchmod 744 file.txt\n\n\n\n\n\n\nMaking your data read-only\nSo, if you want to make your raw data (here: the files in the data/fastq dir) read-only, you can use:\n\nSet only read permissions for everyone:\nchmod a=r data/fastq/*\nTake away write permissions for yourself (no-one else should have it by default):\nchmod u-w data/fastq/*\n\n\n\n\n\n\n\n\nRead/execute permissions for directories\n\n\n\nOne tricky and confusing aspect of file permissions is that to list a directory’s content, you need execute permissions for the dir! This is something to take into account when you want to grant others access to your project e.g. at OSC.\nTo set execute permissions for everyone but only for dirs throughout a dir hierarchy, use an X (uppercase x):\nchmod -R a+X my-dir\n\n\n\nAfter running one or both of the above commands, let’s check the permissions:\nls -l data/fastq\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R2.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R2.fastq\n# [...output truncated...]\nWhat happens when we try to remove write-protected files?\nrm data/fastq/*fastq\nrm: remove write-protected regular empty file ‘data/fastq/sample001_R1.fastq’?\nYou’ll be prompted for every file! If you answer y (yes), you can still remove them. (But note that people other than the file’s owners cannot overried file permissions; only if they are system administrators.)\n\n\n\n\n6.2 Using files across projects: Using symbolic links\n\nSingle files\nA symbolic (or soft) links only links to the path of the original file, whereas a hard link directly links to the contents of the original file. Note that modifying a file via either a hard or soft link will modify the original file.\nCreate a symlink to a file using ln -s &lt;source-file&gt; [&lt;link-name&gt;]:\n# Only provide source =&gt; create link of the same name in the wd:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz\n  \n# The link can also be given an arbitrary name/path:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz shared-fastq.fastq.gz\n\n\n\n\n\n\nUse an absolute path to refer to the source file when creating links\n\n\n\nAt least at OSC, you have to use an absolute path for the source file(s), or the link will not work. The $PWD environment variable, which contains your current working directory can come in handy to do so:\n# (Fictional example, don't run this)\nln -s $PWD/shared-scripts/align.sh project1/scripts/\n\n\n\n\n\nMultiple files\nLink to multiple files in a directory at once:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/* project1/scripts/ \nLink to a directory:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/ project1/scripts/\nln -s $PWD/shared_scripts/ project1/scripts/ln-shared-scripts\n\n\n\n\n\n\nBe careful not to remove source files!\n\n\n\nBe careful when linking to directories: you are creating a point of entry to the original dir. Therefore, even if you enter via the symlink, you are interacting with the original files.\nThis means that a command like the following would remove the original directory!\nrm -r symlink-to-dir\nInstead, use rm symlink-to-dir (the link itself is a file, not a dir, so you don’t need -r!) or unlink symlink-to-dir to only remove the link.\n\n\n\n\n Exercise: Creating symbolic links\n\nCreate a symbolic link in your $HOME dir that points to your personal dir in the project dir (/fs/ess/PAS2700/users/$USER).\nIf you don’t provide a name for the link, it will be your username (why?), which is not particularly informative about its destination. Therefore, give it a name that makes sense to you, like PLNTPTH6193-SP24 or pracs-sp24.\n\n\n\nClick for the solution\n\nln -s /fs/ess/PAS1855/users/$USER ~/PLNTPTH6193-SP24\n\n\nWhat would happen if you do rm -rf ~/PLNTPTH8300-SP21? Don’t try this.\n\n\n\nClick for the solution\n\nThe content of the original dir will be removed."
  },
  {
    "objectID": "week2/w2_shellfiles.html#footnotes",
    "href": "week2/w2_shellfiles.html#footnotes",
    "title": "Managing files in the shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept so-called hidden files.↩︎\nWe’ll talk more about quoting later.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 6193 Spring 2024, second session\n\n\n\n\nWeek\nTue\nThu\nHomework\n\n\n\n\n\n(Pre-course work)\n\n\n\n\n1\n2/27: Course Intro & OSC Intro\n2/29: Shell basics\n   \n\n\n2\n3/05: Project organization & Markdown\n3/07: Managing files in the shell\n   \n\n\n3\n3/19: Getting started with Git\n3/21: Git remotes on GitHub\n   \n\n\n4\n3/26: Shell scripting\n3/28: Running CLI tools with shell scripts\n   \n\n\n5\n4/02: More on OSC & Software at OSC\n4/04: OSC Slurm batch jobs\n      \n\n\n6\n4/09: Nextflow I\n4/11: Nextflow II\n   \n\n\n7\n4/16: Recap\n4/18: Student presentations\n   \n\n\n\n\nHomework column:  = Week overview;  = Exercises;  = Final project checkpoint.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/proposal.html",
    "href": "finalproj/proposal.html",
    "title": "General information about your final project",
    "section": "",
    "text": "Write a concise, informal summary of what you plan to do for your final project (due Monday, Apr 8).\n[10 points]\nSome pointers:\n\nCreate a directory for your project and start a Git repository. This will be a repository for the project as a whole, not just for this proposal. [1]\nWrite the proposal in Markdown and include it in your repository. When you’re done, push your repository to GitHub and tag me (@jelmerp) in an “Issue” on GitHub (please make sure to tag me in the text body for the issue, it won’t be parsed as a tag if you do this in the title of the issue). [1]\nIn the proposal, start with a general description of what your project will be about, without going into detail about coding languages and approaches. Describe what data you will work with and what kind of output your project will produce. (This can be pretty minimal if your project is “code-first” with a trivial data set.) [2]\nNext, summarize how you envision the more technical aspects: in which language(s) do you intend to code, what do you think you will need separate scripts for, and how will you structure results? Will you mostly be coding your data processing/analyses from scratch, or are you primarily running external programs? [2]\nCheck the list of graded aspects on the page with general information about the project. If you are not sure you will be able to fulfill one of these, or intend to skip something (e.g. OSC SLURM jobs), mention that here.\nBriefly mention which aspects of your project you are uncertain about, for instance because you intend to include some topics that we have yet to cover in the course, or because something will depend on how other things go. [2]\nFinally, briefly describe why you chose to pick this project. Because it will be useful for your research? Because it gives you practice with coding topics you like or that you want to / need to get better with? All of the above and/or something else? [2]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/info.html#footnotes",
    "href": "finalproj/info.html#footnotes",
    "title": "General information about your final project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Note that “subsetting” the data may be useful or necessary if you have a large genomic dataset, and I can help you with that↩︎\n A locally run project could be okay if your research does not and will not require the use of supercomputers like those at OSC – check in with me if this is the case and you would therefore prefer not to use OSC.↩︎\n This is a challenging aspect, and your grade won’t plummet if you don’t succeed, but it is important to try!↩︎"
  },
  {
    "objectID": "week0/osc-setup.html",
    "href": "week0/osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore the course starts, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2700).\n\n\nBackground\nMuch of the coding during this course will be done through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC Project, and membership of this specific project will allow you to access our shared files and reserve “compute nodes”.\n\n\nWhat you should do\nAfter completing the pre-course survey, you will receive an invitation email from OSC referencing the course project number PAS2700:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week1/removed.html",
    "href": "week1/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week1/removed.html#bonus-material",
    "href": "week1/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week1/w1_overview.html#links",
    "href": "week1/w1_overview.html#links",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Course intro (slides)\nTue: OSC intro\nThu: Shell basics\n\n\n\nExercises\n\nWeek 1 exercises\n\n\n\n\n\n\n\n\nPre-course assignments (if you didn’t do this already)\n\n\n\n\nPre-course survey\nGet access to the Ohio Supercomputer Center (OSC)"
  },
  {
    "objectID": "week1/w1_overview.html#content-overview",
    "href": "week1/w1_overview.html#content-overview",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an overview of the course and a brief intro to the Ohio Supercomputer (OSC) during the Tuesday meeting, and will be taught the basics of working in a Unix shell environment during the Thursday meeting.\nMore specifically, some of the things you will learn this week:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nOSC Intro (Tuesday class)\n\nWhat is a supercomputer and why is it useful?\nOverview of the resources the Ohio Supercomputer Center (OSC) provides.\nHow to use OSC OnDemand and access a Unix Shell in your browser.\n\n\n\nUnix shell basics (Thursday class, Readings, and Exercises)\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing Unix commands, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week1/w1_overview.html#readings",
    "href": "week1/w1_overview.html#readings",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of the book Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book as well as this course.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell.\nI would recommend reading or at least skimming this before Thursday’s class, when we will go through much of the chapter’s content. That said, I will not assume you’ve read it during class, and you may prefer to focus your reading after the lectures. (We’ll discuss reading expectations and recommendations on Tuesday, too.)\nYou can access the books directly online through the links below, or download PDFs from the CarmenCanvas website. Consider buying a paper copy of one or both if you can afford it.\n\nRequired readings\n\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "week1/w1_osc.html#goals-for-this-session",
    "href": "week1/w1_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC).\nThis is only meant as a brief introductory overview to give some context about the working environment that we will start using this week. During the course, you’ll learn a lot more about most topics touched on in this page — week 5 in particular focuses on OSC."
  },
  {
    "objectID": "week1/w1_osc.html#high-performance-computing",
    "href": "week1/w1_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2700.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week1/w1_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course’s OSC Project PAS2700: /fs/ess/PAS2700. (We’ll talk more about these different file systems in week 5.)\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use in week 5, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs2.\n\n\n\n\n\n\n\n\nAt-home reading: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "week1/w1_osc.html#osc-ondemand",
    "href": "week1/w1_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2700, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2700 project’s “scratch” directory (/fs/scratch/PAS2700)\nThe PAS2700 project’s “project” directory (/fs/ess/PAS2700)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2700.\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files3 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server.\n\n\n\n4.3 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell in the next session."
  },
  {
    "objectID": "week1/w1_osc.html#footnotes",
    "href": "week1/w1_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "week6/w6_exercises.html",
    "href": "week6/w6_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/glossary.html",
    "href": "ref/glossary.html",
    "title": "Glossary of common terms in this course",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\nBuffalo\nCourse book: Bioinformatics Data Skills (Buffalo 2015).\n\n\nCLI\nCommand-line Interface — a software interface in which one types commands (cf. “GUI”).\n\n\nCSB\nCourse book: Computing Skills for Biologists (Allesina & Wilmes 2019).\n\n\ncluster\nAnother word for supercomputer, a set of interconnected computers for high-performance computing\n\n\ndir\nShort for “directory”, which how a folder is often referred to in the Unix shell and Linux.\n\n\nGit\nSoftware for “version control”, a system to track changes in code and other text files, and collaborate on those.\n\n\nGitHub\nA website that hosts Git projects, which are known as repositories.\n\n\nGUI\nGraphical User Interface – a visual software interface with which one interacts by clicking (cf. “CLI”).\n\n\nHPC\nHigh-Performance Computing, for instance as can be done using a supercomputer.\n\n\nLinux\nThe operating system that OSC runs on. Free Linux distributions include Ubuntu.\n\n\nMarkdown\nA simple text markup language (think LaTeX or HTML but much simpler).\n\n\nOSC\nThe Ohio Supercomputer Center.\n\n\nshell\nThe Unix shell is a command-line interface to the operating system that runs within a terminal. There are several shell flavors, and in this course, we will be working with the bash shell.\n\n\nNextflow\nSoftware for automated analysis workflow (pipeline) management.\n\n\nSlurm\nSoftware that schedules “compute jobs” (access to the main parts of the system) at OSC.\n\n\nUnix\nA family of operating systems that includes Mac and Linux, but not Windows.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/further-resources.html",
    "href": "ref/further-resources.html",
    "title": "Further Resources",
    "section": "",
    "text": "An extended version of this introduction\nOSC’s online asynchronous courses\nOSC’s new User Resource Guide 1"
  },
  {
    "objectID": "ref/further-resources.html#footnotes",
    "href": "ref/further-resources.html#footnotes",
    "title": "Further Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC.↩︎"
  },
  {
    "objectID": "ref/shell.html#basic-commands",
    "href": "ref/shell.html#basic-commands",
    "title": "Topic overview: Unix shell",
    "section": "1 Basic commands",
    "text": "1 Basic commands\n\n\n\nCommand\nDescription\nExamples / options\n\n\n\n\npwd\nPrint current working directory (dir).\npwd\n\n\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\n\n\ncd\nChange working dir. As with all commands, you can use an absolute path (starting from the root dir /) or a relative path (starting from the current working dir).\ncd /fs/ess/PAS1855 (With absolute path)  cd ../.. (Two levels up)  cd - (To previous dir)\n\n\ncp\nCopy files or, with -r, dirs and their contents (i.e., recursively).  If target is a dir, file will keep same name; otherwise, a new name can be provided.\ncp *.fq data/ (All .fq files into dir data)  cp my.fq data/new.fq (With new name)  cp -r data/ ~ (Copy dir and contents to home dir) \n\n\nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name)  mv my.fq my.fastq (Simple rename)  mv file1 file2 mydir/ (Last arg is destination)\n\n\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files)  rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\n\n\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir  mkdir -p new1/new2/new3\n\n\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\n\n\ncat\nPrint file contents to standard out (screen).\ncat my.txt  cat *.fa &gt; concat.fq (Concatenate files)\n\n\nhead\nPrint the first 10 lines of a file or specify number with -n &lt;n&gt; or shorthand -&lt;n&gt;.\nhead -n 40 my.fq (print 40 lines)  head -40 my.fq (equivalent)\n\n\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (“trick” to skip first line)  tail -f slurm.out (“follow” file)\n\n\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\n\n\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file:  column -s \",\" -t my.csv\n\n\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\n\n\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x).  ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable)  chmod a=r data/raw/* (Make data read-only)      -R recursive  ddddddddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#data-tools",
    "href": "ref/shell.html#data-tools",
    "title": "Topic overview: Unix shell",
    "section": "2 Data tools",
    "text": "2 Data tools\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples and options\n\n\n\n\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\n\n\ncut\nSelect one or more columns from a file.\nSelect columns 1-4:  cut -f 1-4 my.csv      -d \",\" comma as delimiter\n\n\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically,  column 2 reverse numerically:  sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\n\n\nuniq\nRemove consecutive duplicate lines (often from single-column selection): i.e., removes all duplicates if input is sorted.\nUnique values for column 2:  cut -f2 my.tsv | sort | uniq\n\n\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3:  cut -f3 my.tsv | sort | uniq -c\n\n\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV:  cat my.csv | tr \"\\t\" \",\"  Uppercase to lowercase: tr A-Z a-z &lt; in.txt &gt; out.txt      -d delete      -s squeeze\n\n\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines before match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC:  grep \"A[AG]C\" my.fa  Omit comment lines:  grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only"
  },
  {
    "objectID": "ref/shell.html#miscellaneous",
    "href": "ref/shell.html#miscellaneous",
    "title": "Topic overview: Unix shell",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nexample\n\n\n\n\n/\nRoot directory.\ncd /\n\n\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir)  Use ./ to execute script if not in $PATH:  ./myscript.sh\n\n\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n\n\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n\n\n$USER\nUser name.\nmkdir $USER\n\n\n&gt;\nRedirect standard out to a file.\necho \"My 1st line\" &gt; myfile.txt\n\n\n&gt;&gt;\nAppend standard out to a file.\necho \"My 2nd line\" &gt;&gt; myfile.txt\n\n\n2&gt;\nRedirect standard error to a file.\nSend standard out and standard error for a script to separate files:  myscript.sh &gt;log.txt 2&gt; err.txt\n\n\n&&gt;\nRedirect standard out and standard error to a file.\nmyscript.sh &&gt; log.txt\n\n\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nThe output of the sort command will be piped into head to show the first lines:  sort myfile.txt | head\n\n\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 =&gt; 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31)  touch fig1{A..F} (fig1A, fig1B, …, fig1F)  mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n\n\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files:  echo \"I see $(ls *fastq | wc -l) files\"  Substitute with date in YYYY-MM-DD format:  mkdir results_$(date +%F)  nlines=$(wc -l &lt; $infile)\n\n\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name.  ddddddddddddddddddddddddddddddddddddd\nAdd dir to path:  PATH=$PATH:/new/dir  (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#shell-wildcards",
    "href": "ref/shell.html#shell-wildcards",
    "title": "Topic overview: Unix shell",
    "section": "4 Shell wildcards",
    "text": "4 Shell wildcards\n\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n\n*\nAny number of any character, including nothing.\nls data/*fastq.gz (Matches any file ending in “fastq.gz”)  ls *R1* (Matches any file containing “R1” somewhere in the name.)\n\n\n?\nAny single character.\nls sample1_?.fastq.gz (Matches sample1_A.fastq.gz but not sample1_AA.fastq.gz)\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets.  ddddddddddddddddddddddddddddddddddddd\nls fig1[A-C] (Matches fig1A, fig1B, fig1C)  ls fig[0-3] (Matches fig0, fig1, fig2, fig3)  ls fig[^4]* (Does not match files with a “4” after “fig”)  ddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#regular-expressions",
    "href": "ref/shell.html#regular-expressions",
    "title": "Topic overview: Unix shell",
    "section": "5 Regular expressions",
    "text": "5 Regular expressions\n\n\n\n\n\n\n“ERE” = GNU Extended regular expressions\n\n\n\nWhere it says “yes” in the ERE column, the symbol in questions needs to have ERE turned on in order to work1: use a -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE.\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nERE2\nMatches\nExample\n\n\n\n\n.\n\nAny single character\nMatch Olfr with none or any characters after it:  grep -o \"Olfr.*\"\n\n\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n\n\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits:  grep -E [0-9]+\n\n\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit:  grep -E [0-9]?\n\n\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs:  grep -E \"G{50,100}\"\n\n\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines:  grep -v \"^$\"  Exclude lines beginning with a “#”:  grep -v \"^#\"\n\n\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\n\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\n\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed.\nMatch gene_id followed by a space and a “word”:  grep -E -o 'gene_id \"\\w+\"'  Change any word character to X:  sed s/\\w/X/\n\n\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either intron or exon:  grep -E \"intron|exon\"\n\n\n()\nyes\nGrouping\nFind “AAG” repeated 10 times:  grep (AAG){10}\n\n\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc.  ddddddddddddddddddddddddddddddddddddd\nInvert order of two words:  sed -E 's/(\\w+) (\\w+)/\\2 \\1/'  ddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref/shell.html#more-details-for-a-few-commands",
    "href": "ref/shell.html#more-details-for-a-few-commands",
    "title": "Topic overview: Unix shell",
    "section": "6 More details for a few commands",
    "text": "6 More details for a few commands\n\n6.1 less\n\n\n\n\n\n\n\nKey\nFunction\n\n\n\n\nq\nExit less\n\n\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\n\n\nd / u\nGo down / up half a page.\n\n\ng / G\nGo to the first / last line (home / end also work).\n\n\n/&lt;pattern&gt; or ?&lt;pattern&gt;\nSearch for &lt;pattern&gt; forwards / backwards: type your search after / or ?.\n\n\nn / N\nWhen searching, go to next / previous search match.\ndddddddddddddddddddddddddddddddddddddddddddddddddddd\n\n\n\n\n\n\n6.2 sed\n\nsed flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n-E\nUse extended regular expressions\n\n\n-e\nWhen using multiple expressions, precede each with -e\n\n\n-i\nEdit a file in place\n\n\n-n\nDon’t print lines unless specified with p modifier\n\n\n\n\n\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (&gt;1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#&gt; chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#&gt; words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#&gt; sampleA.fastq\n\n\n\n\n6.3 awk\n\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\n\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 &lt; 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\n\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 &gt; 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\n\n\nawk comparison and logical operators\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\na == b\na is equal to b\n\n\na != b\na is not equal to b\n\n\na &lt; b\na is less than b\n\n\na &gt; b\na is greater than b\n\n\na &lt;= b\na is less than or equal to b\n\n\na &gt;= b\na is greater than or equal to b\n\n\na ~ /b/\na matches regular expression pattern b\n\n\na !~ /b/\na does not match regular expression pattern b\n\n\na && b\nlogical and: a and b\n\n\na || b\nlogical or: a or b [note typo in Buffalo]\n\n\n!a\nnot a (logical negation)\n\n\n\n\n\nawk special variables and keywords\n\n\n\n\n\n\n\nkeyword/variable\nmeaning\n\n\n\n\nBEGIN\nUsed as a pattern that matches the start of the file\n\n\nEND\nUsed as a pattern that matches the end of the file\n\n\nNR\nNumber of Records (running count; in END: total nr. of lines)\n\n\nNF\nNumber of Fields (for each record)\n\n\n$0\nContains entire record (usually a line)\n\n\n$1 - $n\nContains one column each\n\n\nFS\nInput Field Separator (default: any whitespace)\n\n\nOFS\nOutput Field Separator (default: single space)\n\n\nRS\nInput Record Separator (default: newline)\n\n\nORS\nOutput Record Separator (default: newline)\n\n\n\n\n\nSome awk functions\n\n\n\n\n\n\n\nFunction\nMeaning\n\n\n\n\nlength(&lt;string&gt;)\nReturn number of characters\n\n\ntolower(&lt;string&gt;)\nConvert to lowercase\n\n\ntoupper(&lt;string&gt;)\nConvert to uppercase\n\n\nsubstr(&lt;string&gt;, &lt;start&gt;, &lt;end&gt;)\nReturn substring\n\n\nsub(&lt;from&gt;, &lt;to&gt;, &lt;string&gt;)\nSubstitute (replace) regex\n\n\ngsub(&lt;from&gt;, &lt;to&gt; &lt;string&gt;)\n&gt;1 substitution per line\n\n\nprint\nPrint, e.g. column: print $1\n\n\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\n\n\nnext\nDon’t process later fields: to next iteration"
  },
  {
    "objectID": "ref/shell.html#keyboard-shortcuts",
    "href": "ref/shell.html#keyboard-shortcuts",
    "title": "Topic overview: Unix shell",
    "section": "7 Keyboard shortcuts",
    "text": "7 Keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl+Shift+C\nCopy selected text\n\n\nCtrl+Shift+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line3\n\n\nCtrl+W\nCut word before before cursor4\n\n\nCtrl+Y\nPaste (“yank”)\n\n\nAlt+.\nLast argument of previous command (very useful!)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nKill (stop) currently active command\n\n\nCtrl+D\nExit (a program or the shell depending on the context)\n\n\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background."
  },
  {
    "objectID": "ref/shell.html#footnotes",
    "href": "ref/shell.html#footnotes",
    "title": "Topic overview: Unix shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen using the default regular expressions in grep and sed, Basic Regular Expressions (BRE), the symbol would need to be preceded by a backslash to work.↩︎\nGNU Extended Regular Expressions↩︎\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.↩︎"
  },
  {
    "objectID": "week6/w6_overview.html",
    "href": "week6/w6_overview.html",
    "title": "Week 6",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-jelmer",
    "href": "week1/w1_course-intro.html#introductions-jelmer",
    "title": "Course Intro",
    "section": "Introductions: Jelmer",
    "text": "Introductions: Jelmer\n\nBioinformatician at MCIC in Wooster since June 2020\n\nThe majority of my time is spent providing research assistance,\nworking with grad students and postdocs on mostly genomic & transcriptomic data\nI also teach: some courses, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\n\n\n\nIn my free time, I enjoy bird watching – locally & all across the world"
  },
  {
    "objectID": "week1/w1_course-intro.html#introductions-you",
    "href": "week1/w1_course-intro.html#introductions-you",
    "title": "Course Intro",
    "section": "Introductions: You",
    "text": "Introductions: You\n\nName\nLab and Department\nResearch interests and/or current research topics\nSomething about you that is not work-related"
  },
  {
    "objectID": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "href": "week1/w1_course-intro.html#the-core-goals-of-this-course",
    "title": "Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills to:\n\nDo your research more reproducibly and efficiently\nPrepare yourself for working with large “omics” datasets"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility",
    "href": "week1/w1_course-intro.html#course-background-reproducibility",
    "title": "Course Intro",
    "section": "Course background: Reproducibility",
    "text": "Course background: Reproducibility\n\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\nOur focus is on #2."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "href": "week1/w1_course-intro.html#course-background-reproducibility-cont.",
    "title": "Course Intro",
    "section": "Course background: Reproducibility (cont.)",
    "text": "Course background: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n—Karl Broman\n\n\n\nAlso important:\n\nProject organization and documentation (week 2)\nSharing data and code (for code: Git & GitHub, week 3)\nHow you code (e.g. week 4 - shell scripts, and 6 - Nextflow)\n\n\n\n\n\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "href": "week1/w1_course-intro.html#course-background-efficiency-and-automation",
    "title": "Course Intro",
    "section": "Course background: Efficiency and automation",
    "text": "Course background: Efficiency and automation\n\nUsing code enables you to work more efficiently and automatically —\nparticularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo a project after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week1/w1_course-intro.html#course-background-omics-data",
    "href": "week1/w1_course-intro.html#course-background-omics-data",
    "title": "Course Intro",
    "section": "Course background: Omics data",
    "text": "Course background: Omics data\n\nOmics data is increasingly important in biology, and most notably includes:\n\nGenomics + Microbiomics + Metagenomics\nTranscriptomics\nProteomics\nMetabolomics\n\n\n\n\n\nWhile we’ll be using some example omics datasets, this course will not teach you how to analyze omics data in full — our focus is on fundamental computational skills.\n\n\n\n\n\n\n\nI should say that this course is less relevant for proteomics and metabolomics, especially in its slimmed-down half-semester form with no R."
  },
  {
    "objectID": "week1/w1_course-intro.html#the-unix-shell-shell-scripts-wk-1-4",
    "href": "week1/w1_course-intro.html#the-unix-shell-shell-scripts-wk-1-4",
    "title": "Course Intro",
    "section": "The Unix shell & shell scripts (Wk 1 & 4)",
    "text": "The Unix shell & shell scripts (Wk 1 & 4)\nBeing able to work in the Unix shell is a fundamental skill in computational biology.\n\n\nYou’ll spend a lot of time with the Unix shell, starting this week\nWe’ll also write shell scripts, and will use VS Code for this and other purposes.\n\n\n\n\n\n\n\n\n\n\nBash (shell language)\n\n\n\n\n\n\n\n\nVS Code"
  },
  {
    "objectID": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "href": "week1/w1_course-intro.html#project-organization-and-markdown-wk-2",
    "title": "Course Intro",
    "section": "Project organization and Markdown (Wk 2)",
    "text": "Project organization and Markdown (Wk 2)\nGood project organization & documentation is a necessary starting point for reproducible research.\n\n\nYou’ll learn best practices for project organization, file naming, etc.\nTo document and report what you are doing, you’ll use Markdown files.\n\n\n\n\n\n\n\n\nMarkdown"
  },
  {
    "objectID": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "href": "week1/w1_course-intro.html#version-control-with-git-and-github-wk-3",
    "title": "Course Intro",
    "section": "Version control with Git and GitHub (Wk 3)",
    "text": "Version control with Git and GitHub (Wk 3)\nUsing version control, you can more effectively keep track of project progress, collaborate, share code, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories).\nYou’ll also use Git+GitHub to hand in your final project assignments."
  },
  {
    "objectID": "week1/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "href": "week1/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "title": "Course Intro",
    "section": "High-performance computing with OSC (Wk 5)",
    "text": "High-performance computing with OSC (Wk 5)\nThanks to supercomputer resources, you can work with very large datasets at speed — running up to 100s of analyses in parallel, and using much larger amounts of memory and storage space than a personal computer has.\n\n\n\nWe will use OSC throughout the course, and you’ll get a brief intro to it today.\nIn week 5, we’ll learn to submit shell scripts as OSC “batch jobs” with Slurm, and use Conda to manage software."
  },
  {
    "objectID": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "href": "week1/w1_course-intro.html#automated-workflow-management-wk-6",
    "title": "Course Intro",
    "section": "Automated workflow management (Wk 6)",
    "text": "Automated workflow management (Wk 6)\nUsing a workflow written with a workflow manager, you can run and rerun entire analysis pipeline with a single command, and easily change and rerun parts of it, too.\n\n\nWe’ll use the workflow language Nextflow."
  },
  {
    "objectID": "week1/w1_course-intro.html#zoom",
    "href": "week1/w1_course-intro.html#zoom",
    "title": "Course Intro",
    "section": "Zoom",
    "text": "Zoom\n\nBe muted by default, but feel free to unmute yourself to ask questions any time.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week1/w1_course-intro.html#websites-books",
    "href": "week1/w1_course-intro.html#websites-books",
    "title": "Course Intro",
    "section": "Websites & Books",
    "text": "Websites & Books\n\nI will only use the CarmenCanvas website for Announcements.\n\n\n\nThe GitHub website is the main website for the course, containing all course material:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nExercises\nFinal project assignment information\n\n\n\n\n\nBooks:\n\nComputing Skills for Biologists (“CSB”; Allesina & Wilmes 2019)\nBioinformatics Data Skills (“Buffalo”; Buffalo 2015)"
  },
  {
    "objectID": "week1/w1_course-intro.html#final-project-graded",
    "href": "week1/w1_course-intro.html#final-project-graded",
    "title": "Course Intro",
    "section": "Final project (graded)",
    "text": "Final project (graded)\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 4 – 10 points)\nII: Draft (due week 6 – 10 points)\nIII: Oral presentations on Zoom (week 7 – 10 points)\nIV: Final submission (due April 29 – 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIf you have your own data set & analysis ideas, that is ideal. If not, I can provide you with this.\nMore information about the final project will follow in week 2 or 3."
  },
  {
    "objectID": "week1/w1_course-intro.html#ungraded-homework",
    "href": "week1/w1_course-intro.html#ungraded-homework",
    "title": "Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings — somewhat up to you when to do these\nWeekly exercises — I recommend doing these on Fridays\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nWeekly materials & homework\n\n\nI will try add the materials for each week on the preceding Friday — at the least the week’s overview and readings.\nNone of this homework had to be handed in."
  },
  {
    "objectID": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "href": "week1/w1_course-intro.html#weekly-recitation-on-monday",
    "title": "Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nIf there is interest, we can have a weekly Monday meeting in which we go through the exercises for the preceding week.\n\nIf you’re interested, indicate your availability here:\nhttps://www.when2meet.com/?23841132-KV8fY"
  },
  {
    "objectID": "week1/w1_course-intro.html#rest-of-this-week",
    "href": "week1/w1_course-intro.html#rest-of-this-week",
    "title": "Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nBrief introduction to the Ohio Supercomputer Center (OSC)\n\n\n\nUnix shell basics\n\n\n\nHomework:\n\nReadings: mostly CSB Chapter 1\nExercises"
  },
  {
    "objectID": "week1/w1_shell.html#goals-for-this-session",
    "href": "week1/w1_shell.html#goals-for-this-session",
    "title": "Unix Shell Basics",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nIn this session, we’ll cover much of the material from CSB Chapter 1, to learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files.\n\n\n\n\n\n\n\n\nSee the “Topic Overview” page on the Unix shell for an overview of shell commands we’ll cover during this course."
  },
  {
    "objectID": "week1/w1_shell.html#introduction-ch.-1.1-1.2",
    "href": "week1/w1_shell.html#introduction-ch.-1.1-1.2",
    "title": "Unix Shell Basics",
    "section": "2 Introduction (Ch. 1.1-1.2)",
    "text": "2 Introduction (Ch. 1.1-1.2)\n\n2.1 Some terminology\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “*nix”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface2 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably.\n\n\n\n2.2 Why use the Unix shell?\n\nVersus programs with graphical user interfaces:\n\nUsing software\nBest or only option to use many programs, especially in bioinformatics.\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nThe shell keeps a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data\nRemote computing – especially HPCs\nIt is often only possible to work in a terminal when doing remote computing.\n\nVersus scripting languages like Python or R:\n\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nThe Unix shell has a direct interface to other programs."
  },
  {
    "objectID": "week1/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "href": "week1/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "title": "Unix Shell Basics",
    "section": "3 Getting started with Unix (Ch. 1.3)",
    "text": "3 Getting started with Unix (Ch. 1.3)\n\nUnix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nFor example, the path to our OSC project’s dir is /fs/ess/PAS2700. This means: the dir PAS2700 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/&lt;a-project&gt;/&lt;username&gt;.\n\n\n\n\n\n\nGeneric example, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in OSC dir structure"
  },
  {
    "objectID": "week1/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "href": "week1/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "title": "Unix Shell Basics",
    "section": "4 Getting started with the shell (Ch. 1.4)",
    "text": "4 Getting started with the shell (Ch. 1.4)\n\n4.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\n\n\n\n4.2 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nClearing the screen\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\n\n\n\n\n\n4.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n4.4 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this is supposed to be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSo, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options3.\n\n\n\n\n\n4.5 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet4. (You can also combine this new option with other options, if you want.)\n\n\n\nSolution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nSolution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\nBonus: Try to figure out / guess what the cal [options] [[[day] month] year] means. Can you print a calendar for April 2017?\n\n\n\nSolution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\n\n\n\n\n4.6 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2700\n# Double-check that we moved:\npwd\n/fs/ess/PAS2700\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2700\n-bash: cd: /fs/ess/PAs2700: No such file or directory\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names (hence the error above).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n4.7 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2700).\nPress Enter. What does the resulting error mean?\nbash: /fs/ess/PAS2700/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you perhaps expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2700/\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.8 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell5.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.9 Create your own dir & get the CSB data\nOur base OSC directory for the course is the /fs/ess/PAS2700 dir we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.10 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2700)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. CSB/unix/sandbox)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2700/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2700/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week1/w1_shell.html#basic-unix-commands-ch.-1.5",
    "href": "week1/w1_shell.html#basic-unix-commands-ch.-1.5",
    "title": "Unix Shell Basics",
    "section": "5 Basic Unix commands (Ch. 1.5)",
    "text": "5 Basic Unix commands (Ch. 1.5)\n\n5.1 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nFor which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n5.2 cp to copy files\nThe cp command copies files and/or dirs from one location to another. It has two required arguments: what you want to copy (source), and where you want to copy it to (destination). Its basic syntax is cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n5.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n5.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm Buzzard2015_about.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n5.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example6:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "week1/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "href": "week1/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "title": "Unix Shell Basics",
    "section": "6 Advanced Unix commands (Ch. 1.6)",
    "text": "6 Advanced Unix commands (Ch. 1.6)\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n6.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n6.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument with a file name like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n6.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as Unix “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files (Click to expand)\n\n\n\n\n\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\n\nThe cut command will select/“cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\n6.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n6.5 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern.7\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options — more in Week 4:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n6.6 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab8. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\n\nDelete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nRemove consecutive duplicates a’s:\necho \"aabbccddee\" | tr -s a\nabbccddee\n\n\n\n\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension) in the sandbox dir (that’s not where you are located yourself).\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "week1/w1_shell.html#wrap-up-the-unix-philosophy",
    "href": "week1/w1_shell.html#wrap-up-the-unix-philosophy",
    "title": "Unix Shell Basics",
    "section": "7 Wrap-up & the Unix philosophy",
    "text": "7 Wrap-up & the Unix philosophy\n\n7.1 Covered in the chapter but not in today’s lecture\n\nThe less pager to view files\nThe find command to find files\nShowing and changing file permissions\nBasic shell scripting\nfor loops\n$PATH and bash profile settings\n\nWe’ll cover all of these in class over the next few weeks.\n\n\n\n7.2 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n7.3 The Unix shell in the weeks ahead\n\nIn all course weeks, we will be working in the Unix shell, though our focus in several cases will be on a specific tool, such as Git in week 3.\nIn week 4, we’ll fully focus on the shell and shell scripting."
  },
  {
    "objectID": "week1/w1_shell.html#footnotes",
    "href": "week1/w1_shell.html#footnotes",
    "title": "Unix Shell Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\n Though some commands are flexible and accept either order.↩︎\nThere really is only one proper additional options: two others reflect the defaults, and then there’s the version option.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expressions, this is not strictly necessary, it’s good habit to always quote.↩︎\nWe’ll learn more about regular expressions in Week 4.↩︎"
  },
  {
    "objectID": "week1/w1_exercises.html",
    "href": "week1/w1_exercises.html",
    "title": "Week 1 Exercises",
    "section": "",
    "text": "The following are some of the exercises from Chapter 1 of the CSB book."
  },
  {
    "objectID": "week1/w1_exercises.html#getting-set-up",
    "href": "week1/w1_exercises.html#getting-set-up",
    "title": "Week 1 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files in your personal dir within /fs/ess/PAS2700 from Thursday’s class. (If not, cd to /fs/ess/PAS2700/users/$USER, and run git clone https://github.com/CSB-book/CSB.git — this downloads the CSB directory referred to in the first step of the first exercise.)"
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.1",
    "href": "week1/w1_exercises.html#intermezzo-1.1",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.1",
    "text": "Intermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within CSB/python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.2",
    "href": "week1/w1_exercises.html#intermezzo-1.2",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.2",
    "text": "Intermezzo 1.2\nTo familiarize yourself with using basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 23) demonstrates the use of the touch command to create a new, empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week1/w1_exercises.html#intermezzo-1.3",
    "href": "week1/w1_exercises.html#intermezzo-1.3",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.3",
    "text": "Intermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv (in CSB/unix/data/) in alphabetical order, which is the first species? And which is the last?\n\n\nShow hints\n\nYou can either first select the 5th column using cut and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter (if you use the latter approach, check the book for how to do that with sort).\nTo view just the first or the last line so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nCheck the first line of the file to see which column contains the family, and then select the relevant column with cut.\nUse the “tail trick” we saw in class to exclude the first line.\nRemember to sort before using uniq."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "href": "week1/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.1: Next-Generation Sequencing Data",
    "text": "Exercise 1.10.1: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n4. How many “contigs” (FASTA entries, in this case) are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pass the output of grep to wc -l.\n\n5. Modify my_file.fasta to replace the original “two-spaces” delimiter with a comma (i.e. don’t just print the output to screen, but end up with a modified file). You’ll probably want to take a look at the “output file hints” below to see how you can end up with modified file contents.\n\n\nShow output file hints\n\n Due to the “streaming” nature of Unix commands, we can’t write output to a file that also serves as input (see here). So the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n\n\nShow other hints\n\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically — we didn’t see those sort options in class, so check the book for details."
  },
  {
    "objectID": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "href": "week1/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.2: Hormone Levels in Baboons",
    "text": "Exercise 1.10.2: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. The data file is in CSB/unix/data/Gesquiere2011_data.csv.\nEvery individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can first use cut to extract just the maleID column from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week1/w1_exercises.html#solutions",
    "href": "week1/w1_exercises.html#solutions",
    "title": "Week 1 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nIntermezzo 1.1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\n\nWith cd -:\n# The '-' shortcut for cd will move you back to the previously visited dir\n# (Note: you can't keep going back with this: using it a second time will toggle you \"forward\" again.)\ncd -\nUsing a relative path:\ncd ../data\n\n\n\n\n\nIntermezzo 1.2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n9515 Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\nPapers and reviews  toremove.txt\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nIntermezzo 1.3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\nAbditomys latidens\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\nZyzomys woodwardi\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t \";\" -k 5 Pacifici2013_data.csv | head -n 1\n42641;Rodentia;Muridae;Abditomys;Abditomys latidens;268.09;PanTHERIA;no information;no information;no information;no information;no information;no information;639.6318318208;Mean_family_same_body_mass\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n152\n\n\n\n\nExercise 1.10.1: Next-Generation Sequencing Data\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 ../data/Marra2014_data.fasta\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n560K    ../data/Marra2014_data.fasta\n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,. Importantly, we also write the output to a new file (see the Hints for details):\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nIf we want to change the original file, we can now overwrite it as follows:\nmv my_file.tmp my_file.fasta\nLet’s take a look to check whether out delimiter replacement worked:\ngrep \"&gt;\" my_file.fasta | head\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n&gt;contig00003,length=541,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00004,length=291,numreads=3,gene=isogroup00001,status=it_thresh\n&gt;contig00005,length=580,numreads=12,gene=isogroup00001,status=it_thresh\n&gt;contig00006,length=3288,numreads=35,gene=isogroup00001,status=it_thresh\n&gt;contig00008,length=1119,numreads=10,gene=isogroup00001,status=it_thresh\n&gt;contig00010,length=202,numreads=4,gene=isogroup00001,status=it_thresh\n&gt;contig00011,length=5563,numreads=61,gene=isogroup00001,status=it_thresh\n&gt;contig00012,length=824,numreads=10,gene=isogroup00001,status=it_thresh\n\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\ngrep '&gt;' my_file.fasta | head -n 2\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, add cut to extract the 4th column:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\ngene=isogroup00001\ngene=isogroup00001\nFinally, add sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n43\n\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nFirst, we need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n&gt;contig00001,numreads=2\n&gt;contig00002,numreads=8\n&gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t '=' to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n&gt;contig00089,numreads=1\n&gt;contig00176,numreads=1\n&gt;contig00210,numreads=1\n&gt;contig00001,numreads=2\n&gt;contig00003,numreads=2\nAdding the sort option -r, we can sort in reverse order, which tells us that contig00302 has the highest coverage, with 3330 reads:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n&gt;contig00302,numreads=3330\n\n\n\n\n\nExercise 1.10.2: Hormone Levels in Baboons\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s move back into the data dir:\ncd ../data\nNext, let’s take a look at the structure of the file:\nhead -n 3 Gesquiere2011_data.csv\nmaleID        GC      T\n1     66.9    64.57\n1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 Gesquiere2011_data.csv | head -n 3\nmaleID\n1\n1\n\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the option -w to match whole “words” – this will make it match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 3\n61\n# For maleID 27\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 27\n5"
  },
  {
    "objectID": "week0/overview.html#assignments",
    "href": "week0/overview.html#assignments",
    "title": "Before the course starts",
    "section": "1 Assignments",
    "text": "1 Assignments\nPlease complete these assignments in the week of Feb 19th:\n\nFill out the pre-course survey\nCreate or check your OSC account"
  },
  {
    "objectID": "week0/overview.html#readings",
    "href": "week0/overview.html#readings",
    "title": "Before the course starts",
    "section": "2 Readings",
    "text": "2 Readings\n\nSyllabus\nPlease read the updated syllabus, which you can find here and on the CarmenCanvas site for the course.\n\n\nAdditional information\n\nCourse websites\nThe main place to access information about this course is this github.io website. I will only use the CarmenCanvas site for this course for announcements and as a place to share some files with you.\n\n\nNo R in the course\nI’ve had to remove the modules on R while transitioning this course from 3-credit full-semester to 2-credit half-semester. If this is disappointing to you and you want to learn R at OSU, I can recommend the weekly Code Club meeting that I co-organize. And if you’re also interested in microbial metabarcoding data, or would like to learn how you can use R in a bioinformatics/omics data context, contact Plant Pathology professor Soledad Benitez-Ponce to sign up for a metabarcoding workshop during Spring break from March 13th-15th.\n\n\nGlossary\nIf some of the terms in the description of the course material are unfamiliar to you, take a look at the glossary page of this website. Of course, you will learn a lot more about these terms and concepts during the course!"
  },
  {
    "objectID": "week7/w7_overview.html",
    "href": "week7/w7_overview.html",
    "title": "Week 7",
    "section": "",
    "text": "Content TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/finalproject_submission.html",
    "href": "finalproj/finalproject_submission.html",
    "title": "Final project: submission",
    "section": "",
    "text": "The submission of your final project is due on April 30th. [20 points]\n\n0.1 How to submit\nAs usual, open a new GitHub issue for your repository and tag @jelmerp in the text body of the Issue.\n\n\n0.2 What to submit\nYour repository should now contain:\n\nA finished set of scripts.\nFinal documentation in one or more README files that clearly describes:\n\nWhat the project does as a whole.\nWhat each script does.\nWhere to access the data at OSC, assuming that the data is not in your repository.\nHow the project’s scripts can be rerun using a single script or Snakefile.\n\nA single script or Snakefile that aims to rerun the full workflow.\nA file (e.g. submission_notes.md) or a section in your main README file that provides some additional information for the instructor to grade your project appropriately. Some hypothetical examples of things you may want to include:\n\nAdditional instructions the instructor will need to try and rerun your project.\nYou want to alert the instructor to some files files in the repository that should be ignored.\nYou want to explain why you don’t have a functioning script or Snakefile, or why you didn’t run any SLURM jobs (which can be acceptable in some cases).\n\n\n\n\n0.3 Graded aspects\nBelow is a long list of graded aspects and what to aim for if you want a perfect score. I’m providing a lot of detail here, so there are no surprises. The TLDR is that you should aim to have a reproducible, well-organized and well-documented workflow – workflow size/complexity on the other hand, is fairly unimportant. (See also the General Info page for the final project for some more general background.)\n\n\n\n\n\n\n\n\nCategory\nMax.  score\nMax. score if your project (examples given):\n\n\n\n\nProject organization\n2\n\nHas a clear and appropriate directory structure.\nHas informative and appropriate directory and file names.\nDoes not mix data, scripts, and results in individual directories.\n\n\n\nProject background and documentation\n2\n\nHas a clear description of its background and goals.\nHas a clear description of how different scripts are being used to achieve these goals.\nWhere appropriate, indicates what is still a work-in-progress (and optionally future directions).\n\n\n\nScript documentation\n2\n\nUses extensive (yet succinct) comments to document what is being done within scripts.\n\n\n\nGood practices in scripts\n4\n\nUses no absolute paths in scripts.\nUses scripts that take arguments where appropriate and minimizes “hard-coding” of potentially variable things like input/output dirs, file names, and some software settings. Any hard-coded variables/constants that are present are clearly set at the top of scripts.\nHas individual scripts that are not overly long and don’t do multiple unrelated things.\nHas no or only clearly annotated lines that are “commented out” in scripts.\nUses Bash scripts with proper set settings, and similar good practices as taught in the course.\n\n\n\nCoding quality and complexity\n3\n\nHas code that demonstrates an understanding of topics covered in the course.\nHas code that is appropriately broken up in small parts within scripts, e.g. with functions in Python.\nUses tools and commands that are (by and large) appropriate to accomplish its goals. (I will not dig in to fine details and parameter settings.)\n\n\n\nWorkflow  reproducibility\n3\n\nHas a script or Snakefile that includes all steps in the workflow and that can be run by anybody with access to your repository and the raw data files.\nHas information for the instructor (or any other reader of the project!) about where at OSC to find the raw data files and other details needed to try to rerun the analyses.\nBonus: good software management, e.g. Conda environments (preferred) or OSC modules and no manual installs unless necessary; YAML files describing environments.\n\n\n\nSLURM jobs at OSC\n2\n\nHas one or more scripts that are run as OSC jobs at SLURM.\nUses appropriate SLURM directives – either in the scripts or in a Snakemake profile YAML file.\n\n\n\nVersion control\nddddddddddddddddd\n2\ndddddd\n\nHas Git commit messages that are informative.\nHas reasonably appropriate commits, e.g. individual Git commits don’t consist of multiple completely unrelated edits.\nHas a single .gitignore file that ignores files like large raw data files, and in most cases, results files.\nBonus: has a Git tag for the submitted version.\n\ndddddddddddddddddddddddddddddddddddddddddddddd dddddddd dddddddd\n\n\n\n\n\n0.4 Questions and advice\nDon’t hesitate to contact Jelmer (or, where fitting, Zach) for questions about topics like:\n\nSpecific expectations for the final project that are unclear to you.\nWhether you are on the right track in making some adjustments that I asked for after your progress report.\nAdvice on how to code or organize aspects of your project.\n\nWe’re happy to answer questions by e-mail or in a Zoom meeting!\n\n\n0.5 Late submissions\nLate submission may be accommodated depending on circumstances, but you will need to contact Jelmer before 3 pm on April 30th and we can take it from there.\nFor late submissions with no advance notice, 4 points will be subtracted for each day the submission is late. (Things like forgetting to open an Issue won’t lead to subtracted points.)\n\n\n0.6 Good luck!!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "finalproj/finalproject_present.html",
    "href": "finalproj/finalproject_present.html",
    "title": "Final project: presentations",
    "section": "",
    "text": "Every student is expected to give a 10-minute presentation about their final project during the Zoom sessions on April 20th and 22nd. [10 points]\n\nA few pointers:\n\nAim for the presentation itself to take about 10 minutes – the acceptable range is roughly 8-11 minutes.\nAfterwards, be prepared to answer a few questions both from your peers and the TA or instructor. You’re also expected to ask one or more questions (in total) to other students after their presentations.\nPrepare at least several slides. Your entire presentation can be given using slides. But if you want, you can also switch to showing your actual repository / scripts during part of the presentation (just be mindful of font sizes if you do the latter).\nStart with some general background about the data / research project, and an overview of the goals of the project.\nYou can see for yourself if you would like to run through some code line-by-line, or give a more high-level overview of the code you’ve written.\nAt the end, briefly mention what you still have to do – and if this is work that you will continue after this course, you can also discuss this in a broader sense. You can even explicitly ask for some advice, if you want.\n\nWhat you will be graded on:\n\nTechnical content [3 points]\nContextualization [2 points]\nDelivery [2 points]\nClarity [2 points]\nQuestions for other students [1 point]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 6193), a 2-credit Independent Studies course at Ohio State University during the Spring semester of 2024.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which usually cannot be analyzed on a desktop computer, where most software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, installing software, and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell basics and tools\nShell scripting\nComputing at OSC with Slurm batch jobs and Conda software management\nVersion control with Git and GitHub\nProject documentation with Markdown and project organization\nReproducible workflows with Nextflow\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#vs-code",
    "href": "week2/w2_vscode-markdown.html#vs-code",
    "title": "VS Code and Markdown",
    "section": "1 VS Code",
    "text": "1 VS Code\n\n1.1 Why VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, and it’s also called “Code Server” at OSC.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code.\nSome advantages of VS Code:\n\nWorks with all operating systems, is free, and open source.\nHas an integrated terminal.\nVery popular nowadays – lots of development going on including by users (extensions).\nAvailable at OSC OnDemand (and also allows you to SSH-tunnel-in with your local installation).\n\n\n\n\n1.2 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2700.\nThe “Number of hours” we want to make a reservation for: 2\nThe “Working Directory” for the program: your personal folder in /fs/ess/PAS2700/users (e.g. /fs/ess/PAS2700/users/jelmer)\nThe “Codeserver Version”: 4.8 (most recent)\n\nClick Launch.\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Welcome/Get Started page — you don’t have to go through steps that may be suggested there.\n\n\n\n\n1.3 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide/Primary) Side Bar options:\n\nExplorer: File browser & outline for the active file.\nSearch: To search recursively across all files in the active folder.\nSource Control: To work with Git (next week).\nDebugger\nExtensions: To install extensions (up soon).\n\n\n\n\n\n\n\n\nToggle (hide/show) the side bars\n\n\n\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar and the Primary Side Bar.\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\n\n\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nCreate a directory for this week, e.g.:\n# You should be in your personal dir in /fs/ess/PAS2700\npwd\n/fs/ess/PAS2700/users/jelmer\nmkdir week02\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Welcome document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n Let’s create and save a new file:\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside the dir you just created, as a Markdown file, e.g. markdown-intro.md. (Markdown files have the extension .md.)\n\n\n\n\n\n1.4 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/ess/PAS2700/users/$USER.\n\n\n\n\n\n\nIf you need to switch folders, click      &gt;   File   &gt;   Open Folder.\n\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in that it will:\n\nRe-open any files you had open in the editor pane\nRe-open a terminal if you had one active\n\nThis is quite convenient, especially when you start working on multiple projects and frequently switch between those.\n\n\n\n\nSome tips and tricks\n\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\nSpecific useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you’ll have to replace Ctrl with ⌘):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl/⌘+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl/⌘+Shift+K will delete a line\nAlt/Option+⬆/⬇ will move lines up or down.\n\n\n\n\n\n\n\n Exercise: Install two extensions\nClick the gear icon  and then Extensions, and search for and then install:\n\nshellcheck (by simonwong) — this will check our shell scripts later on!\nRainbow CSV (by mechatroner) — make CSV/TSV files easier to view with column-based colors"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#an-introduction-to-markdown",
    "href": "week2/w2_vscode-markdown.html#an-introduction-to-markdown",
    "title": "VS Code and Markdown",
    "section": "2 An introduction to Markdown",
    "text": "2 An introduction to Markdown\nMarkdown is a very lightweight text markup language that is:\n\nEasy to write — a dozen or so syntax constructs is nearly all you use.\nEasy to read — also in its raw (non-rendered) form.\n\nFor example, surrounding one or more characters by single or double asterisks (*) will make those characters italic or bold, respectively:\n\nWhen you write *italic example* this will be rendered as: italic example.\nWhen you write **bold example**this will be rendered as: bold example.\n\nSource Markdown files are plain text files (they can be “rendered” to HTML or PDF). I recommend that you use Markdown files (.md) instead of plain text (.txt) files to document your research projects as outlined in the previous session.\n\n\n\n\n\n\nMarkdown documentation\n\n\n\nLearn more about Markdown and its syntax in this excellent documentation: https://www.markdownguide.org.\n\n\n\n\nMarkdown in VS Code\nBelow, we’ll be trying some Markdown syntax in the markdown-intro.md file we created earlier.\nWhen you save a file in VS Code with an .md extension, as you have done:\n\nSome formatting will be automatically applied in the editor.\nYou can open a live rendered preview by pressing the icon to “Open Preview to the Side” (top-right corner):\n\n\n\n\n\n\nThat will look something like this in VS Code:\n\n\n\n\n\n\n\n\n2.1 Most common syntax\nHere is an overview of the most commonly used Markdown syntax:\n\n\n\nSyntax\nResult\n\n\n\n\n*italic*\nitalic (alternative: single _)\n\n\n**bold**\nbold (alternative: double _)\n\n\n[link text](website.com)\nlink text\n\n\n&lt;https://website.com&gt;\nClickable link: https://website.com\n\n\n# My Title\nHeader level 1 (largest)\n\n\n## My Section\nHeader level 2\n\n\n### My Subsection\nHeader level 3 – and so forth\n\n\n- List item\nUnordered (bulleted) list\n\n\n1. List item\nOrdered (numbered) list\n\n\n`inline code`\ninline code\n\n\n```\nStart/end of generic code block (on its own line)\n\n\n```bash\nStart of bash code block (end with ```)\n\n\n---\nHorizontal rule (line)\n\n\n&gt; Text\nBlockquote (like quoted text in emails)\n\n\n![](path/to/figure.png)\n[The figure will be inserted]\n\n\n\n\nLet’s try some of these things — type:\n# Introduction to Markdown\n\n## Part 1: Documentation\n\n- The Markdown _documentation_ can be found [here](https://www.markdownguide.org/)\n- To be clear, **the URL is &lt;https://www.markdownguide.org/&gt;**.\n\n## Part 2: The basics\n\n1. When you create a _numbered_ list...\n1. ...you don't need the numbers to increment.\n1. Markdown will take care of that for you.\n\n--------\n\n### Part 2b: Take it from the experts\n\n&gt; Markdown will take your science to the next level\n&gt; -- Wilson et al. 1843\n\n--------\n\n## Part 3: My favorite shell commands\n\nThe `date` command is terribly useful.\n\nHere is my shell code in a code block:\n\n```bash\n# Print the current date and time\ndate\n\n# List the files with file sizes\nls -lh\n```\n\n**The end.**\n\nThat should be previewed/rendered as:\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for a side-by-side screenshot in VS Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 Tables\nTables are not all that convenient to create in Markdown, but you can do it as follows.\n\n\n\n\nThis:\n| city             | inhabitants |\n|——————|——————|\n| Columbus   | 906 K       |\n| Cleveland   | 368 K       |\n| Cincinnati   | 308 K       |\n\nWill be rendered as:\n\n\n\ncity            \ninhabitants\n\n\n\n\nColumbus  \n906 K      \n\n\nCleveland  \n368 K      \n\n\nCincinnati  \n308 K      \n\n\n\n\n\n\n\n\n2.3 Whitespace\n\nIt’s recommended (in some cases necessary) to leave a blank line between different sections: lists, headers, etc.:\n## Section 2: List of ...\n\n- Item 1\n- Item 2\n\nFor example, ....\n\n\n\nA blank line between regular text will start a new paragraph, with some whitespace between the two:\n\n\n\n\n\nThis:\n\nParagraph 1.\n  \nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1.\nParagraph 2.\n\n\n\n\nWhereas a single newline will be completely ignored!:\n\n\n\n\n\nThis:\n\nParagraph 1.\nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1. Paragraph 2.\n\n\n\n\n\n\n\nThis:\n\nWriting  \none  \nword  \nper  \nline.\n\nWill be rendered as:\n\nWriting one word per line.\n\n\n\n\nMultiple consecutive spaces and blank line will be “collapsed” into a single space/blank line:\n\n\n\n\n\nThis:\n\nEmpty             space\n\nWill be rendered as:\n\nEmpty space\n\n\n\n\n\n\n\nThis:\n\nMany\n\n\n\n\nblank lines\n\nWill be rendered as:\n\nMany\nblank lines\n\n\n\n\nA single linebreak can be forced using two or more spaces (i.e., press the spacebar twice) or a backslash \\ after the last character on a line:\n\n\n\n\n\nThis:\n\nMy first sentence.\\\nMy second sentence.\n\nWill be rendered as:\n\nMy first sentence.\nMy second sentence.\n\n\n\n\nIf you want more vertical whitespace than what is provided between paragraphs, you’ll have to resort to HTML1: each &lt;br&gt; item forces a visible linebreak.\n\n\n\n\n\nThis:\n\nOne &lt;br&gt; word &lt;br&gt; per line\nand &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\nseveral blank lines.\n\nWill be rendered as:\n\nOne  word  per line and      several blank lines.\n\n\n\n\n\n\n\n\n\nSidenote: HTML and CSS in Markdown\n\n\n\n\nIf you need “inline colored text”, you can also use HTML:\ninline &lt;span style=\"color:red\"&gt;colored&lt;/span&gt; text.\nFor systematic styling of existing or custom elements, you need to use CSS. For example, including the following anywhere in a Markdown document will turn all level 1 headers (#) red:\n&lt;style&gt;\nh1 {color: red}\n&lt;/style&gt;\n\n\n\n\n\n\n2.4 Markdown extensions – Markdown for everything?!\nSeveral Markdown extensions allow Markdown documents to contain code that runs, and whose output can be included in rendered documents:\n\nR Markdown (.Rmd) and the follow-up Quarto\nJupyter Notebooks\n\nThere are many possibilities with Markdown! For instance, consider that:\n\nThis website and last week’s slides are written using Quarto.\nR Markdown/Quarto also has support for citations, journal-specific formatting, etc., so you can even write manuscripts with it.\n\n\n\n\n\n\n\n\nPandoc to render Markdown files (Click to expand)\n\n\n\n\n\nI very rarely render “plain” Markdown files because:\n\nMarkdown source is so well readable\nGitHub will render Markdown files for you\n\nThat said, if you do need to render a Markdown file to, for example, HTML or PDF, use Pandoc:\npandoc README.md &gt; README.html\npandoc -o README.pdf README.md\nFor installation (all OS’s): see https://pandoc.org/installing.html.\n\n\n\n\n\n\n\n\n\nSome additional Markdown syntax (Click to expand)\n\n\n\n\n\nThe below is “extended syntax” that is not supported by all interpreters:\n\n\n\nSyntax\nResult\n\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\nFootnote ref[^1]\nFootnote ref1\n\n\n[^1]: Text\nThe actual footnote"
  },
  {
    "objectID": "week2/w2_vscode-markdown.html#footnotes",
    "href": "week2/w2_vscode-markdown.html#footnotes",
    "title": "VS Code and Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any HTML markup in Markdown!↩︎"
  },
  {
    "objectID": "week2/w2_project-org.html#overview-of-this-week",
    "href": "week2/w2_project-org.html#overview-of-this-week",
    "title": "Project organization",
    "section": "1 Overview of this week",
    "text": "1 Overview of this week\n\nThis page:\n\nLearn some best practices for project organization, documentation, and management.\n\nAlso today:\n\nGet to know our text editor, VS Code.\nLearn how to use Markdown for documentation (and beyond).\n\nThursday\n\nLearn how to manage files in the Unix shell."
  },
  {
    "objectID": "week2/w2_project-org.html#project-organization-best-practices-recommendations",
    "href": "week2/w2_project-org.html#project-organization-best-practices-recommendations",
    "title": "Project organization",
    "section": "2 Project organization: best practices & recommendations",
    "text": "2 Project organization: best practices & recommendations\nGood project organization and documentation facilitates:\n\nCollaborating with others (and yourself in the future…)\nReproducibility\nAutomation\nVersion control\nPreventing your files slowly devolving into a state of incomprehensible chaos\n\nIn short, it is a necessary foundation to use this course’s tools and to reach some of its goals.\n\n\n2.1 Some underlying principles\n\nUse one dir (dir hierarchy) for one project\nUsing one directory hierarchy for one project means:\n\nDon’t mix files/subdirs for multiple distinct projects inside one dir.\nDon’t keep files for one project in multiple places.\n\nWhen you have a single directory hierarchy for each project, it is:\n\nEasier to find files, share your project, not throw away stuff in error, etc.\nPossible to use relative paths within a project’s scripts, which makes it more portable (more on that in a bit).\n\n\n\n\n\nTwo project dir hierarchies, nicely separated and self-contained.\n\n\n\n\n\nSeparate different kinds of files using a consistent dir structure\nWithin your project’s directory hierarchy:\n\nSeparate code from data.\nSeparate raw data from processed data & results.\n\nAlso:\n\nTreat raw data as read-only.\nTreat generated output as somewhat disposable and as possible to regenerate.\n\nAnd, as we’ll talk about below:\n\nUse consistent dir and file naming that follow certain best practices.\nSlow down and document what you’re doing.\n\n\n\n\n\n2.2 Absolute versus relative paths\nRecall that:\n\nAbsolute paths start from the computer’s root dir and do not depend on your working dir.\nRelative paths start from a specific working dir (and won’t work if you’re elsewhere).\n\n\n\nDon’t absolute paths sound better? What could be a disadvantage of them?\n\nAbsolute paths: - Don’t generally work across computers - Break when your move a project\nWhereas relative paths, as long as you consistently use the root of the project as the working dir, keep working when moving the project within and between computers.\n\n\n\n\n\nTwo project dir hierarchies, and the absolute and relative path to a FASTQ file.\n\n\n\n\n\n\nNow everything was moved into Dropbox.The absolute path has changed, but the relative path remains the same.\n\n\n\n\n\n2.3 But how to define and separate projects?\nFrom Wilson et al. 2017 - Good Enough Practices in Scientific Computing:\n\nAs a rule of thumb, divide work into projects based on the overlap in data and code files:\n\nIf 2 research efforts share no data or code, they will probably be easiest to manage independently.\nIf they share more than half of their data and code, they are probably best managed together.\nIf you are building tools that are used in several projects, the common code should probably be in a project of its own.\n\n\n\nProjects with shared data or code\nTo access files outside of the project (e.g., shared across projects), it is easiest to create links to these files:\n\n\n\nThe data is located in project1 but used in both projects.project2 contains a link to the data.\n\n\nBut shared data or scripts are generally better stored in separate dirs, and then linked to by each project using them:\n\n\n\nNow, the data is in it’s own top-level dir, with links to it in both projects.\n\n\nThese strategies do decrease the portability of your project, and moving the shared files even within your own computer will cause links to break.\nA more portable method is to keep shared (multi-project) files online — this is especially feasible for scripts under version control:\n\n\n\nA set of scripts shared by two projects is stored in an online repository like at GitHub.\n\n\n\nFor data, this is also possible but often not practical due to file sizes. It’s easier after data has been deposited in a public repository.\n\n\n\n\n\n2.4 Example project dir structure\nHere is one good way of organizing a project with top-levels dirs:\n\n\n\n\n\n\n\n\n\n\n\nOther reasonable options\n\n\n\nThese recommendations only go so far, and several things do depend on personal preferences and project specifics:\n\ndata as single top-level dir, or separate metadata, refdata, raw_data dirs?\n\nNaming of some dirs, like:\n\nresults vs analysis (Buffalo)\nsrc (source) vs scripts\n\nSometimes the order of subdirs can be done in multiple different ways. For example, where to put QC figures — results/plots/qc or results/qc/plots/?\n\n\n\nAnother important good practice is to use subdirectories liberally and hierarchically. For example, in omics data analysis, it often makes sense to create subdirs within results for each piece of software that you are using:"
  },
  {
    "objectID": "week2/w2_project-org.html#file-naming",
    "href": "week2/w2_project-org.html#file-naming",
    "title": "Project organization",
    "section": "3 File naming",
    "text": "3 File naming\nThree principles for good file names (from Jenny Bryan):\n\nMachine-readable\nHuman-readable\nPlaying well with default ordering\n\n\nMachine-readable\nConsistent and informative naming helps you to programmatically find and process files.\n\nIn file names, provide metadata like Sample ID, date, and treatment:\n\nsample032_2016-05-03_low.txt\n\nsamples_soil_treatmentA_2019-01.txt\n\nWith such file names, you can easily select samples from e.g. a certain month or treatment (more on Thursday):\nls *2016-05*\n\nls *treatmentA*\nSpaces in file names lead to inconvenience at best and disaster at worst (see example below).\nMore generally, only use the following in file names:\n\nAlphanumeric characters A-Za-z0-9\nUnderscores _\nHyphens (dashes) -\nPeriods (dots) .\n\n\n\n\n\n Spaces in file names — what could go wrong?\n\nSay, you have a dir with some raw data in the dir raw:\nls\nraw\nNow you create a dir for sequences, with a space in the file name:\nmkdir \"raw sequences\"\nYou don’t want this dir after all, and carelessly try to remove it\nrm -r raw sequences\n\n\n\n\nWhat will go wrong in the example above? (Click for the answer)\n\nThe rm command will not remove the dir with the space in the file name, but it will remove the “earlier” raw dir.\n\n\n\n\nHuman-readable\n\n“Name all files to reflect their content or function. For example, use names such as bird_count_table.csv, manuscript.md, or sightings_analysis.py.”\n— Wilson et al. 2017\n\n\n\n\nCombining machine- and human-readable\n\nOne good way (opinionated recommendations):\n\nUse underscores (_) to delimit units you may later want to separate on: sampleID, batch, treatment, date.\nWithin such units, use dashes (-) to delimit words: grass-samples.\nLimit the use of periods (.) to indicate file extensions.\nGenerally avoid capitals.\n\nFor example:\nmmus001_treatmentA_filtered-mq30-only_sorted_dedupped.bam\nmmus002_treatmentA_filtered-mq30-only_sorted_dedupped.bam\n.\n.\nmmus086_treatmentG_filtered-mq30-only_sorted_dedupped.bam\n\n\n\n\nPlaying well with default ordering\n\nUse leading zeros for lexicographic sorting: sample005.\nDates should always be written as YYYY-MM-DD: 2020-10-11.\nGroup similar files together by starting with same phrase, and number scripts by execution order:\nDE-01_normalize.R\nDE-02_test.R\nDE-03_process-significant.R"
  },
  {
    "objectID": "week2/w2_project-org.html#slow-down-and-document",
    "href": "week2/w2_project-org.html#slow-down-and-document",
    "title": "Project organization",
    "section": "4 Slow down and document",
    "text": "4 Slow down and document\n\nUse README files to document\nUse README files to document the following:\n\nYour methods\nWhere/when/how each data and metadata file originated\nVersions of software, databases, reference genomes\n…Everything needed to rerun whole project\n\n\n\n\n\n\n\nSee this week’s Buffalo chapter (Ch. 2) for further details.\n\n\n\n\n\n\n\n\n\nFor documentation, use plain text files\nPlain text files offer several benefits over proprietary & binary formats (like .docx and .xlsx)1:\n\nCan be accessed on any computer, including over remote connections\nAre future-proof\nAllow to be version-controlled\n\nMarkdown files are plain-text and strike a nice balance between ease of writing and reading, and added functionality — we’ll talk about those next."
  },
  {
    "objectID": "week2/w2_project-org.html#footnotes",
    "href": "week2/w2_project-org.html#footnotes",
    "title": "Project organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese considerations apply not just to files for documentation, but also to data files, etc!↩︎"
  },
  {
    "objectID": "week2/w2_exercises.html#exercise-1-course-notes-in-markdown",
    "href": "week2/w2_exercises.html#exercise-1-course-notes-in-markdown",
    "title": "Week 2 exercises",
    "section": "Exercise 1: Course notes in Markdown",
    "text": "Exercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\n\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, and hyperlinks.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline."
  },
  {
    "objectID": "week2/w2_exercises.html#exercise-2-organize-project-files",
    "href": "week2/w2_exercises.html#exercise-2-organize-project-files",
    "title": "Week 2 exercises",
    "section": "Exercise 2: Organize project files",
    "text": "Exercise 2: Organize project files\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\n\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. Do this within your personal dir in the course’s project dir (e.g. /fs/ess/PAS2700/users/$USER/week02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\n\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nCreate mock “alignment” files1\n\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results)\nInside the alignment dir, create files with names like sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam for all combinations of:\n\n30 samples (01-30)\n5 treatments (A-E)\n2 dates (08-14-2020 and 09-16-2020 – yes, use this date format for now)\n\n\nThese 300 files can be created with a single touch command2.\n\n\n\nHints\n\nUse brace expansion three times in the command: to expand (1) sample IDs, (2) treatments, and (3) dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with BAM files, and use a for loop to rename them, changing the extension from .sam to .bam.\n\n\n\nHints\n\n\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\n\n\n\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\n\nThe sample ID/number should be 01-19, and\nThe treatment should be A, B, or C.\n\nCreate a README.md in the dir that explains what you did.\n\n\n\nHints\n\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\n\nCreate a README\nInclude a project-wide README.md that described what you did. Again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\n\n\n\nHints\n\n\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\n\n\n\nBonus: Change file permissions\nMake sure no-one has write permissions for the raw data files, not even yourself. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\n\n\n\nHints\n\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee this Bonus section of the Managing files in the shell page for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents)."
  },
  {
    "objectID": "week2/w2_exercises.html#bonus-exercises",
    "href": "week2/w2_exercises.html#bonus-exercises",
    "title": "Week 2 exercises",
    "section": "Bonus exercises",
    "text": "Bonus exercises\n\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\n\n\n\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3."
  },
  {
    "objectID": "week2/w2_exercises.html#solutions",
    "href": "week2/w2_exercises.html#solutions",
    "title": "Week 2 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 2\n\n\n1. Getting set up\n\n# For example:\nmkdir /fs/ess/PAS2700/users/$USER/week02/ex2\n\ncd /fs/ess/PAS2700/users/$USER/week02/ex2\n\n\n\n2. Create a disorganized mock project\n\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n\n\n3. Organize the mock project\n\nAn example:\n\nCreate directories:\nmkdir -p data/{fastq,meta,ref}\nmkdir -p results/{bam,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/fastq/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/bam/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n\n\n\n4. Create mock alignment files\n\nmkdir -p results/alignment\ncd results/alignment \n\n# Create the files:\ntouch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\nls | wc -l\n300\n\n\n\n5. Rename files in a batch\n\nfor oldname in *.sam; do\n   newname=$(basename \"$oldname\" sam)bam\n   mv -v \"$oldname\" \"$newname\"\ndone\nIn the code above:\n\n$oldname will contain the old file name in each iteration of the loop.\nWe remove the sam suffix using basename \"$oldname\" sam.\nWe use command substitution ($() syntax) to catch the output of the basename command, and paste bam at the end.\n\nAlso, note that:\n\nWe don’t need a special construction to paste strings together: we simply type bam after what will be the extension-less file name from the basename command.\nI used informative variable names (oldname and newname), not cryptic ones like i and o.\n\n\n\n\n6. Copy files with wildcards\n\n\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\n\nThe first digit should be a 0 or a 1 [01] (or [0-1]),\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp -v sample[01][0-9]_[A-C]* subset/\n‘sample01_A_08-14-2020.bam’ -&gt; ‘subset/sample01_A_08-14-2020.bam’\n‘sample01_A_09-16-2020.bam’ -&gt; ‘subset/sample01_A_09-16-2020.bam’\n‘sample01_B_08-14-2020.bam’ -&gt; ‘subset/sample01_B_08-14-2020.bam’\n‘sample01_B_09-16-2020.bam’ -&gt; ‘subset/sample01_B_09-16-2020.bam’\n‘sample01_C_08-14-2020.bam’ -&gt; ‘subset/sample01_C_08-14-2020.bam’\n‘sample01_C_09-16-2020.bam’ -&gt; ‘subset/sample01_C_09-16-2020.bam’\n‘sample02_A_08-14-2020.bam’ -&gt; ‘subset/sample02_A_08-14-2020.bam’\n# [...output truncated...]\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir 'subset' and copied only files for samples 1-29\nand treatments A-C into this dir.\" &gt; subset/README.md\nCheck the resulting README files:\ncat subset/README.md\nOn Mon Mar 18 10:07:17 EDT 2024, created a dir 'subset' and copied only files for samples 1-29\nand treatments A-C into this dir.\n\n\n\n\n8. Bonus: a trickier renaming loop\n\n\nIn the loop, first use cut to extract the month, day, and year:\n\nStart by extracting the entire date: cut by an _ and take the third item (cut -d \"_\" -f 3).\nThen extract the different components of the date separately for month, date, and year, with cut -d \"-\": the first item is the month, the second is the day, and the third is the year.\nSave these compoinents of the date in variables using command substitution ($()).\n\nSecond, use cut to extract what we may call the “sample prefix”, which contains the sample number and the treatment.\nThird, build the new file name simply by putting the variables in the right order with _ and - delimiters.\nUse mv to rename the files — below, I’ve added -v for verbose so it will report what it does.\n\nfor oldname in *.bam; do\n     # Extract and store the month, day, and year:\n     # (First cut by '_' taking the 3rd item, then by '-')\n     month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n     day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n     year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n     \n     # Extract and store the sample prefix:\n     sample_prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n     \n     # Paste together the new name:\n     newname=\"$sample_prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n     \n     # Execute the move:\n     mv -v \"$oldname\" \"$newname\"\ndone\n‘sample01_A_08-14-2020.bam’ -&gt; ‘sample01_A_2020-08-14.bam’\n‘sample01_A_09-16-2020.bam’ -&gt; ‘sample01_A_2020-09-16.bam’\n‘sample01_B_08-14-2020.bam’ -&gt; ‘sample01_B_2020-08-14.bam’\n‘sample01_B_09-16-2020.bam’ -&gt; ‘sample01_B_2020-09-16.bam’\n‘sample01_C_08-14-2020.bam’ -&gt; ‘sample01_C_2020-08-14.bam’\n‘sample01_C_09-16-2020.bam’ -&gt; ‘sample01_C_2020-09-16.bam’\n‘sample01_D_08-14-2020.bam’ -&gt; ‘sample01_D_2020-08-14.bam’\n‘sample01_D_09-16-2020.bam’ -&gt; ‘sample01_D_2020-09-16.bam’\n‘sample01_E_08-14-2020.bam’ -&gt; ‘sample01_E_2020-08-14.bam’\n# [...output truncated...]\n\n\n\n9. Change file permissions\n\nBefore we start, let’s check the current file permissions:\nls -lh data/fastq\nls -lh data/fastq/ | head\ntotal 0\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample005_F.fastq.gz\nThe file “owner”/“user” (you) and the “group” (in this case, PAS0471, likely a different group for you) have read and write permissions, and “others” have no permissions at all.\nThere are several different ways to change permissions with the chmod command. Here are some examples which would ensure that no-one has write permission for the raw data:\n\nSet read(-only) permissions for all:\n# a=r =&gt; all=read\nchmod a=r data/fastq/*\nTake away write permissions for all:\n# a-w =&gt; all minus write\nchmod a-w data/fastq/*\nYou can also use the “numeric” syntax:\nchmod 444 data/fastq/*\n\nWhereas after running the second option, others won’t have read-access, the first and third option should give this result:\nls -lh data/fastq\nls -lh data/fastq/ | head\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample005_F.fastq.gz"
  },
  {
    "objectID": "week2/w2_exercises.html#footnotes",
    "href": "week2/w2_exercises.html#footnotes",
    "title": "Week 2 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReal alignment files like SAM/BAM are generated by aligning FASTQ sequence reads to a reference genome.↩︎\n If you already happened to have an alignment dir among your mock project dirs, first delete its contents or rename it.↩︎"
  },
  {
    "objectID": "week5/w5_overview.html#links",
    "href": "week5/w5_overview.html#links",
    "title": "Week 5",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – More about OSC.\nTuesday – Using software at OSC.\nThursday – Submitting shell scripts as batch jobs with Slurm.\n\n\n\nExercises & assignments\n\nYour final project proposal is due on Monday, April 8th\nExercises for this week"
  },
  {
    "objectID": "week5/w5_overview.html#content-overview",
    "href": "week5/w5_overview.html#content-overview",
    "title": "Week 5",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we’ll start with an introduction to OSC and supercomputers. Then, we’ll see how we can submit shell scripts to OSC’s queue for compute jobs with the widely-used Slurm resource manager. Finally, we will learn about installing and managing software with Conda, and loading pre-installed software with the “module” system.\nSome of the things you will learn this week:\n\nWhen and why we need to run our analyses on supercomputers.\nKey terminology around supercomputers.\nDifferent ways to access and transfer data to and from OSC.\nDifferent ways to start “compute jobs”: via OnDemand, with interactive jobs, and with scripts.\nSome strategies around requesting appropriate resources for your compute jobs.\nThe Slurm/sbatch syntax to request specific resources for your compute jobs.\nHow to monitor Slurm jobs.\nHow to load pre-installed software at OSC with the “module” system.\nHow to install and manage software with Conda.\n\n\n2.1 Optional readings\n\nBuffalo Chapter 4: “Working with Remote Machines”"
  },
  {
    "objectID": "week5/w5_exercises.html#exercise-0",
    "href": "week5/w5_exercises.html#exercise-0",
    "title": "Week 5 exercises",
    "section": "Exercise 0",
    "text": "Exercise 0\nDo any exercises on the software and Slurm batch job lecture pages that we did not get to in class."
  },
  {
    "objectID": "week5/w5_exercises.html#exercise-1",
    "href": "week5/w5_exercises.html#exercise-1",
    "title": "Week 5 exercises",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe version of FastQC on OSC (0.11.8) is not the latest one (0.12.1). Let’s assume that you really need the latest version for your analysis:\n\nCreate a Conda environment for FastQC and install FastQC version 0.12.1 into it.\nActivate the environment and check that you have the correct version of FastQC."
  },
  {
    "objectID": "week5/w5_exercises.html#exercise-2-fastqc-with-multiple-cores",
    "href": "week5/w5_exercises.html#exercise-2-fastqc-with-multiple-cores",
    "title": "Week 5 exercises",
    "section": "Exercise 2: FastQC with multiple cores",
    "text": "Exercise 2: FastQC with multiple cores\nAs a starting point, copy the FastQC shell script from the last exercise on the Slurm batch job page:\n# After this, your script will be at 'scripts/fastqc.sh'\ncp -v ../class_slurm/scripts/fastqc.sh scripts\n\n\nIf you don’t have it, here’s the contents of the starting FastQC script (Click to expand)\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --output=slurm-fastqc-%j.out\n#SBATCH --mail-type=END,FAIL\n\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Initial reporting\necho \"# Starting script fastqc.ch\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir=\"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Done with script fastqc.sh\"\ndate\n\n\nUse your brand new Conda FastQC environment in the script instead of OSC’s module.\nChange the #SBATCH options in the script so that Slurm only sends you an email if the job fails.\nChange the #SBATCH options in the script so that your batch job will reserve 8 cores.\nChange the FastQC command in the script so that FastQC will use all 8 reserved cores. (Run fastqc --help to find the relevant option.)\nSubmit the script as a batch job with input FASTQ file ERR10802863_R1.fastq.gz as before.\nYou’ll be running FastQC for all files next, so let’s remove this Slurm log file and all FastQC output files."
  },
  {
    "objectID": "week5/w5_exercises.html#exercise-3-fastqc-batch-jobs-in-a-loop",
    "href": "week5/w5_exercises.html#exercise-3-fastqc-batch-jobs-in-a-loop",
    "title": "Week 5 exercises",
    "section": "Exercise 3: FastQC batch jobs in a loop",
    "text": "Exercise 3: FastQC batch jobs in a loop\n\nLoop over all Garrigos et al. FASTQ files, and submit your FastQC script from Exercise 2 as a batch job for each file.\nCheck the Slurm queue immediately after running the loop, and keep checking it every couple of seconds until all your FastQC jobs have disappeared from the list (i.e., are done).\nYou now arguably have too many Slurm log files to go through.\n\nThe Slurm email feature should help: check that you didn’t get one or more emails about failed jobs.\nHere’s another thing you can do for a quick check: run the tail command with a glob (using *) so it will print the last lines of each of the output files. Try this.\nScroll through that tail output: it should be pretty easy to spot any files that don’t end the way the should.\nCheck the output files as well.\n\nIt can be good to keep your Slurm log files, but things will soon get very messy if you keep them all in your working dir: create a dir called logs in the FastQC output dir, and move all Slurm log files into that dir."
  },
  {
    "objectID": "week5/w5_exercises.html#exercise-4-trimgalore-batch-jobs-in-a-loop",
    "href": "week5/w5_exercises.html#exercise-4-trimgalore-batch-jobs-in-a-loop",
    "title": "Week 5 exercises",
    "section": "Exercise 4: TrimGalore batch jobs in a loop",
    "text": "Exercise 4: TrimGalore batch jobs in a loop\nAs a starting point, take your TrimGalore shell script from last week’s exercises, and save it as scripts/trimgalore.sh.\n\n\nHint: the starting TrimGalore script (Click to expand)\n\n#!/bin/bash\nset -euo pipefail\n\n# Load TrimGalore\nmodule load miniconda3/23.3.1-py310\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1_in\"\necho \"# Input R2 FASTQ file:      $R2_in\"\necho \"# Output dir:               $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --fastqc \\\n    --output_dir \"$outdir\" \\\n    \"$R1_in\" \\\n    \"$R2_in\"\n\n# Report\necho\necho \"# Done with script trimgalore.sh\"\ndate\n\n\nSwitch to using your own (instead of my) TrimGalore Conda environment — the one you created in class this week.\nAdd #SBATCH options to the top of the TrimGalore shell script to specify:\n\nThe account\nThe number of cores (8)\nThe Slurm log file name\nThat Slurm emails you upon job failure\n\nUse the TrimGalore option --cores to make it use all 8 reserved cores.\nSubmit the script as a batch job for sample ERR10802863, and check if everything went well. Then remove these test outputs.\nRun TrimGalore for all FASTQ file pairs from the Garrigos et al. data, and check if everything went well.\nMove the Slurm log files into a dir logs in the TrimGalore output dir.\n\n\n\n\n\n\n\nSaving time\n\n\n\nCompared to running TrimGalore last week, we now saved time in two ways:\n\nWe used 8 cores, which decreases the running time, for example for the first sample, from nearly a minute to 17 seconds.\nWe ran TrimGalore simultaneously instead of consecutively for each sample. If Slurm/OSC quickly granted you compute jobs, running TrimGalore on all files cost well under a minute instead of over 20 minutes."
  },
  {
    "objectID": "week5/w5_exercises.html#solutions",
    "href": "week5/w5_exercises.html#solutions",
    "title": "Week 5 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\n1 - Create a FastQC Conda environment\n\nThis should install the latest version, but you could also replace the fastqc at the end of the line with fastqc=0.12.1:\nmodule load miniconda3/23.3.1-py310\nconda create -y -n fastqc -c bioconda fastqc\n\n\n\n2 - Activate the environment and check the version\n\nsource activate fastqc\nfastqc --version\nFastQC v0.12.1\n\n\n\n\nExercise 2\n\n\n1 - Use your Conda environment\n\nReplace the line module load fastqc with:\nmodule load miniconda3/23.3.1-py310\nsource activate fastqc\n\n\n\n2 - Only email upon failure\n\n#SBATCH --mail-type=FAIL\n\n\n\n3 - Reserve 8 cores\n\n#SBATCH --cpus-per-task=8\n\n\n\n4 - Make FastQC use 8 cores\n\nfastqc --threads 8 --outdir \"$outdir\" \"$fastq_file\"\n\n\n\n5 - Submit the script\n\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\nSubmitted batch job 12431988\n\n\n\n6 - Clean up\n\nrm slurm-fastqc*\nrm results/fastqc/*\n\n\n\n\nExercise 3\n\n\n1 - Submit a FastQC batch job for each FASTQ file\n\nfor fastq_file in ../../garrigos_data/fastq/*fastq.gz; do\n   sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\nSubmitted batch job 27902555\nSubmitted batch job 27902556\nSubmitted batch job 27902557\nSubmitted batch job 27902558\nSubmitted batch job 27902559\nSubmitted batch job 27902560\nSubmitted batch job 27902561\n[...output truncated...]\n\n\n\n2 - Check the Slurm queue\n\nsqueue -u $USER -l\n\n\n\n3 - Check the output\n\nls\nresults                    slurm-fastqc-27902531.out  slurm-fastqc-27902543.out  slurm-fastqc-27902555.out\nrun                        slurm-fastqc-27902532.out  slurm-fastqc-27902544.out  slurm-fastqc-27902556.out\nscripts                    slurm-fastqc-27902533.out  slurm-fastqc-27902545.out  slurm-fastqc-27902557.out\nslurm-fastqc-27902522.out  slurm-fastqc-27902534.out  slurm-fastqc-27902546.out  slurm-fastqc-27902558.out\nslurm-fastqc-27902523.out  slurm-fastqc-27902535.out  slurm-fastqc-27902547.out  slurm-fastqc-27902559.out\nslurm-fastqc-27902524.out  slurm-fastqc-27902536.out  slurm-fastqc-27902548.out  slurm-fastqc-27902560.out\nslurm-fastqc-27902525.out  slurm-fastqc-27902537.out  slurm-fastqc-27902549.out  slurm-fastqc-27902561.out\nslurm-fastqc-27902526.out  slurm-fastqc-27902538.out  slurm-fastqc-27902550.out  slurm-fastqc-27902562.out\nslurm-fastqc-27902527.out  slurm-fastqc-27902539.out  slurm-fastqc-27902551.out  slurm-fastqc-27902563.out\nslurm-fastqc-27902528.out  slurm-fastqc-27902540.out  slurm-fastqc-27902552.out  slurm-fastqc-27902564.out\nslurm-fastqc-27902529.out  slurm-fastqc-27902541.out  slurm-fastqc-27902553.out  slurm-fastqc-27902565.out\nslurm-fastqc-27902530.out  slurm-fastqc-27902542.out  slurm-fastqc-27902554.out\ntail slurm-fastqc*\n==&gt; slurm-fastqc-27902563.out &lt;==\nApprox 80% complete for ERR10802885_R2.fastq.gz\nApprox 85% complete for ERR10802885_R2.fastq.gz\nApprox 90% complete for ERR10802885_R2.fastq.gz\nApprox 95% complete for ERR10802885_R2.fastq.gz\nApprox 100% complete for ERR10802885_R2.fastq.gz\nAnalysis complete for ERR10802885_R2.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n==&gt; slurm-fastqc-27902564.out &lt;==\nApprox 80% complete for ERR10802886_R1.fastq.gz\nApprox 85% complete for ERR10802886_R1.fastq.gz\nApprox 90% complete for ERR10802886_R1.fastq.gz\nApprox 95% complete for ERR10802886_R1.fastq.gz\nApprox 100% complete for ERR10802886_R1.fastq.gz\nAnalysis complete for ERR10802886_R1.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n==&gt; slurm-fastqc-27902565.out &lt;==\nApprox 80% complete for ERR10802886_R2.fastq.gz\nApprox 85% complete for ERR10802886_R2.fastq.gz\nApprox 90% complete for ERR10802886_R2.fastq.gz\nApprox 95% complete for ERR10802886_R2.fastq.gz\nApprox 100% complete for ERR10802886_R2.fastq.gz\nAnalysis complete for ERR10802886_R2.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n# [...output truncated...]\nls -lh results/fastqc\ntotal 48M\n-rw-rw----+ 1 poelstra PAS0471 718K Mar 31 16:29 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 364K Mar 31 16:29 ERR10802863_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 688K Mar 31 16:29 ERR10802863_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 363K Mar 31 16:29 ERR10802863_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 714K Mar 31 16:29 ERR10802864_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 366K Mar 31 16:29 ERR10802864_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 695K Mar 31 16:29 ERR10802864_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 351K Mar 31 16:29 ERR10802864_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 713K Mar 31 16:29 ERR10802865_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 367K Mar 31 16:29 ERR10802865_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 698K Mar 31 16:29 ERR10802865_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 358K Mar 31 16:29 ERR10802865_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 718K Mar 31 16:29 ERR10802866_R1_fastqc.html\n# [...output truncated...]\n\n\n\n4 - Clean the Slurm log files\n\nmkdir results/fastqc/logs\nmv slurm-fastqc* results/fastqc/logs\n\n\n\n\nExercise 4\n\n\n1 - Switch the Conda environment\n\nReplace the line conda activate /fs/ess/PAS0471/jelmer/conda/trimgalore with:\nconda activate trim-galore-0.6.10\n\n\n\n2 - #SBATCH options\n\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=8\n#SBATCH --output=slurm-trimgalore-%j.out\n#SBATCH --mail-type=FAIL\n\n\n\n3 - Make TrimGalore use the reserved 8 cores\n\nSimply add --cores 8 to your TrimGalore command.\n\n\n\n4 - Submit the script once\n\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/trimgalore.sh \"$fastq_file\" results/trimgalore\nTo check that everything went well, look at the Slurm log files and the TrimGalore output files in results/trimgalore.\nTo remove the outputs from this test:\nrm slurm-trimgalore*\nrm results/trimgalore/*\n\n\n\n5 - Submit the script for all samples\n\nfor fastq_R1 in ../../garrigos_data/fastq/*R1.fastq.gz; do\n   sbatch scripts/trimgalore.sh \"$fastq_R1\" results/trimgalore\ndone\nTo check that everything went well, check your email and look at the TrimGalore output files in results/trimgalore. You can also run tail slurm-trimgalore* to check the last lines of each Slurm log file.\n\n\n\n6 - Move the Slurm log files\n\nmkdir results/trimgalore/logs\nmv slurm-trimgalore* results/trimgalore/logs"
  },
  {
    "objectID": "week4/w4_1_scripts.html#overview-and-setting-up",
    "href": "week4/w4_1_scripts.html#overview-and-setting-up",
    "title": "Shell scripting",
    "section": "Overview and setting up",
    "text": "Overview and setting up\n\nThis and next week\nThis week, you will learn how to write shell scripts and then how to use shell scripts to run programs, like various bioinformatics tools, with command-line interfaces (CLIs).\nNext week’s material will then cover the final main set of fundamental computational skills needed to analyze large-scale omics datasets at OSC: submitting your shell scripts as batch compute jobs, and using software at OSC.\n\n\nThis session\nIn this session, we will talk about:\n\nThe basics of shell scripts\nBoilerplate shell script header lines: shebang and safe settings\nCommand-line arguments to scripts\nSome more details on shell variables ($myvar etc)\nConditionals (if statements) — if we get to that\n\n\n\n\n\n\n\n\nVS Code improvements\n\n\n\nThese two settings will make life easier when writing shell scripts in VS Code.\nFirst, we’ll add a keyboard shortcut to send code from your editor to the terminal. This is the same type of behavior that you may be familiar with from RStudio, and will mean that won’t have to copy-and-paste code into the terminal:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter1.\n\nIn VS Code’s editor pane, the entire line that your cursor is on is selected by default. As such, your keyboard shortcut will send the line that your cursor is in to the terminal; you can also send multiple lines to the terminal after selecting them.\n\nSecond, we’ll add the ShellCheck VS Code extension. This extension checks shell scripts for errors like referencing variables that have not been assigned. Potential problems show up as colored squiggly lines. It also provided links with more information about the error and how to improve your code. This extension is incredibly useful!\n\nClick on the Extensions icon in the far left (narrow) sidebar in VS Code.\nType “shellcheck” and click the small purple “Install” button next to the entry of this name (the description should include “Timon Wong”, who is the author)."
  },
  {
    "objectID": "week4/w4_1_scripts.html#introduction-to-shell-scripts",
    "href": "week4/w4_1_scripts.html#introduction-to-shell-scripts",
    "title": "Shell scripting",
    "section": "1 Introduction to shell scripts",
    "text": "1 Introduction to shell scripts\nMany bioinformatics tools (programs/software) that are used to analyze omics data are run from the command line. We can run them using command line expressions that are structurally very similar to how we’ve been using basic Unix shell commands.\nHowever, we’ve been running shell commands in a manner that we may call “interactive”, by typing or pasting them into the shell, and then pressing Enter. But when you run bioinformatics tools, it is in most cases a much better idea to run them via shell scripts, which are plain-text files that contain shell code.\n\n“Most Bash scripts in bioinformatics are simply commands organized into a rerunnable script with some added bells and whistles to check that files exist and ensuring any error causes the script to abort.” — Buffalo Ch. 12\n\nTherefore, shell scripts are relatively straightforward to write with what you already know! We will learn about those bells and whistles from the quote above in this session.\n\n\n\n\n\n\nBash vs. shell\n\n\n\nSo far, we’ve mostly used talked about the Unix shell and shell scripts. The quote above uses the word “Bash”, and we’ll see that term more often this week. The difference is this: there are multiple Unix shell (language) variants and the specific one we have been using, which is also by far the most common, is the Bash shell. Our shell scripts are therefore in the Bash language and can be specifically called Bash scripts.\n\n\n\n\nRunning commands interactively vs. via scripts\nBefore we see why it’s often a better idea to use scripts than to run code interactively, let’s go through a minimal example of both approaches with the tool FastQC, which performs FASTQ file quality control (QC; more on FastQC in the next session).\n\nHere’s how you can run FastQC on one FASTQ file — the command fastqc followed by a file name:\nfastqc data/fastq/A_R1.fastq.gz\nThis is what a minimal shell script to do the same thing would look like:\n#!/bin/bash\nfastqc data/fastq/A_R1.fastq.gz\nIf the above shell script is saved as fastqc.sh in our working dir, it can be executed as follows:\nbash fastqc.sh\n\n\n\n\nWhy use shell scripts\nThere are several general reasons why it can be beneficial to use shell scripts instead of running code interactively line-by-line:\n\nIt is a good way to save and organize your code.\nYou can easily rerun scripts and re-use them in similar contexts.\nRelated to the point above, they provide a first step towards automating the set of analyses in your project.\nWhen your code is tucked away in a shell script, you only have to call the script to run what is potentially a large set of commands.\n\nAnd very importantly for our purposes at OSC, we can submit scripts as “batch jobs” to the compute job scheduling program (which is called Slurm), and this allows us to:\n\nRun scripts remotely without needing to stay connected to the running process, or even to be connected at all to it: we can submit a script, log out from OSC and shut down our computer, and it will still run.\nEasily run analyses that take many hours or even multiple days.\nRun a script many times simultaneously, such as for different files/samples.\n\n\n\n\nSummary of what we need to learn about\n\nWriting shell scripts (this week)\nSubmitting scripts to the Slurm job scheduler (next week)\nMaking software available at OSC (next week)"
  },
  {
    "objectID": "week4/w4_1_scripts.html#a-basic-shell-script",
    "href": "week4/w4_1_scripts.html#a-basic-shell-script",
    "title": "Shell scripting",
    "section": "2 A basic shell script",
    "text": "2 A basic shell script\n\n2.1 A one-line script to start\nCreate your first script, printname.sh (note that shell scripts usually have the extension .sh) as follows:\n# First, let's create and move into a new dir\nmkdir -p week04/scripts\ncd week04\n# Create an empty file\ntouch scripts/printname.sh\nA nice VS Code trick is that is if you hold Ctrl (Cmd on Mac) while hovering over a file path in the terminal, the path should become underlined and you can click on it to open the file. Try that with the printname.sh script2.\nOnce the file is open in your editor pane, type or paste the following inside the script:\necho \"This script will print a first and a last name\"\nShell scripts mostly contain the same regular Unix shell code that we have gotten familiar with, but have so far directly typed in the terminal. As such, our single line with an echo command constitutes a functional shell script!\nOne way of running the script is by typing bash followed by the path to the script:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat worked! The script doesn’t yet print any names like it “promises” to do, but we will add that functionality in a little bit. But first, we’ll learn about two header lines that are good practice to add to every shell script.\n\n\n\n\n\n\n\nAuto Save in VS Code (Click to expand)\n\n\n\n\n\nAny changes you make to this and other files in the editor pane should be immediately, automatically saved by VS Code. If that’s not happening for some reason, you should see an indication of unsaved changes like a large black dot next to the script’s file name in the editor pane tab header.\nIf the file is not auto-saving, you can always save it manually (including with Ctrl/Cmd+S) like you would do in other programs. However, it may be convenient to turn Auto Save on: press Ctrl/Cmd+Shift+S to open the Command Palette and type “Auto Save”. You should see an option “Toggle Auto Save”: click on that.\n\n\n\n\n\n\n2.2 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which computer language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\n#!/bin/bash\nSuch a line starts with #! (hash-bang), basically marking it as a special type of comment. After those two characters comes the file path of the relevant program: in our case Bash, which itself is just a program with an executable file that is located at /bin/bash on Linux and Mac computers.\nWhile not always strictly necessary, adding a shebang line to every shell script is good practice, especially when you submit your script to OSC’s Slurm queue, as we’ll do next week.\n\n\n\n2.3 Shell script settings\nAnother best-practice line you should add to your shell scripts will change some default settings to safer alternatives.\n\nBad default shell settings\nThe following two default settings of the Bash shell are bad ideas inside scripts:\n\nWhen you reference a non-existent (“unset”) variable, the shell replaces that with nothing without complaint:\necho \"Hello, my name is $myname. What is yours?\"\nHello, my name is . What is yours?\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an existing variable (e.g. you misspelled its name, or forgot to assign it altogether). Even more problematically, this can lead to potentially very destructive file removal, as the box below illustrates.\nA Bash script keeps running after encountering errors. That is, if an error is encountered when running, say, line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” can still be completely wrong.\n\n\n\n\n\n\n\n\nAccidental file removal with unset variables\n\n\n\nThe shell’s default behavior of ignoring the referencing of unset variables can lead to accidental file removal as follows:\n\nUsing a variable, we try to remove some temporary files whose names start with tmp_:\n# NOTE: DO NOT run this!\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*\nUsing a variable, we try to remove a temporary directory:\n# NOTE: DO NOT run this!\ntempdir=output/tmp\nrm -r $tmpdir/*\n\n\n\nAbove, the text specified the intent of the commands. What would have actually happened? (Click to expand)\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem: recall that a leading / in a path is a computer’s root directory. (-r makes the removal recursive and -f makes forces removal).\n\n\nNote this is especially likely to happen inside scripts, where it is common to use variables and to work non-interactively.\nBefore you get too scared of creating terrible damage, note that at OSC, you would not be able to remove any essential files3, since you don’t have the permissions to do so. On your own computer, this could be more genuinely dangerous, though even there, you would not be able to remove operating system files without specifically requesting “admin” rights.\n\n\n\n\n\nSafer settings\nThe following three settings will make your shell scripts more robust and safer. With these settings, the script terminates with an appropriate error message if:\n\nset -u — an “unset” (non-existent) variable is referenced.\nset -e — almost any error occurs.\nset -o pipefail — an error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\nset -e -u -o pipefail\nOr even more concisely:\nset -euo pipefail\n\n\n\n\n2.4 Adding the header lines to our script\nAdd the discussed header lines to your printname.sh script, so it will now contain the following:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\nAnd run the script again:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat didn’t change anything to the output, but at least we confirmed that the script still works.\n\n\n\n\n\n\nCan I run scripts without the bash command? (Click to expand)\n\n\n\n\n\nBecause our script has a shebang line, we have taken one step towards being able to execute the script without the bash command, or in other words, to run the script basically “as a command”. With that method, we could run a script using just its path:\nsandbox/printname.sh\n(Or if the script was in our current working dir, using ./printname.sh. In that case the ./ is necessary to make it explicit that we are referring to a file name: otherwise, when running just printname.sh, the shell would look for a command or program of that name, and wouldn’t be able to find it.)\nHowever, this would also require us to “make the script executable”, which we won’t talk about. But I’m mentioning it here because you might see this way of running scripts being used elsewhere."
  },
  {
    "objectID": "week4/w4_1_scripts.html#command-line-arguments-for-scripts",
    "href": "week4/w4_1_scripts.html#command-line-arguments-for-scripts",
    "title": "Shell scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script to run it, you can pass command-line arguments to it, such as a file to operate on. This is much like when you provide a command like ls with arguments:\n# [Don't run any of this, these are just syntax examples]\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\nAnd here is what it looks like to pass arguments to scripts:\n# [Don't run any of this, these are just syntax examples]\n\n# Run scripts without any arguments:\nbash scripts/fastqc.sh\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash scripts/fastqc.sh data/sampleA.fastq.gz  # 1 argument: a filename\nbash scripts/printname.sh John Doe            # 2 arguments: strings representing names\nIn the next section, we’ll see what happens with the arguments we pass to a script inside that script.\n\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments that you pass to it are automatically available in “placeholder” variables. Specifically:\n\nAny first argument will be assigned to the variable $1\nAny second argument will be assigned to $2\nAny third argument will be assigned to $3, and so on.\n\n\n\n\n In the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values? (Click for the solution)\n\n\n\nIn bash scripts/fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\nHowever, while they are made available, these placeholder variables are not “automagically” used. So, unless we explicitly include code in the script to do something with these variables, nothing extra really happens.\nTherefore, let’s add some code to our printname.sh script to “process” any first and last name that are passed to the script. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# [Paste this into you script - don't enter this directly in your terminal.]\nNext, we’ll run the script, passing the arguments John and Doe:\nbash scripts/printname.sh John Doe\nThis script will print a first and a last name\nFirst name: John\nLast name: Doe\n\n\n Exercise: Command-line arguments\nIn each scenario that is described below, think about what might happen. Then, run the script as instructed in the scenario to test your prediction.\n\nRunning the script printname.sh without passing arguments to it.\n\n\n\nClick here for the solution\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\nbash scripts/printname.sh\nprintname.sh: line 5: $1: unbound variable\n\n\nAfter commenting out the line with set settings, running the script again without passing arguments to it.\n\n\n\nClick here to learn what “commenting out” means\n\nYou can deactivate a line of code without removing it (because perhaps you’re not sure you may need this line in the end) by inserting a # as the first character of that line. This is often referred to as “commenting out” code.\nFor example, below I’ve commented out the ls command, and nothing will happen if I run this line:\n#ls\n\n\n\nClick here for the solution\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\nbash scripts/printname.sh\necho \"First name:\"\necho \"Last name:\"\nBeing “commented out”, the set line should read:\n#set -euo pipefail\n\n\nDouble-quoting the entire name when you run the script, e.g.: bash scripts/printname.sh \"John Doe\".\n\n\n\nClick here for the solution\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\nbash scripts/printname.sh \"John Doe\"\necho \"First name: John Doe\"\necho \"Last name:\"\n\nTo get back to where you were, remove the # you inserted in the script in step 2 above to reactive the set line.\n\n\n\n\n3.3 Copying placeholders to variables with descriptive names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables — for example:\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nUsing descriptively named variables in your scripts has several advantages, such as:\n\nIt will make your script easier to understand for others and for your future self.\nIt will make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name.\n$# contains the number of command-line arguments passed to the script."
  },
  {
    "objectID": "week4/w4_1_scripts.html#more-on-shell-variables",
    "href": "week4/w4_1_scripts.html#more-on-shell-variables",
    "title": "Shell scripting",
    "section": "4 More on shell variables",
    "text": "4 More on shell variables\n\n4.1 Why use variables\nAbove, we saw that variables are useful to be able to pass arguments to a script, so you can easily rerun a script with a different input file / settings / etc. Let’s take a step back and think about variables and their uses a bit more.\n\n“Processing pipelines having numerous settings that should be stored in variables (e.g., which directories to store results in, parameter values for commands, input files, etc.).\nStoring these settings in a variable defined at the top of the file makes adjusting settings and rerunning your pipelines much easier.\nRather than having to change numerous hardcoded values in your scripts, using variables to store settings means you only have to change one value—the value you’ve assigned to the variable.”\n— Buffalo ch. 12\n\nIn brief, use variables for things that:\n\nYou refer to repeatedly and/or\nAre subject to change.\n\n\n\n\n4.2 Quoting variables\nI have mentioned that it is good practice to quote variables (i.e. to use \"$myvar\" instead of $myvar). So what can happen if you don’t do this?\n# Start by making and moving into a dir to create some messy files\nmkdir sandbox\ncd sandbox\nIf a variable’s value contains spaces:\n# Assign a string with spaces to variable 'today', and print its value:\ntoday=\"Tue, Mar 26\"\necho $today\nTue, Mar 26\n# Try to create a file with a name that includes this variable: \ntouch README_$today.txt\n\n# (Using the -1 option to ls will print each entry on its own line)\nls -1\n26.txt\nMar\nREADME_Tue,\nOops! The shell performed “field splitting” to split the value into three separate units — as a result, three files were created. This can be avoided by quoting the variable:\ntouch README_\"$today\".txt\nls -1\nREADME_Tue, Mar 26.txt\nAdditionally, without quoting, we can’t explicitly indicate where a variable name ends:\n# We intend to create a file named 'README_Tue, Mar 26_final.txt'\ntouch README_$today_final.txt\nls -1\nREADME_.txt\n\n\nDo you understand what happened here? (Click for the solution)\n\nWe have assigned a variable called $today, but the shell will instead look for a variable called $today_final. This is because we have not explicitly indicated where the variable name ends, so the shell will include all characters until it hits a character that cannot be part of a shell variable name: in this case a period, ..\n\nQuoting solves this, too:\ntouch README_\"$today\"_final.txt\nls -1\nREADME_Tue, Mar 26_final.txt\n\n\n\n\n\n\n\nCurly braces notation: ${myvar} (Click to expand)\n\n\n\n\n\nThe $var notation to refer to a variable in the shell is actually an abbreviation of the full notation, which includes curly braces:\necho ${today}\nTue, Mar 26\nPutting variable names between curly braces will also make it clear where the variable name begins and ends, although it does not prevent field splitting:\ntouch README_${today}_final.txt\n\nls\n26_final.txt  Mar  README_Tue,\nBut you can combine curly braces and quoting:\ntouch README_\"${today}\"_final.txt\n\nls\n'README_Tue, Mar 26_final.txt'\n\n\n\n\n\n\n\n\n\nQuoting as “escaping” special meaning & double vs. single quotes (Click to expand)\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, double quotes will escape other “special characters”, such as shell wildcards. Compare:\n# Due to shell expansion, this will echo/list all files in the current working dir\necho *\n18.txt Aug README_Thu, README_Thu, Aug 18.txt\n# This will simply print the literal \"*\" character \necho \"*\"\n*\nHowever, double quotes not turn off the special meaning of $ (which is to denote a string as a variable):\necho \"$today\"\nThu, Aug 18\n…but single quotes will:\necho '$today'\n$today\n\n\n\n\n\n\n\n4.3 Variable names\nIn the shell, variable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods (.), dashes (-), or other special symbols4.\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”.\n\n\n\n\n\n\nCase and environment variables\n\n\n\nAll-uppercase variable names are pretty commonly used — and recall that so-called environment variables are always in uppercase (we’ve seen $USER and $HOME). Alternatively, you can use lowercase for variables and uppercase for “constants”, like when you include certain file paths or settings in a script without allowing them to be set from outside of the script.\n\n\n# Move out of the 'sandbox' dir (back to /fs/ess/PAS2700/users/$SUER/week04)\ncd .."
  },
  {
    "objectID": "week4/w4_1_scripts.html#conditionals",
    "href": "week4/w4_1_scripts.html#conditionals",
    "title": "Shell scripting",
    "section": "5 Conditionals",
    "text": "5 Conditionals\nWith conditionals like if statements, we can run one or more commands only if some condition is true. Also, we can run a different set of commands if the condition is not true. This can be useful in shell scripts because we may, for instance, want to process a file differently depending on its file type.\n\n5.1 Basic syntax\nThis is the basic syntax of an if statement in Bash (note that similarities with for loop syntax):\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nfi\nWe’ll have to add an else clause to run alternative command(s) if the condition is false:\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nelse\n    # Commands(s) to run if the condition is false\nfi\n\n\n\n5.2 String comparisons\nFirst, an if statement that tests the file type of say an input file, and runs different code depending on the result:\n# [Hypothetical example - don't run this]\n# Say we have a variable $filetype that contains a file's type\n\nif [[ \"$filetype\" == \"fastq\" ]]; then\n    echo \"Processing FASTQ file...\"\n    # Commands to process the FASTQ file\nelse\n    echo \"Unknown filetype!\"\n    exit 1\nfi\nIn the code above, note that:\n\nThe double square brackets [[ ]] represent a test statement5.\nThe spaces bordering the brackets on the inside are necessary: [[\"$filetype\" == \"fastq\"]] would fail!\nDouble equals signs (==) are common in programming to test for equality — this is to contrast it with a single =, which is used for variable assignment.\nWhen used inside a script, the exit command will stop the execution of the script. With exit 1, the exit status of our script is 1: in bash, an exit status of 0 means success — any other integer, including 1, means failure.\n\n\n\n\n\n\n\n\n\nString comparison\nEvaluates to true if\n\n\n\n\nstr1 == str2\nStrings str1 and str2 are identical6\n\n\nstr1 != str2        \nStrings str1 and str2 are different                \n\n\n-z str\nString str is null/empty (useful with variables)\n\n\n\n\n\n\n5.3 File tests\nThe code below tests whether an input file exists using the file test -f and if it does not (hence the !), it will stop the execution of the script:\n# [Hypothetical example - don't run this]\n\n# '-f' is true if the file exists,\n# and '! -f' is true if the file doesn't exist\nif [[ ! -f \"$fastq_file\" ]]; then\n    echo \"Error: Input file $fastq_file not found!\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nFile/dir test\nEvaluates to true if\n\n\n\n\n-f file\nfile exists and is a regular file (not a dir or link)\n\n\n-d dir\ndir exists and is a directory          \n\n\n-e file/dir\nfile/dir exists\n\n\n\n\n\n\n5.4 Integer (number) comparisons\nTo avoid unexpected or hard-to-understand errors later on in a shell script, we may choose to test at the beginning whether the correct number of arguments was passed to the script, and abort the script if this is not the case:\n# [Hypothetical example - don't run this]\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nInteger comparisons\nEvaluates to true if\n\n\n\n\nint1 -eq int2\nIntegers int1 and int2 are equal\n\n\nint1 -ne int2\nIntegers int1 and int2 are not equal\n\n\nint1 -lt int2\nInteger int1 is less than int2 (-le for less than or equal to)\n\n\nint1 -gt int2\nInteger int1 is greater than int2 (-ge for greater than or equal to)\n\n\n\n\n\n\n\n\n\n\nAnother integer comparison example (Click to expand)\n\n\n\n\n\nSay that we want to run a program with options that depend on our number of samples. With the number of samples determined from the number of lines in a hypothetical file samples.txt and stored in a variable $n_samples, we can test if the number is greater than 9 with \"$n_samples\" -gt 9, where gt stands for “greater than”:\n# [Hypothetical example - don't run this]\n\n# Store the number of samples in variable $n_samples:\nn_samples=$(cat samples.txt | wc -l)\n\n# With '-gt 9', the if statement tests whether the number of samples is greater than 9:\nif [[ \"$n_samples\" -gt 9 ]]; then\n    # Commands to run if nr of samples &gt;9:\n    echo \"Processing files with algorithm A\"\nelse\n    # Commands to run if nr of samples is &lt;=9:\n    echo \"Processing files with algorithm B...\"\nfi\n\n\n\n\n\n\n\n\n\nCombining multiple expressions with && and || (Click to expand)\n\n\n\n\n\nTo test for multiple conditions at once, use the && (“and”) and || (“or”) shell operators — for example:\n\nIf the number of samples is less than 100 and at least 50 (i.e. 50-99):\nif [[ \"$n_samples\" -lt 100 && \"$n_samples\" -ge 50 ]]; then\n    # Commands to run if the number of samples is 50-99\nfi\nIf either one of two FASTQ files don’t exist:\nif [[ ! -f \"$fastq_R1\" || ! -f \"$fastq_R2\" ]]; then\n    # Commands to run if either file doesn't exist - probably report error & exit\nfi\n\n\n\n\n\n\n Exercise: No middle names allowed!\nIn your printname.sh script, add the if statement from above that tests whether the correct number of arguments were passed to the script. Then, try running the script consecutively with 1, 2, or 3 arguments.\n\n\nStart with this printname.sh script we wrote above.\n\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\nClick for the solution\n\nNote that the if statement should come before you copy the variables to first_name and last_name, otherwise you get the “unbound variable error” before your descriptive custom error, when you pass 0 or 1 arguments to the script.\nThe final script:\n#!/bin/bash\nset -euo pipefail\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nRun it with different numbers of arguments:\nbash scripts/printname.sh Jelmer\nError: wrong number of arguments\nYou provided 1 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\nbash scripts/printname.sh Jelmer Poelstra\nFirst name: Jelmer\nLast name: Poelstra\nbash scripts/printname.sh Jelmer Wijtze Poelstra\nError: wrong number of arguments\nYou provided 3 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\n\n\n\n Exercise: Conditionals II\nOpen a new script sandbox.sh and in it, write an if statement that tests whether the script scripts/printname.sh exists and is a regular file, and:\n\nIf it is (then block), report the outcome with echo (e.g. “The file is found”).\nIf it is not (else block), also report that outcome with echo (e.g. “The file is not found”).\n\nThen:\n\nRun your if statement by pasting the code into the terminal — it should report that the file is found.\nIntroduce a typo in the file name in the if statement, and run it again, to check that the file is not indeed not found.\n\n(Note that your new script isn’t meant to be run per se, but it is much easier to write multi-line statements in a text file than directly in the terminal.)\n\n\nClick for the solution\n\n# Note: you need single quotes when using exclamation marks with echo!\nif [[ -f scripts/printname.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nPhew! The file is found.\nAfter introducing a typo:\nif [[ -f scripts/printnames.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nOh no! The file is not found!"
  },
  {
    "objectID": "week4/w4_1_scripts.html#footnotes",
    "href": "week4/w4_1_scripts.html#footnotes",
    "title": "Shell scripting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Don’t worry about the warning that other keybindings exist for this shortcut.↩︎\n Alternatively, find the script in the file explorer in the side bar and click on it there.↩︎\n And more generally, you can’t remove or edit files that are not yours unless you’ve explicitly been given permission for this.↩︎\n Compare this with the situation for file names, which ideally do not contain spaces and special characters either, but in which - and . are recommended.↩︎\n You can also use single square brackets [ ] but the double brackets have more functionality and I would recommend to always use these.↩︎\nA single = also works but == is clearer.↩︎"
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#overview-and-setting-up",
    "href": "week4/w4_2_cli-tools.html#overview-and-setting-up",
    "title": "Running command-line tools with shell scripts",
    "section": "Overview and setting up",
    "text": "Overview and setting up\nThis session focuses on using programs, like various bioinformatics tools, with command-line interfaces (CLIs), and on running these inside shell scripts.\n\nThe strategy that you’ll learn is to write scripts that run a single program a single time, even if you need to run the program many times: in that case, you’ll loop over files outside of that script.\nThis means you’ll also need a higher-level “runner” script in which you save the looping code and more. We will talk about why this strategy makes sense, especially when you have a supercomputer at your disposal.\n\nWe’ll start with a quick intro to FASTQ files and the FastQC program, which will be our first example of a CLI tool.\n\nOur practice data set\nOur practice data set is from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published last year in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitos infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquitos according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days\n\nHowever, to keep things manageable for our practice, I have subset the data to omit the 21-day samples and only keep 500,000 reads per FASTQ file. All in all, our set of files consists of:\n\n44 paired-end Illumina FASTQ files for 22 samples.\nCulex pipiens reference genome file from NCBI: assembly in FASTA format and annotation in GTF format.\nA metadata file in TSV format with sample IDs and treatment & time point info.\nA README file describing the data set.\n\n\n\nGet your own copy of the data\n# (Assuming you are in /fs/ess/PAS2700/users/$USER)\ncd week04\n\ncp -rv /fs/ess/PAS2700/share/garrigos_data .\n‘/fs/ess/PAS2700/share/garrigos_data’ -&gt; ‘./garrigos_data’\n‘/fs/ess/PAS2700/share/garrigos_data/meta’ -&gt; ‘./garrigos_data/meta’\n‘/fs/ess/PAS2700/share/garrigos_data/meta/metadata.tsv’ -&gt; ‘./garrigos_data/meta/metadata.tsv’\n‘/fs/ess/PAS2700/share/garrigos_data/ref’ -&gt; ‘./garrigos_data/ref’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.gtf’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.gtf’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.fna’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.fna’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq’ -&gt; ‘./garrigos_data/fastq’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802870_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802870_R1.fastq.gz’\n# [...output truncated...]\nTake a look at the files you just copied with the tree command, a sort of recursive ls with simple tree-like output:\n# -C will add colors (not shown in the output below)\ntree -C garrigos_data\ngarrigos_data\n├── fastq\n│   ├── ERR10802863_R1.fastq.gz\n│   ├── ERR10802863_R2.fastq.gz\n│   ├── ERR10802864_R1.fastq.gz\n│   ├── ERR10802864_R2.fastq.gz\n│   ├── [...other FASTQ files not shown...]\n├── meta\n│   └── metadata.tsv\n├── README.md\n└── ref\n    ├── GCF_016801865.2.fna\n    └── GCF_016801865.2.gtf\n\n3 directories, 48 files"
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#fastq-files-and-fastqc",
    "href": "week4/w4_2_cli-tools.html#fastq-files-and-fastqc",
    "title": "Running command-line tools with shell scripts",
    "section": "1 FASTQ files and FastQC",
    "text": "1 FASTQ files and FastQC\n\n1.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines. Like most genomic data files, these are plain text files. Each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe nucleotide sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic sequence data format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”).\n\n\n\n\n\n\n1.2 Our FASTQ files\nTake a look at a file listing of your FASTQ files:\nls -lh garrigos_data/fastq\ntotal 941M\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802863_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802863_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802864_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802864_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802866_R1.fastq.gz\n# [...output truncated...]\nNote that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads).\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\nThe files are ~21-22 Mb in size — considerably smaller than the original file sizes (around 1-2 Gb, which is typical) because they were subsampled.\n\n\n\n\n1.3 Viewing the contents of FASTQ files\nNext, try to take a peak inside one of these FASTQ files. Use -n 8 with head to print the first 8 lines (2 reads):\nhead -n 8 garrigos_data/fastq/ERR10802863_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWe were directly presented with the contents of the compressed file, which isn’t human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don’t need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we’ll use the less command, which automatically displays gzip-compressed files in human-readable form:\nless -S garrigos_data/fastq/ERR10802863_R1.fastq.gz\n@ERR10802863.8435456 8435456 length=74\nCAACGAATACATCATGTTTGCGAAACTACTCCTCCTCGCCTTGGTGGGGATCAGTACTGCGTACCAGTATGAGT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.27637245 27637245 length=74\nGCCACACTTTTGAAGAACAGCGTCATTGTTCTTAATTTTGTCGGCAACGCCTGCACGAGCCTTCCACGTAAGTT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE&lt;EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.10009244 10009244 length=73\nCTCGGCGTTAACTTCATCACGCAGATCATTCCGTTCCAGCAGCTGAAGCAAGACTACCGTCAGTACGAGATGA\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.6604176 6604176 length=74\nAACTACAAATCTTCCTGTGCCGTTTCCAGCAAGTACGTCGATACCTTCGATGGACGCAACTACGAGTACAACAT\n+\nAAAAAEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n\nThe -S option to less suppresses line-wrapping: lines in the file will not be “wrapped” across multiple lines.\n\n\n\n\n\n\n\n\n Exercise: Explore the file with less\nAfter running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nRecall that you won’t get your shell prompt back until you press q to quit less.\nDo you notice anything that strikes you as potentially unusual, some reads that look different from others?\n\n\nClick for the solution\n\nThere are a number of reads that are much shorter than the others and only consist of N, i.e. uncalled bases. For example:\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n1.4 FastQC\nFastQC is a ubiquitous tool for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph:\n\n\n\nA FastQC per-base quality score graph for files with reasonably good quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read."
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#running-fastqc-interactively",
    "href": "week4/w4_2_cli-tools.html#running-fastqc-interactively",
    "title": "Running command-line tools with shell scripts",
    "section": "2 Running FastQC interactively",
    "text": "2 Running FastQC interactively\nThe command fastqc will run the FastQC program. If you want to analyze a FASTQ file with default FastQC settings, a complete FastQC command would simply be fastqc followed by the file name:\n# (Don't run this)\nfastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the same dir that contains the input FASTQ files — this means mixing your raw data with your results, which we don’t want!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle — while FastQC is installed at OSC1, we have to first “load it”2:\nmodule load fastqc/0.11.8\n\n\n Exercise: FastQC help and output dir\nPrint FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nRunning fastqc -h or fastqc --help will work to show the help info. You’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\n# We'll have to first create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\n# Now we run FastQC\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R1.fastq.gz\nIn the output dir we specified, we have a .zip file, which contains tables with FastQC’s data summaries, and an .html (HTML) file, which contains the graphs:\nls -lh results/fastqc\ntotal 512K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n\n\n\n\n\n\nSpecifying an output dir vs. output file(s)\n\n\n\nFastQC allows us to specify the output directory, but not the output file names: these will be automatically determined based on the input file name(s). This kind of behavior is fairly common for bioinformatics programs, since they will often produce multiple output files.\n\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir or a separate one?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be more convenient to have all results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\nApprox 15% complete for ERR10802863_R2.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R2.fastq.gz\nls -lh results/fastqc\ntotal 1008K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471 234K Mar 21 09:55 ERR10802863_R2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 244K Mar 21 09:55 ERR10802863_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#running-fastqc-with-a-shell-script",
    "href": "week4/w4_2_cli-tools.html#running-fastqc-with-a-shell-script",
    "title": "Running command-line tools with shell scripts",
    "section": "3 Running FastQC with a shell script",
    "text": "3 Running FastQC with a shell script\nInstead of running FastQC interactively, we’ll want to write a script that runs it. Specifically, our script will deliberately run FastQC on only one FASTQ file.\nIn bioinformatics, you commonly need to run a CLI tool many times, because most tools can or have to be run separately run for each file or sample. Instead of writing a script that runs one file or sample, a perhaps more intuitive approach would be writing a script that processes all files/samples in a single run. That can be accomplished by:\n\nLooping over files/samples inside the script; or\nPassing many file names or a glob with * to a single run of the tool (this can be done with some tools).\n\nHowever, given that we have access to OSC’s clusters, it will save running time -potentially a lot of it- when we submit a separate batch job for each FASTQ file. This is why we will write a script such that runs only one file, and then we’ll run that script many times using a loop outside of te script.\nFor now, we’ll practice with writing scripts this way, and running them interactively. Next week, we will take the next step and submit each run of the script as a batch job.\n\n\n3.1 Arguments to the script\nOur favored approach of running the script for one FASTQ file at a time means that our script needs to accept a FASTQ file name as an argument. So instead of using a line like the one we ran above…\n# [Don't copy or run this]\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\n…we would use a variable for the file name — for example:\n# [Don't copy or run this]\nfastqc --outdir results/fastqc \"$fastq_file\"\nAnd while we’re at it, we may also want to use a variable for the output dir:\n# [Don't copy or run this]\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nOf course, these variables don’t appear out of thin air completely — we need to pass arguments to the script, and copy the placeholder variables inside the script:\n# [Don't copy or run this]\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n\n\n\n\n\n\nRunning the script\n\n\n\nAnd such a script would be run for a single file as follows:\n# [Don't copy or run this]\n# Syntax: 'bash &lt;script-path&gt; &lt;argument1&gt; &lt;argument2&gt;'\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R2.fastq.gz results/fastqc\nAnd it would be run for all files by looping over all them as follows:\n# [Don't copy or run this]\n# Run the script separately for each FASTQ file\nfor fastq_file in data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\n\n\n\n3.2 Creating an initial script\nWe saw some code to run FastQC inside a script, to which we should add a number of “boilerplate” bits of code:\n\nThe shebang line and strict Bash settings:\n#!/bin/bash\nset -euo pipefail\nA line to load the relevant OSC software module:\nmodule load fastqc/0.11.8\nA line to create the output directory if it doesn’t yet exist:\nmkdir -p \"$outdir\"\n\n\n\n\n\n\n\nRefresher: the -p option to mkdir (Click to expand)\n\n\n\n\n\nUsing the -p option does two things at once, and both are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once (i.e., to act recursively): by default, mkdir errors out if the parent directory/ies of the specified directory don’t yet exist.\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n# This successfully creates both directories\nmkdir -p newdir1/newdir2\nIf the directory already exists, it won’t do anything and won’t return an error. Without this option, mkdir would return an error in this case, which would in turn lead the script to abort at that point with our set settings:\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: File exists\n# This does nothing since the dirs already exist\nmkdir -p newdir1/newdir2\n\n\n\n\nWith those additions, our partial script would look like this:\n# [Don't copy or run this - we'll add to it later]\n\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nNotice that this script to run a CLI tool is very similar to our “toy scripts” from the previous sessions: mostly standard (“boilerplate”) code with just a single command to run our program of interest. Therefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n3.3 Add some “logging” statements\nIt is often useful to have your scripts “report” or “log” what is going on. For instance:\n\nAt what date and time did we run this script.\nWhich arguments were passed to the script.\nWhat are the designated output dirs/files.\nPerhaps even summaries of the output (we won’t do this here).\n\nAll of this can help with troubleshooting and record-keeping3. Let’s try this with our FastQC script:\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Initial reporting\necho \"# Starting script fastqc.sh\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Done with script fastqc.sh\"\ndate\nA couple of notes about the lines that were added to the script above:\n\nWe printed a “marker line” Done with script that indicates the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nRunning date at the beginning and end of the script is one way to check the running time of the script.\nPrinting the input files can be particularly useful for troubleshooting.\nThe lines that only have echo will simply print a blank line, basically as a separator between sections.\n\n Create a script to run FastQC:\ntouch scripts/fastqc.sh\nOpen it and paste in the code in the box above."
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#a-runner-script",
    "href": "week4/w4_2_cli-tools.html#a-runner-script",
    "title": "Running command-line tools with shell scripts",
    "section": "4 A “runner” script",
    "text": "4 A “runner” script\n\n4.1 Running our FastQC script for 1 file\nLet’s run our FastQC script for one FASTQ file:\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R1.fastq.gz results/fastqc\n# Starting script fastqc.sh\nWed Mar 27 21:53:13 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nWed Mar 27 21:53:19 EDT 2024\nHowever, as discussed above, we’ll want to run the script for each FASTQ file. We’ll have to write a loop to do so, and that loop will go in a “runner script”.\n\n\n\n4.2 What are runner scripts and why do we need them\nThe loop code could be directly typed in the terminal, but it’s better to save this in a file/script as well.\nWe will now create such a file, which has the overall purpose of documenting the steps we took. You can think of this file as akin to an analysis lab notebook4. Because it will contain shell code, we will save it as a shell script (.sh) just like the script to run fastqc.sh and other individual analysis steps.\nHowever, it is important to realize that this script is conceptually different from the scripts that run individual steps of your analysis. The latter are meant to be run/submitted in their entirety by the runner script, whereas commands in the former typically have to be run one-by-one, i.e. interactively. This kind of script is sometimes called a “runner” or “master” script.\nTo summarize, we’ll separate our code into two hierarchical levels of scripts:\n\nScripts that run individual steps of your analysis, like fastqc.sh. These will eventually be submitted a batch jobs.\nAn overarching “runner” script with code that we run interactively, to orchestrates batch job submission of the individual steps.\n\nTo make this division clearer, we’ll also save these scripts in separate directories:\n\nscripts for the analysis scripts.\nrun for the runner script(s).\n\n\n\n\n\n\n\n\nKeep the scripts for individual steps simple (Click to expand)\n\n\n\n\n\nIt is a good idea to keep the shell scripts you will submit (e.g., fastqc.sh) simple in the sense that they should generally just run one program, and not a sequence of programs.\nOnce you get the hang of writing these scripts, it may seem appealing to string a series of programs/steps together in a single script, so that it’s easier to rerun everything at once — but in practice, that will often end up leading to more difficulties than convenience. If you do want to develop a workflow that can be easily run and rerun from start to finish, you should learn a workflow management system like Snakemake or Nextflow — we will talk about Nextflow in week 6.\n\n\n\n\n\n\n\n\n\nWhy the runner script generally can’t itself be run at once in its entirety (Click to expand)\n\n\n\n\n\nFirst off, not that this applies only once we start submitting our scripts as batch jobs.\nOnce we’ve added multiple batch job steps, and the input of a later step uses the output of an earlier step, we won’t be able to just run the script as is. This is because the runner script would then submit jobs from different steps all at once, and that later step would start running before the earlier step has finished.\nFor example, consider the following series of two steps, in which the second step uses the output of the first step:\n# This script would create a genome \"index\" for STAR, that will be used in the next step\n# ('my_genome.fa' = input genome FASTA, 'results/star_index' = output index dir)\nsbatch scripts/star_index.sh my_genome.fa results/star_index\n\n# This script would align a FASTQ file to the genome index created in the previous step\n# ('results/star_index' = input index dir, 'sampleA.fastq.gz' = input FASTQ file,\n# 'results/star_align' = output dir)\nsbatch scripts/star_align.sh results/star_index sampleA.fastq.gz results/star_align \nIf these two lines were included in your runner script, and you would run that script in its entirety all at once, the script in the second step would be submitted just a split-second after the first one (when using sbatch, you get your prompt back immediately – there is no waiting). As such, it would fail because of the missing output from the first step.\nIt is possible to make sbatch batch jobs wait for earlier steps to finish (e.g. with the --dependency option), but this quickly gets tricky. If you want to create a workflow/pipeline that can run from start to finish in an automated way, you should consider using a workflow management system like Snakemake or NextFlow — we will talk about Nextflow in week 6!\n\n\n\n\n\n\n4.3 Creating our runner script\nCreate a new file, and open it after running these commands:\nmkdir run\ntouch run/run.sh\nIn this script, add the code that runs our fastqc.sh script for each FASTQ file, and then run that code:\n# Run FastQC for each FASTQ file\nfor fastq_file in garrigos_data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n# Starting script fastqc.sh\nThu Mar 21 10:06:46 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n\n# Starting script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#looping-over-samples-rather-than-files",
    "href": "week4/w4_2_cli-tools.html#looping-over-samples-rather-than-files",
    "title": "Running command-line tools with shell scripts",
    "section": "5 Looping over samples rather than files",
    "text": "5 Looping over samples rather than files\nIn some cases, we can’t simply loop over all files like we have done so far. For example, in many tools that process paired-end FASTQ files, the corresponding R1 and R2 files for each sample must be processed together. That is, we don’t run the tool separately for each FASTQ file, but separately for each sample i.e. each pair of FASTQ files.\nHow can we loop over pairs of FASTQ files instead? There are two main ways:\n\nCreate a list of sample IDs, loop over these IDs, and find the pair of FASTQ files with matching names.\nLoop over the R1 files only and then infer the name of the corresponding R2 file within the loop. This is generally straightforward because the file names should be identical other than the read-direction identifier (R1/R2).\n\nBelow, we will use the second method — but first, we’ll recap/learn a few prerequisites.\n\n\n5.1 Recap of basename, and dirname\nRunning the basename command on a filename will strip any directories in its name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz\nERR10802863_R1.fastq.gz\nYou can also provide any arbitrary suffix to also strip from the file name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz .fastq.gz\nERR10802863_R1\nIf you instead want the directory part of the path, use the dirname command:\ndirname garrigos_data/fastq/ERR10802863_R1.fastq.gz\ngarrigos_data/fastq\n\n\n\n5.2 Parameter expansion\nYou can use so-called “parameter expansion”, with parameter basically being another word for variable, to search-and-replace text in your variable’s values. For example:\n\nAssign a short DNA sequence to a variable:\ndna_seq=\"AAGTTCAT\"\necho \"$dna_seq\"\nAAGTTCAT\nUse parameter expansion to replace all Ts with U to convert the DNA to RNA:\necho \"${dna_seq//T/U}\"\nAAGUUCAT\nYou can also assign the result of the parameter expansion back to a variable:\nrna_seq=\"${dna_seq//T/U}\"\necho \"$rna_seq\"\nAAGUUCAT\n\nSo, the syntax for this type of parameter expansion is {var_name//&lt;search&gt;/replace} — let’s deconstruct that:\n\nReference the variable, using the full notation, with braces (${var_name})\nAfter the first two forward slashes, enter the search pattern: (T)\nAfter another forward slash, enter the replacement: (U).\n\nIf you needed to replace at most one of the search patterns, a single backslash after the variable name would suffice: {var_name/&lt;search&gt;/replace}.\n\n\n Exercise: Get the R2 file name with parameter expansion\nFile names of corresponding R1 and R2 FASTQ files should be identical other than the marker indicating the read direction, which is typically R1/R2 (and in some cases just 1 and 2).\nAssign the file name garrigos_data/fastq/ERR10802863_R1.fastq.gz to a variable and use parameter expansion to get the name of the corresponding R2 file name. Also save the R2 file name in a variable.\n\n\nSolutions\n\nfastq_R1=garrigos_data/fastq/ERR10802863_R1.fastq.gz\nfastq_R2=${fastq_R1/_R1/_R2}\nAbove, e.g. ${fastq_R1/R1/R2}, that is without underscores, would have also worked. But note that it’s generally good to avoid unwanted search-pattern matches by making the search string as specific as possible. So perhaps ${fastq_R1/_R1.fastq.gz/_R2.fastq.gz} would have been even better.\nTest that it worked:\necho \"$fastq_R2\"\ngarrigos_data/fastq/ERR10802863_R2.fastq.gz\n\n\n\n\n\n5.3 A per-sample loop\nWe will now create a loop that:\n\nLoops over R1 FASTQ files only, and then infers the corresponding R2 file name.\nDefines output file names that are the same as the input file names but in a different dir.\n\nTo stay focused just on the shell syntax here, we won’t include code to run an actual bioinformatics tool (you’ll do that in this week’s exercises), but will use a fictional tool trimmer:\n# [Don't run or copy this]\n\n# Loop over the R1 files - our glob is `*_R1.fastq.gz` to only select R1 files\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    # Get the R2 file name with parameter expansion\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Define the output files (assume that a variable $outdir exists)\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    echo \"Output files: $R1_out $R2_out\"\n    \n    # Use the imaginary program 'trimmer' with options --in1/--in2 for the R1/R2 input files,\n    # and --out1/--out2 for the R1/R2 output files:\n    trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\ndone\n\n\n\n5.4 Converting to the single-sample script format\nBut wait! Aren’t we supposed to write a script that only processes one sample at a time, and then run/submit that script with a loop? That’s right, so now that we know what to do, let’s switch to that setup.\nCreate a new script scripts/trim_mock.sh and paste the following code into it:\n#!/bin/bash\nset -euo pipefail\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Infer the R2_in file name\nR2_in=${R1_in/_R1/_R2}\n    \n# Define the output file names\nR1_out=\"$outdir\"/$(basename \"$R1_in\")\nR2_out=\"$outdir\"/$(basename \"$R2_in\")\n\n# Initial reporting\necho \"# Starting script trim_mock.sh\"\ndate\necho \"# Input R1 file:       $R1_in\"\necho \"# Input R2 file:       $R2_in\"\necho \"# Output R1 file:      $R1_out\"\necho \"# Output R2 file:      $R2_out\"\necho\n\n# Mock-run the tool: I preface the command with 'echo' so this will just report\n# and not try to run a program that doesn't exist\necho trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\n\n# Final reporting\necho\necho \"# Done with script trim_mock.sh\"\ndate\nNow, add the following code to our run.sh script, and run that:\n# Run the trim_mock.sh script for each sample\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    bash scripts/trim_mock.sh \"$R1_in\" results/trim_mock\ndone\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802863_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802863_R2.fastq.gz\n\ntrimmer --in1 garrigos_data/fastq/ERR10802863_R1.fastq.gz --in2 garrigos_data/fastq/ERR10802863_R2.fastq.gz --out1 results/trim_mock/ERR10802863_R1.fastq.gz --out2 results/trim_mock/ERR10802863_R2.fastq.gz\n\n# Done with script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802864_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802864_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802864_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802864_R2.fastq.gz\n# [...output truncated...]\nCheck the files in the output dir:\nls -lh results/trim_mock\ntotal 0\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802866_R1.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "week4/w4_2_cli-tools.html#footnotes",
    "href": "week4/w4_2_cli-tools.html#footnotes",
    "title": "Running command-line tools with shell scripts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list.↩︎\n We’ll talk more about loading (and installing) software at OSC next week.↩︎\nWe’ll see in the upcoming Slurm module that we when submit scripts to the OSC queue (rather than running them directly), the output of scripts that is normally printed to screen, will instead go to a sort of “log” file. So, your script’s reporting will end up in this file.↩︎\n Or depending on how you use this exactly, as your notebook entry that contains the final protocol you followed.↩︎"
  },
  {
    "objectID": "week4/w1_removed.html",
    "href": "week4/w1_removed.html",
    "title": "Removed w1 exercises",
    "section": "",
    "text": "0.1 1.10.2 Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. Every individual was sampled several times.\nWrite a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nShow hints\n\n\nYou want to turn the solution of part 1 into a script; to do so, open a new file and copy the code you’ve written.\nIn the script, you can use the first two so-called positional parameters, $1 and $2, to represent the file name and the maleID, respectively.\n$1 and $2 represent the first and the second argument passed to a script like so: bash myscript.sh arg1 arg2.\n\n\n\n\n\n0.2 1.10.3 Plant–Pollinator Networks\nSaavedra and Stouffer (2013) studied several plant–pollinator networks. These can be represented as rectangular matrices where the rows are pollinators, the columns plants, a 0 indicates the absence and 1 the presence of an interaction between the plant and the pollinator.\nThe data of Saavedra and Stouffer (2013) can be found in the directory CSB/unix/data/Saavedra2013.\nWrite a script that takes one of these files and determines the number of rows (pollinators) and columns (plants). Note that columns are separated by spaces and that there is a space at the end of each line. Your script should return:\nbash netsize.sh ../data/Saavedra2013/n1.txt\n# Filename: ../data/Saavedra2013/n1.txt\n# Number of rows: 97\n# Number of columns: 80\n\n\nShow hints\n\nTo build the script, you need to combine several commands:\n\nTo find the number of rows, you can use wc.\nTo find the number of columns, take the first line, remove the spaces, remove the line terminator \\n, and count characters.\n\n\n\n\n\n0.3 1.10.4 Data Explorer\nBuzzard et al. (2016) collected data on the growth of a forest in Costa Rica. In the file Buzzard2015_data.csv you will find a subset of their data, including taxonomic information, abundance, and biomass of trees.\nWrite a script that, for a given CSV file and column number, prints:\n\nThe corresponding column name;\nThe number of distinct values in the column;\nThe minimum value;\nThe maximum value.\n\nFor example, running the script as below should produce the following output:\nbash explore.sh ../data/Buzzard2015_data.csv 7\nColumn name:\nbiomass\nNumber of distinct values:\n285\nMinimum value:\n1.048466198\nMaximum value:\n14897.29471\n\n\nShow hints\n\n\nYou can select a given column from a csv file using the command cut. Then:\n\nThe column name is going to be in the first line (header); access it with head.\nThe number of distinct values can be found by counting the number of lines when you have sorted them and removed duplicates (using a combination of tail, sort and uniq).\nThe minimum and maximum values can be found by combining sort and head (or tail),\nTo write the script, use the positional parameters $1 and $2 for the file name and column number, respectively.\n\n\n\n\n\n\n0.4 Solutions\n\n\n0.5 1.10.2 Hormone Levels in Baboons\n\n\n2. Write a script taking as input the file name and the ID of the individual, and returning the number of records for that ID.\n\n\nIn the script, we just need to incorporate the arguments given when calling the script, using $1 for the file name and $2 for the individual ID, into the commands that we used above:\n\ncut -f 1 $1 | grep -c -w $2\n\nA slightly more verbose and readable example:\n\n#!/bin/bash\n\nfilename=$1\nind_ID=$2\necho \"Counting the nr of occurrences of individual ${ind_ID} in file ${filename}:\" \ncut -f 1 ${filename} | grep -c -w ${ind_ID}\n\n\n\n\n\n\nNote\n\n\n\nVariables are assigned using name=value (no dollar sign!), and recalled using $name or ${name}. It is good practice to put curly braces around the variable name. We will talk more about bash variables in the next few weeks.\n\n\n\nTo run the script, assuming it is named count_baboons.sh:\n\nbash count_baboons.sh ../data/Gesquiere2011_data.csv 27\n# 5\n\n\n\n\n0.6 1.10.3 Plant–Pollinator Networks\n\n\nSolution\n\n\nCounting rows:\n\nCounting the number of rows amount to counting the number of lines. This is easily done with wc -l. For example:\n\nwc -l ../data/Saavedra2013/n10.txt \n# 14 ../data/Saavedra2013/n10.txt\n\nTo avoid printing the file name, we can either use cat or input redirection:\n\ncat ../data/Saavedra2013/n10.txt | wc -l\nwc -l &lt; ../data/Saavedra2013/n10.txt \nCounting rows:\n\nCounting the number of columns is more work. First, we need only the first line:\n\nhead -n 1 ../data/Saavedra2013/n10.txt\n# 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n\nNow we can remove all spaces and the line terminator using tr:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n'\n# 01000001000000000100\n\nFinally, we can use wc -c to count the number of characters in the string:\n\nhead -n 1 ../data/Saavedra2013/n10.txt | tr -d ' ' | tr -d '\\n' | wc -c\n# 20\nFinal script:\n#!/bin/bash\n\nfilename=$1\n\necho \"Filename:\"\necho ${filename}\necho \"Number of rows:\"\ncat ${filename} | wc -l\necho \"Number of columns:\"\nhead -n 1 ${filename} | tr -d ' ' | tr -d '\\n' | wc -c\n\nTo run the script, assuming it is named counter.sh:\n\nbash counter.sh ../data/Saavedra2013/n10.txt\n# 5\n\n\n\n\n\n\nWe’ll learn about a quicker and more general way to count columns in a few weeks.\n\n\n\n\n\n\n\n\n\n\n0.7 1.10.4 Data Explorer\n\n\nSolution\n\n\nFirst, we need to extract the column name. For example, for the Buzzard data file, and col 7:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | head -n 1\n# biomass\n\nSecond, we need to obtain the number of distinct values. We can sort the results (after removing the header), and use uniq:\n\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort | uniq | wc -l\n# 285\n\nThird, to get the max/min value we can use the code above, sort using -n, and either head (for min) or tail (for max) the result.\n\n# Minimum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | head -n 1\n# 1.048466198\n\n# Maximum:\ncut -d ',' -f 7 ../data/Buzzard2015_data.csv | tail -n +2 | sort -n | tail -n 1\n# 14897.29471\n\nHere is an example of what the script could look like:\n\n#!/bin/bash\n\nfilename=$1\ncolumn_nr=$2\n\necho \"Column name\"\ncut -d ',' -f ${column_nr} ${filename} | head -n 1\necho \"Number of distinct values:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort | uniq | wc -l\necho \"Minimum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | head -n 1\necho \"Maximum value:\"\ncut -d ',' -f ${column_nr} ${filename} | tail -n +2 | sort -n | tail -n 1\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week4/usethis.html",
    "href": "week4/usethis.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "More shell keyboard shortcuts\n\n\n\n\nPress ⇧ to get the previous command back on the prompt, and then press Ctrl+U to delete text until the beginning of the line.\nCtrl+U actually cuts the text: “Yank” it back with Ctrl+Y. Press Enter again.\nType cd and then space, and then press Alt+. (or Esc+. on a Mac). That should insert /fs/ess/PAS2700 on the line. In general terms, this keyboard shortcut will insert the last word (argument) from the last command line (and you can cycle back by pressing it multiple times!).\n\n\n\n\n\n Exercise: Add keyboard shortcut to run shell commands from the editor:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week3/w3_git2.html#remote-repositories",
    "href": "week3/w3_git2.html#remote-repositories",
    "title": "Git: Remotes on GitHub",
    "section": "1 Remote repositories",
    "text": "1 Remote repositories\nSo far, we have been locally version-controlling our originspecies repository. Now, we also want to put this repo online, so we can:\n\nShare our work (e.g. alongside a publication) and/or\nHave an online backup and/or\nCollaborate with others2.\n\nWe will use the GitHub website as the place to host our online repositories. Online counterparts of local repositories are usually referred to as “remote repositories” or simply “remotes”.\nUsing remote repositories will mean adding a couple of Git commands to our toolbox:\n\ngit remote to add and manage connections to remotes.\ngit push to upload (“push”) changes from local to remote.\ngit pull to download (“pull”) changes from remote to local.\n\nBut we will need to start with some one-time setup to enable GitHub authentication.\n\n\n1.1 One-time setup: GitHub authentication\nIn order to link your local Git repositories to their online counterparts on GitHub, you need to set up GitHub authentication. There are two options for this (see the box below) but we will use SSH access with an SSH key.\n\nUse the ssh-keygen command to generate a public-private SSH key pair — in the command below, replace your_email@example.com with the email address you used to sign up for a GitHub account:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou’ll be asked three questions, and for all three, you can accept the default simply by pressing Enter:\n# Enter file in which to save the key (&lt;default path&gt;):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/users/PAS0471/jelmer/.ssh/id_ed25519): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.\nYour public key has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:KO73cE18HFC/aHKIXR7nf9Fk4++CiIw6GTk7ffG+p2c your_email@example.com\nThe key's randomart image is:\n+--[ED25519 256]--+\n|          ...    |\n|           . .   |\n|            + o.o|\n|       . + = *.+o|\n|    ... S * B oo.|\n|   .+.  .o =   .o|\n|    .*.o.+.. .  +|\n|   .= +o+ o E ...|\n|    o= o..+*   ..|\n+----[SHA256]-----+\nNow, you have a file ~/.ssh/id_ed25519.pub, which is your public key. To enable authentication, you will put this public key (which interacts with your private key) on GitHub. Print the public key to your screen using cat:\ncat ~/.ssh/id_ed25519.pub\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBz4qiqjbNrKPodoGyoF/v7n0GKyvc/vKiW0apaRjba2 your_email@example.com\nCopy the line that was printed to screen to your clipboard.\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, then select “Settings”.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nIn the form (see screenshot below):\n\nGive the key an arbitrary, informative “Title” (name), e.g. “OSC” to indicate that you will use this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the “Key” box.\nClick the green Add SSH key button. Done!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication and GitHub URLs\n\n\n\nThe current two options to authenticate to GitHub when connecting with remotes are:\n\nSSH access with an SSH key (which we have just set up)\nHTTPS access with a Public Access Token (PAT). See this GitHub page to set this up instead of or in addition to SSH access.\n\nFor everything on GitHub, there are separate SSH and HTTPS URLs. When using SSH (as we are), we need to use URLs with the following format:\ngit@github.com:&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git\n(And when using HTTPS, you would use URLs like https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git)\n\n\n\n\n\n1.2 Create a remote repository\nWhile we can interact with online repos using Git commands, we can’t create a new online repo with the Git CLI. Therefore, we will to go to the GitHub website to create a new online repo:\n\nOn the GitHub website, click the + next to your avatar (top-right) and select “New repository”:\n\n\n\n\n\n\n\nIn the box “Repository name”, we’ll use the same name that we gave to our local directory: originspecies3.\n\n\n\n\n\n\n\nLeave other options as they are, as shown below, and click “Create repository”:\n\n\n\n\n\n\n\n\n\n1.3 Link the local and remote repositories\nAfter you clicked “Create repository”, a page similar to this screenshot should appear, which gives us some information about linking the remote and local repositories:\n\n\n\n\n\nWe go back to VS Code, where we’ll enter the commands that GitHub provided to us under the “…or push an existing repository from the command line” heading shown at the bottom of the screenshot above:4\n\nFirst, we tell Git to add a “remote” connection with git remote, providing three arguments to this command:\n\nadd — because we’re adding a remote connection.\norigin — the arbitrary nickname we’re giving the connection (usually called “origin” by convention).\n\nThe URL to the GitHub repo (in SSH format: click on the HTTPS/SSH button to toggle the URL type).\n\n# git remote add &lt;remote-nickname&gt; &lt;URL&gt;\ngit remote add origin git@github.com:&lt;user&gt;/originspecies.git\nSecond, we upload (“push”) our local repo to remote using git push. When we push a repository for the first time, we need to use the -u option to set up an “upstream” counterpart:\n# git push -u &lt;connection&gt; &lt;branch&gt;\ngit push -u origin main\nYou should then get a message like this: type yes and press Enter.\nThe authenticity of host 'github.com (140.82.114.4)' can't be established.\nECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM.\nECDSA key fingerprint is MD5:7b:99:81:1e:4c:91:a5:0d:5a:2e:2e:80:13:3f:24:ca.\nAre you sure you want to continue connecting (yes/no)?\nThen, the push/upload should go through, with a message along these lines printed to screen:\nCounting objects: 18, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (12/12), done.\nWriting objects: 100% (18/18), 1.67 KiB | 0 bytes/s, done.\nTotal 18 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), done.\nremote: To git@github.com:jelmerp/originspecies2.git\n* [new branch]      main -&gt; main\n\n\n\n\n\n\n\n\nPushing will be easier from now on\n\n\n\nNote that when we don’t give git push any arguments, it will push:\n\nTo the default remote connection.\nTo & from the currently active repository “branch” (default: main)5.\n\nBecause we only have one remote connection and one branch, we can from now on simply use the following to push:\ngit push\n\nAlso, note that you can check your remote connection settings for a repo with git remote -v:\ngit remote -v\norigin  git@github.com:jelmerp/originspecies.git (fetch)\norigin  git@github.com:jelmerp/originspecies.git (push)\n\n\n\n\n\n\n1.4 Explore the repository on GitHub\nBack at GitHub on your repo page, click where it says &lt;&gt; Code in the lower of the top bars:\n\n\n\n\n\nThis is basically our repo’s “home page”, and we can see the files that we just uploaded from our local repo:\n\n\n\n\n\nNext, click where it says x commits (x should be 10 in this case):\n\n\n\n\n\nYou’ll get an overview of the commits that you made, somewhat similar to what you get when you run git log:\n\n\n\n\n\nYou can click on a commit to see the changes that were made by it:\n\n\n\nThe line that is highlighted in green was added by this commit.\n\n\nOn the right-hand side, a &lt; &gt; button will allow you to see the state of the repo at the time of that commit:\n\n\n\n\n\nOn the commit overview page, scroll down all the way to the first commit and click the &lt; &gt;: you’ll see the repo’s “home page” again, but now with only the origin.txt file, since that was the only file in your repo at the time:\n\n\n\n\n\n\n\nGitHub “Issues”\nEach GitHub repository has an “Issues” tab — issues are mainly used to track bugs and other (potential) problems with a repository. In an issue, you can reference specific commits and people, and use Markdown formatting.\n\n\n\n\n\n\nWhen you hand in your final project submissions, you will create an issue simply to notify me about your repository.\n\n\n\n\n\n\nTo go to the Issues tab for your repo, click on Issues towards the top of the page:\n\n\n\n\n\nAnd you should see the following page — in which you can open a new issue with the “New” button:"
  },
  {
    "objectID": "week3/w3_git2.html#remote-repo-workflows-single-user",
    "href": "week3/w3_git2.html#remote-repo-workflows-single-user",
    "title": "Git: Remotes on GitHub",
    "section": "2 Remote repo workflows: single-user",
    "text": "2 Remote repo workflows: single-user\nIn a single-user workflow, all changes are typically made in the local repository, and the remote repo is simply periodically updated (pushed to). So, the interaction between local and remote is unidirectional:\n\n\n\n\n\nAWhen pushing to remote for the first time, you first set up the connection with git remote and use the u option to git push.\n\n\n\n\n\n\n\n\nBNow, the local (“My repo”) and remote (“GitHub”) are in sync.\n\n\n\n\n\n\n\n\n\n\nCNext, you’ve made a new commit locally: the local and remote are out of sync.\n\n\n\n\n\n\n\n\nDYou simply use git push to update the remote, after which the local and remote will be back in sync (latter not shown).\n\n\n\n\n\nIn a single-user workflow with a remote, you commit just like you would without a remote in your day-to-day work, and in addition, push to remote occasionally — let’s run through an example.\n\nStart by creating a README.md file for your repo:\necho \"# Origin\" &gt; README.md\necho \"Repo for book draft on my new **theory**\" &gt;&gt; README.md\nAdd and commit the file:\ngit add README.md\ngit commit -m \"Added a README file\"\n[main 63ce484] Added a README file\n1 file changed, 2 insertions(+)\ncreate mode 100644 README.md\nIf you now run git status, you’ll see that Git knows that your local repo is now one commit “ahead” of its remote counterpart:\nOn branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n(use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\nBut Git will not automatically sync. So now, push to the remote repository:\ngit push\nCounting objects: 4, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 404 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nremote: To git@github.com:jelmerp/originspecies.git\n4968e62..b1e6dad  main -&gt; main\n\n\nLet’s go back to GitHub: note that the README.md Markdown has been automatically rendered!\n\n\n\n\n\n\n\n\n\n\n\n\nWant to collaborate on repositories?\n\n\n\nThe version control bonus page goes into collaboration with Git and Github, i.e. multi-user workflows."
  },
  {
    "objectID": "week3/w3_git2.html#footnotes",
    "href": "week3/w3_git2.html#footnotes",
    "title": "Git: Remotes on GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Git is available at OSC even without loading this module, but that’s a much older version.↩︎\n We will not cover collaboration workflows in class, though – see the bonus material for this.↩︎\nThough note that these names don’t have to match up.↩︎\n Though we can skip the second one, git branch -M main, since our branch is already called “main”.↩︎\n To learn more about branches, see the Git bonus material.↩︎"
  },
  {
    "objectID": "week3/w3_exercises.html",
    "href": "week3/w3_exercises.html",
    "title": "Week 3 exercises",
    "section": "",
    "text": "In this exercise, you will primarily be practicing your Git skills. Some tips:"
  },
  {
    "objectID": "week3/w3_exercises.html#exercise",
    "href": "week3/w3_exercises.html#exercise",
    "title": "Week 3 exercises",
    "section": "1 Exercise",
    "text": "1 Exercise\n\nLocal Git workflow\n\nCreate a new directory at OSC for these exercises, and move there.\nFor example, /fs/ess/PAS2700/users/$USER/week03/exercises.\nLoad the OSC Git module.\nDon’t forget to do this, or you will be working with a much older version of Git.\nInitialize a local Git repository inside your new directory.\nCreate a README file in Markdown format.\nThe file should be named README.md, and for now, just contain a header saying that this is a repository for your exercises.\nStage and then commit the README file with an appropriate commit message.\nCreate a second Markdown file with some more contents.\nName it as you like and in it, describe the basic Git workflow and commands with bullet points and headers1.\nCreate at least two commits while you work on the Markdown file.\nTry to break your progress up into logical units that can be summarized with a descriptive commit message.\nUpdate the README.md file.\nBriefly describe the current contents of your repo now that you actually have some contents.\nStage and commit the updated README file.\n\nCreate a results directory with an empty file and make Git ignore this directory.\nAlso, commit your .gitignore file.\n\n\n\n\nCreate and sync an online version of the repo\nPhew, you made several commits! Time to share all this work with the world.\n\nCreate a Github repository.\nGo to https://github.com, sign in, and create a new repository. It’s a good idea to give it the same name as your local repo, but these names don’t have to match. Like in class, you want to create an empty GitHub repository, because you will upload all the contents from your local repo.\nPush your local repo to the online one you just created.\nWhen you’re done, click the Code button, and admire the nicely rendered README on the front page of your Github repo.\nOpen a GitHub Issue\nAs practice for when you need to do this for your Final Project submissions, open an Issue for your repo: in the Issues tab, click “New Issue”, and in the box where you can type your issue, tag me. For example:\nHey @jelmerp, can you please take a look at my repo?"
  },
  {
    "objectID": "week3/w3_exercises.html#solutions",
    "href": "week3/w3_exercises.html#solutions",
    "title": "Week 3 exercises",
    "section": "2 Solutions",
    "text": "2 Solutions\n\nCreate a new directory at OSC for these exercises, and move there.\nmkdir /fs/ess/PAS2700/users/$USER/week03/exercises\ncd /fs/ess/PAS2700/users/$USER/week03/exercises\nLoad the OSC Git module.\nmodule load git/2.39.0\nInitialize a local Git repository inside your new directory.\ngit init\nCreate a README file in Markdown format.\necho \"# This dir is for the week3 exercises on Git\" &gt; README.md\nStage and then commit the README file with an appropriate commit message.\ngit add README.md\ngit commit -m \"Added a README file\"\nCreate a second Markdown file with some more contents.\ngit_notes.md\nCreate at least two commits while you work on the Markdown file.\n# Commit #1\ngit add git_notes.md\ngit commit -m \"Started a document with notes on Git\"\n\n# (Make changes to git_notes.md)\n\n# Commit #1\ngit add git_notes.md\ngit commit -m \"Descriptive commit message #2\"\nUpdate the README.md file.\necho \"Added a file with notes on Git\" &gt;&gt; README.md\nStage and commit the updated README file.\ngit add README.md\ngit commit -m \"Added a description of the repo's contents to the README\"\nCreate a results dir and file, and make Git ignore this directory.\n# Create the dir and file that should be ignored\nmkdir results\ntouch results/results.txt\n# Create a gitignore file with instructions to ignore the 'results' dir\necho \"results/\" &gt; .gitignore\n# Add and commit the .gitignore\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\nCreate a GitHub repository.\nSee the lecture page.\nPush your local repo to the online one you just created.\n# Step 1: Set up the connection to the remote repo\n# (replace &lt;SSH-URL-to-repo&gt; with your actual SSH (not HTTPS!) URL)\ngit remote add origin &lt;SSH-URL-to-repo&gt;\n# Step 2: Push to remote\ngit push -u origin main\nOpen a GitHub Issue.\nSee the lecture page."
  },
  {
    "objectID": "week3/w3_exercises.html#footnotes",
    "href": "week3/w3_exercises.html#footnotes",
    "title": "Week 3 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Recall that in VS Code, you can open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.↩︎"
  }
]